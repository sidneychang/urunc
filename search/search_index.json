{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"urunc: A Lightweight Container Runtime for Unikernels","text":"<p>The main goal of <code>urunc</code> is to bridge the gap between traditional unikernels and containerized environments, enabling seamless integration with cloud-native architectures. Designed to fully leverage the container semantics and benefits from the OCI tools and methodology, <code>urunc</code> aims to become \u201crunc for unikernels\u201d, while offering compatibility with the Container Runtime Interface (CRI). Unikernels are packaged inside OCI-compatible images and <code>urunc</code> launches the unikernel on top of the underlying Virtual Machine or seccomp monitors. Thus, developers and administrators can package, deliver, deploy and manage unikernels using familiar cloud-native practises.</p> <p>For the above purpose <code>urunc</code> acts as any other OCI runtime. The main difference of <code>urunc</code> with other container runtimes is that instead of spawning a simple process, it uses a Virtual Machine Monitor (VMM) or a sandbox monitor to run the unikernel. It is important to note that <code>urunc</code> does not require any particular software running alongside the user's application inside or outside the unikernel. As a result, <code>urunc</code> is able to support any unikernel framework or similar technologies, while maintaining as low overhead as possible.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>OCI Compatibility: Compatible with the Open Container Initiative (OCI) standards, enabling the use of existing container tools and workflows.</li> <li>Container Runtime Interface (CRI) Support: Compatible with Kubernetes and other CRI-based systems for seamless integration into container orchestration platforms.</li> <li>Unikernel Support: Run applications and user code as unikernels, unlocking the performance and security advantages of unikernel technology.</li> <li>Integration with VMMs and other strong sandboxing mechanisms: Use lightweight VMMs or sandbox monitors to launch unikernels, facilitating efficient resource isolation and management.</li> <li>Un-opinionated and Extensible: Straightforward and easy integration of new unikernel frameworks and sandboxing mechanisms without any porting overhead.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<p>Unikernels are well known as a good fit for a variety of use cases, such as:</p> <ul> <li>Microservices: The lightweight and almost diminished OS noise of unikernels   can significantly improve the execution of applications, making unikernels an   attractive fit for microservices.</li> <li>Serverless and FaaS: The extremely fast instantiation time of unikernels   satisfies the event-driven, short-lived and scalable characteristics of   serverless computing</li> <li>Edge computing: The lightweight notion of unikernels suits very well with edge   devices, where resources constraints and performance are critical.</li> <li>Sensitive environments: The inherited strong VM-based isolation, along with   the minimized attack surface of unikernels, provide strong security guarantees   for sensitive applications which demand high security standards.</li> </ul> <p>In all the above use cases, <code>urunc</code> facilitates the seamless integration of unikernels with existing cloud-native tools and technologies, enabling the effortless distribution and management of applications running as unikernels.</p>"},{"location":"#current-support-of-unikernels-and-vmsandbox-monitors","title":"Current support of unikernels and VM/Sandbox monitors","text":"<p>The following table provides an overview of the currently supported VMMs and Sandbox monitors, along with the unikernels that can run on top of them.</p> Unikernel VM/Sandbox Monitor Arch Storage Rumprun Solo5-hvt, Solo5-spt x86, aarch64 Block/Devmapper Unikraft Qemu, Firecracker x86 Initrd, 9pfs MirageOS Qemu, Solo5-hvt, Solo5-spt x86, aarch64 Block/Devmapper Mewz Qemu x86 In-memory Linux Qemu, Firecracker x86, aarch64 Initrd, Block/Devmapper, 9pfs, Virtiofs"},{"location":"#community-and-meetings","title":"Community and Meetings","text":"<p>Join us for our monthly open meetings, held every last Wednesday of the month. These sessions are a great opportunity to share ideas, ask questions, and stay connected with the project team and other contributors.</p> <ul> <li>Meeting Frequency: Monthly (last Wednesday of the month)</li> <li>Time: 15:00 UTC</li> <li>Format: Open agenda + roadmap review Minutes &amp; Agenda</li> <li>Platform: LF meetings</li> <li>Invitation: link</li> <li>Slack channel</li> </ul>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Urunc Slack channel</li> <li>Contributing</li> <li>Getting metrics from <code>urunc</code></li> <li>Integration with k8s</li> </ul> <p> urunc is a Cloud Native Computing Foundation sandbox project. </p> <p> </p>"},{"location":"Sample-images/","title":"Sample Unikernel OCI images","text":"<p>In this document, you can find the images used to perform <code>urunc</code>'s end-to-end tests. This might be helpful for anyone looking to spawn some example unikernels using <code>urunc</code>.</p> <p>The naming convention used for these images is $APPLICATION-$HYPERVISOR-$UNIKERNEL-$ADDITIONAL_INFO:tag We plan to create and maintain multi-platform images soon, as well as enrich this list with new images.</p> <ul> <li>harbor.nbfc.io/nubificus/urunc/hello-hvt-rumprun-nonet:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-hvt-rumprun:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-hvt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-spt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-spt-rumprun-nonet:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-spt-rumprun:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-qemu-mewz:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-qemu-unikraft:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-world-qemu-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-firecracker-unikraft:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-world-firecracker-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-env-qemu-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-exec-env-qemu-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-exec-env-qemu-linux-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-env-firecracker-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-exec-env-firecracker-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-qemu-linux-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-qemu-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-hvt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-spt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-firecracker-linux-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-firecracker-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-server-qemu-mewz:latest</li> <li>harbor.nbfc.io/nubificus/urunc/httpreply-firecracker-unikraft:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-qemu-linux-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-qemu-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-qemu-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-firecracker-linux-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-firecracker-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/net-hvt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/net-spt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/net-qemu-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/block-test-hvt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/block-test-spt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/whoami-qemu-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/whoami-firecracker-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/busybox-qemu-linux-raw:latest</li> <li>harbor.nbfc.io/nubificus/urunc/busybox-firecracker-linux-raw:latest</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p><code>urunc</code> supports configuration through a TOML configuration file that allows you to customize various runtime behaviors including logging, timestamping, and monitor defaults. This document explains how to configure <code>urunc</code> using the configuration file.</p>"},{"location":"configuration/#configuration-file-location","title":"Configuration File Location","text":"<p><code>urunc</code> looks for its configuration file at <code>/etc/urunc/config.toml</code>. If the file doesn't exist or contains invalid configuration, <code>urunc</code> will use sensible defaults and continue to operate normally.</p>"},{"location":"configuration/#configuration-file-format","title":"Configuration File Format","text":"<p>The configuration file uses the TOML format and is organized into several sections:</p> <pre><code>[log]\nlevel = \"info\"\nsyslog = false\n\n[timestamps]\nenabled = false\ndestination = \"/var/log/urunc/timestamps.log\"\n\n[monitors.qemu]\ndefault_memory_mb = 512\ndefault_vcpus = 2\npath = \"/usr/bin/qemu-system-x86_64\"\n\n[monitors.firecracker]\ndefault_memory_mb = 256\ndefault_vcpus = 1\npath = \"/usr/local/bin/firecracker\"\n\n[monitors.hvt]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n\n[monitors.spt]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n\n[extra_binaries.virtiofsd]\npath = \"/usr/libexec/virtiofsd\"\noptions = \"--sandbox none\"\n</code></pre>"},{"location":"configuration/#configuration-sections","title":"Configuration Sections","text":""},{"location":"configuration/#log-configuration","title":"Log Configuration","text":"<p>The <code>[log]</code> section controls logging behavior for <code>urunc</code>:</p> Option Type Default Description <code>level</code> string <code>\"info\"</code> Log level. Valid values: <code>\"trace\"</code>, <code>\"debug\"</code>, <code>\"info\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>, <code>\"fatal\"</code>, <code>\"panic\"</code> <code>syslog</code> boolean <code>false</code> Enable syslog output in addition to stderr <p>Example:</p> <pre><code>[log]\nlevel = \"debug\"\nsyslog = true\n</code></pre> <p>Note: The effective log level is determined by both the configuration file and the command-line <code>--debug</code> flag. - If <code>--debug</code> is not specified, the log level from the configuration file is used. - If <code>--debug</code> is specified, the effective log level is the more verbose of:   - the configuration file\u2019s log level, and   - <code>\"debug\"</code> (the level implied by the flag).  </p> <p>For example: - Config: <code>\"warn\"</code>, CLI: <code>--debug</code> \u2192 effective level = <code>\"debug\"</code> - Config: <code>\"trace\"</code>, CLI: <code>--debug</code> \u2192 effective level = <code>\"trace\"</code> - Config: <code>\"error\"</code>, no <code>--debug</code> \u2192 effective level = <code>\"error\"</code> </p>"},{"location":"configuration/#timestamps-configuration","title":"Timestamps Configuration","text":"<p>The <code>[timestamps]</code> section controls timestamp logging for performance monitoring:</p> Option Type Default Description <code>enabled</code> boolean <code>false</code> Enable timestamp logging for performance metrics <code>destination</code> string <code>\"/var/log/urunc/timestamps.log\"</code> File path where timestamps will be written <p>Example:</p> <pre><code>[timestamps]\nenabled = true\ndestination = \"/tmp/urunc-timestamps.log\"\n</code></pre> <p>When enabled, <code>urunc</code> will log performance timestamps to help with debugging and optimization.</p>"},{"location":"configuration/#monitor-configuration","title":"Monitor Configuration","text":"<p>The <code>[monitors]</code> section allows you to configure default settings for different VM/Sandbox monitors. Each monitor is configured as a subsection with its own default values.</p>"},{"location":"configuration/#supported-monitors-and-their-respective-subsection-names","title":"Supported monitors and their respective subsection names","text":"<ul> <li>QEMU/KVM - <code>qemu</code></li> <li>Firecrakcer - <code>firecracker</code></li> <li>Solo5-hvt - <code>hvt</code> - Solo5 hvt (KVM-based tender)</li> <li>Solo5-spt - <code>spt</code> - Solo5 spt (Seccomp-based tender)</li> </ul>"},{"location":"configuration/#monitor-options","title":"Monitor Options","text":"<p>Each monitor subsection supports the following options:</p> Option Type Default Description <code>default_memory_mb</code> integer <code>256</code> Default memory allocation in megabytes <code>default_vcpus</code> integer <code>1</code> Default number of virtual CPUs <code>path</code> string (empty) Optional custom path to the monitor binary. If not specified, urunc will search for the binary in PATH <code>data_path</code> string (empty) Optional custom path for the monitor's data file directory <p>Since Qemu is the only currently supported monitor which requires extra data to boot a VM, <code>urunc</code> wll first check <code>/usr/local/share</code> and then <code>/usr/share</code> for Qemu's data files.</p> <p>Example:</p> <pre><code>[monitors.qemu]\ndefault_memory_mb = 1024\ndefault_vcpus = 4\npath = \"/usr/local/bin/qemu-system-x86_64\"\ndata_path = \"/usr/local/share/\"\n\n[monitors.firecracker]\ndefault_memory_mb = 512\ndefault_vcpus = 2\npath = \"/opt/firecracker/firecracker\"\n</code></pre>"},{"location":"configuration/#extra-binaries-configuration","title":"Extra binaries Configuration","text":"<p>The <code>[extra_binaries]</code> section allows users to configure default settings for different extra binaries to be included in the monitor's container. Each extra binary is configured as a subsection with its own default values.</p>"},{"location":"configuration/#supported-extra-binaries","title":"Supported extra binaries","text":"<ul> <li><code>virtiofsd</code> - vhost-user virtio-fs device backend written in Rust</li> </ul>"},{"location":"configuration/#extra-binaries-options","title":"Extra binaries Options","text":"<p>Each extra binary subsection supports the following options:</p> Option Type Default Description <code>path</code> string (empty) Optional custom path to the extra binary. If not specified, urunc will search for the binary in PATH <code>options</code> string (empty) Optional custom cli options for the extra binary <p>Specifically for <code>virtiofsd</code> the default values are the following:</p> <ul> <li><code>path</code>: <code>/usr/libexec/virtiofsd</code></li> <li><code>options</code>: <code>--cache always --sandbox none</code></li> </ul> <p>Example:</p> <pre><code>[extra_binaries.virtiofsd]\npath = \"/usr/local/bin/virtiofsd\"\noptions = \"--sandbox none --cache always\"\n</code></pre>"},{"location":"configuration/#creating-the-configuration-file","title":"Creating the Configuration File","text":"<p>To create a configuration file, you can:</p> <ol> <li> <p>Create the directory structure:</p> <pre><code>sudo mkdir -p /etc/urunc\n</code></pre> </li> <li> <p>Create the configuration file:</p> </li> </ol> <pre><code>sudo tee /etc/urunc/config.toml &gt; /dev/null &lt;&lt;EOF\n[log]\nlevel = \"info\"\nsyslog = false\n\n[timestamps]\nenabled = false\ndestination = \"/var/log/urunc/timestamps.log\"\n\n[monitors.qemu]\ndefault_memory_mb = 512\ndefault_vcpus = 2\n\n[monitors.firecracker]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n\n[monitors.hvt]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n\n[monitors.spt]\ndefault_memory_mb = 256\ndefault_vcpus = 1\nEOF\n</code></pre>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p><code>urunc</code> will validate the configuration file when it starts. If the configuration file:</p> <ul> <li>Does not exist: <code>urunc</code> uses default values and logs a warning</li> <li>Contains syntax errors: <code>urunc</code> uses default values and logs a warning about the parsing error</li> <li>Contains invalid values: <code>urunc</code> will either use default values for invalid fields or fail to start if critical errors are found</li> </ul>"},{"location":"configuration/#default-values","title":"Default Values","text":"<p>If no configuration file is provided, <code>urunc</code> uses these default values:</p> <pre><code>[log]\nlevel = \"info\"\nsyslog = false\n\n[timestamps]\nenabled = false\ndestination = \"/var/log/urunc/timestamps.log\"\n\n[monitors.qemu]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n# path is not set by default - urunc will search in PATH\n\n[monitors.firecracker]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n# path is not set by default - urunc will search in PATH\n\n[monitors.hvt]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n# path is not set by default - urunc will search in PATH\n\n[monitors.spt]\ndefault_memory_mb = 256\ndefault_vcpus = 1\n# path is not set by default - urunc will search in PATH\n</code></pre>"},{"location":"configuration/#notes","title":"Notes","text":"<ul> <li>The configuration file is only fully loaded during <code>urunc create</code>. The configuration options' values are then stored as Annotations in the <code>state.json</code> file inside the respective container's bundle. For subsequent urunc commands (such as <code>start</code>, <code>kill</code>, etc.), configuration options are loaded from the <code>state.json</code> annotations. In that way, all urunc configuration values except logging configuration and timestamping (see below) remain the same throughout the specific container lifecycle.</li> <li>The configuration file is partially loaded every time urunc is invoked to parse the logging configuration and timestamping options. This way, the user has fine-grained control over the logging level and whether to redirect urunc logs to syslog. Similarly, the user can enable and disable timestamping.</li> </ul>"},{"location":"hypervisor-support/","title":"Supported VMMs and software-based monitors","text":"<p>One of the main goals of <code>urunc</code> is to be a generic OCI unikernel runtime for various unikernel frameworks and similar technologies. In order to achieve that, we want to support as many Virtual Machine Monitors (VMMs) and other types of sandboxing mechanisms such as user-space monitors based on seccomp.</p> <p>In this document, we will go through the current state of <code>urunc</code>'s support for VMMs and monitors that utilize software-based isolation technologies. We will provide a brief description about them, along with installation instructions and a few comments regarding their integration with <code>urunc</code>.</p> <p>Note: In general, <code>urunc</code> expects all supported VM/Sandbox monitors to be available somewhere in the <code>$PATH</code>.</p>"},{"location":"hypervisor-support/#virtual-machine-monitors-vmms","title":"Virtual Machine Monitors (VMMs)","text":"<p>VMMs use hardware-assisted virtualization technologies in order to create a Virtual Machine (VM) where a guest OS will execute. It is one of the most widely used technology for providing strong isolation in multi-tenant environments. For the time being <code>urunc</code> supports 3 types of such VMMs: 1) Qemu, 2) Firecracker and 3) Solo5-hvt.</p>"},{"location":"hypervisor-support/#qemu","title":"Qemu","text":"<p>Qemu (Quick Emulator) is an open-source virtualization platform that enables the emulation of various hardware architectures. By leveraging Linux's KVM, Qemu is able to create VMs and manage their execution.  Some of the biggest advantages of Qemu are the mature and stable interface and codebase. In addition, Qemu supports various paravirtual devices, mostly based on VirtIO and allows the direct use of the host's devices with passthrough.</p>"},{"location":"hypervisor-support/#installing-qemu","title":"Installing Qemu","text":"<p>We can easily install Qemu through almost all package managers. For more details check Qemu's download page. For instance, in the case of Ubuntu, we can simply run the following command: <pre><code>sudo apt-get install qemu-system\n</code></pre></p>"},{"location":"hypervisor-support/#qemu-and-urunc","title":"Qemu and <code>urunc</code>","text":"<p>In the case of Qemu, <code>urunc</code> makes use of its <code>virtio-net</code> device to provide network support for the unikernel through a tap device. In addition, <code>urunc</code> can leverage Qemu's initrd option in order to provide the Unikernel with an initial RamFS (initramfs). However, Qemu supports various ways to provide storage in VMs such as block devices through virtio-blk, shared-fs through 9pfs, virtiofs and initramfs.</p> <p>We plan to add support for all the above options, but as previously mentioned only Initramfs is supported for the time being.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Unikraft</li> <li>MirageOS</li> <li>Mewz</li> <li>Linux</li> </ul> <p>An example unikernel:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n</code></pre>"},{"location":"hypervisor-support/#aws-firecracker","title":"AWS Firecracker","text":"<p>AWS Firecracker is an open-source virtualization technology developed by Amazon Web Services (AWS) that is designed to run serverless workloads efficiently. Firecracker provides a minimalist VMM, allowing the creation of lightweight virtual machines, called microVMs, that are faster and more resource-efficient than traditional VMs. In contrast with Qemu, Firecracker aims to provide a smaller set of devices for the VMs. The main benefit of Firecracker comes from its fast VM instantiation and guest OS boot.</p>"},{"location":"hypervisor-support/#installing-firecracker","title":"Installing Firecracker","text":"<p>Firecracker is not available through a package manger, but it can easily be installed. The Getting Started guide of Firecracker describes how users can set up Firecracker. Long story short, we can fetch a Firecracker binary with the following commands:</p> <pre><code>ARCH=\"$(uname -m)\" \nVERSION=\"v1.7.0\"\nrelease_url=\"https://github.com/firecracker-microvm/firecracker/releases\"\ncurl -L ${release_url}/download/${VERSION}/firecracker-${VERSION}-${ARCH}.tgz | tar -xz\n# Rename the binary to \"firecracker\"\nsudo mv release-${VERSION}-$(uname -m)/firecracker-${VERSION}-${ARCH} /usr/local/bin/firecracker\nrm -fr release-${VERSION}-$(uname -m)\n</code></pre> <p>It is important to note that <code>urunc</code> expects to find the <code>firecracker</code> binary located in the <code>$PATH</code> and named <code>firecracker</code>.</p> <p>Note: Since only Unikraft can boot on top of Firecracker (from the supported unikernels in <code>urunc</code>) we use the v1.7.0 version of Firecracker, due to some booting issues of Unikraft in newer versions.</p>"},{"location":"hypervisor-support/#firecracker-and-urunc","title":"Firecracker and <code>urunc</code>","text":"<p>In the case of Firecracker, <code>urunc</code> makes use of its <code>virtio-net</code> device to provide network support for the unikernel though a tap device. In addition, <code>urunc</code> can leverage Firecracker's initrd option in order to provide the Unikernel with an initial RamFS (initramfs). Firecracker does not support shared-fs between the host and the guest. However, it does provide support for virtio-block.</p> <p>We plan to add support for virtio-block, but as previously mentioned only Initramfs is supported for the time being.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Unikraft</li> <li>Linux</li> </ul> <p>An example unikernel:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n</code></pre>"},{"location":"hypervisor-support/#solo5-hvt","title":"Solo5-hvt","text":"<p>Solo5-hvt is a lightweight, high-performance VMM designed to run unikernels in a virtualized environment. As a part of the broader Solo5 project, Solo5-hvt provides a minimal, efficient abstraction layer for running unikernels on modern hardware, leveraging hardware virtualization technologies Some of the key benefits of Solo5-hvt is its simplicity and and extremely fast boot times of unikernels. In contrast to the other VMMs, Solo5-hvt does not provide support for virtIO devices. Instead, it defines its own interface, which can be used for network and block I/O.</p>"},{"location":"hypervisor-support/#installing-solo5-hvt","title":"Installing Solo5-hvt","text":"<p>Solo5 can be installed by building from source. However, in order to do that, we will need a few packages.</p> <pre><code>sudo apt install libseccomp-dev pkg-config build-essential\n</code></pre> <p>Next, we can clone and build <code>solo5-hvt</code>.</p> <pre><code>git clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh &amp;&amp; make -j$(nproc)\n</code></pre> <p>It is important to note that <code>urunc</code> expects to find the <code>solo5-hvt</code> binary located in the <code>$PATH</code> and named as <code>solo5-hvt</code>. Therefore, to install it:</p> <pre><code>sudo cp tenders/hvt/solo5-hvt /usr/local/bin\n</code></pre>"},{"location":"hypervisor-support/#solo5-hvt-and-urunc","title":"Solo5-hvt and <code>urunc</code>","text":"<p>In the case of Solo5-hvt, <code>urunc</code> supports all the devices and utilizes a tap device to provide network in the unikernel. For the storage part, <code>urunc</code> supports the block storage interface of Solo5-hvt, which can be used in two ways, either with a block image inside the container image, or using the devmapper as a snapshotter.</p> <p>In the first case, we copy inside the container image a block image that contains all the data we want to pass in the unikernel.</p> <p>In the second case, we copy directly all the files we want the unikernel to access inside the container's image. Using devmapper <code>urunc</code> will use the container's image snapshot as a block image for the unikernel. It is important to note that the unikernel framework must support the respective filesystem type (e.g. ext2/3/4). This is the case for Rumprun unikernel.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Rumprun</li> <li>MirageOS</li> </ul> <p>An example unikernel with a block image inside the conntainer's rootfs:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest\n</code></pre>"},{"location":"hypervisor-support/#software-based-isolation-monitors","title":"Software-based isolation monitors","text":"<p>Except for the traditional VM-based isolation solutions, there are other solutions which provide isolation using software-based technologies too. In that case the monitor interacts with a user-space kernel on top of which the application is running. The user-space kernel intercepts or defines a set of system calls and then forwards them to the monitor. To further strengthen security, it is common to use seccomp filters to limit the exposure of the host OS to the monitor.</p> <p>A well-known example of such a technology is gVisor. Unfortunately, gVisor does not support the execution of any unikernel framework.</p>"},{"location":"hypervisor-support/#solo5-spt","title":"Solo5-spt","text":"<p>In a similar way, Solo5-spt is a specialized backend for the Solo5 project, designed to run unikernels in systems that do not have access to hardware-assisted virtualization technologies. Solo5-spt executes a unikernel monitor with a seccomp filter allowing only seven system calls. The unikernel running on top of Solo5-spt interacts with this monitor through a similar interface with Solo5-hvt, facilitating network and block storage I/O. Solo5-spt can provide extremely fast instantiation times, very small overhead, along with performant execution.</p>"},{"location":"hypervisor-support/#installing-solo5-spt","title":"Installing Solo5-spt","text":"<p>The installation process of Solo5-spt is similar with the Solo5-hvt one. In fact, both projects share the same repository. Hence we can follow the same steps as in Solo5-hvt. At first, make sure to install the necessary packages.</p> <pre><code>sudo apt install libseccomp-dev pkg-config build-essential\n</code></pre> <p>Next, we can clone and build <code>solo5-spt</code>.</p> <pre><code>git clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh &amp;&amp; make -j$(nproc)\n</code></pre> <p>It is important to note that <code>urunc</code> expects to find the <code>solo5-spt</code> binary located in the <code>$PATH</code> and named <code>solo5-spt</code>. Therefore, to install it:</p> <pre><code>sudo cp tenders/spt/solo5-spt /usr/local/bin\n</code></pre>"},{"location":"hypervisor-support/#solo5-spt-and-urunc","title":"Solo5-spt and <code>urunc</code>","text":"<p>Similarly with Solo5-hvt, <code>urunc</code> supports all the devices of Solo5-spt. For more information take a look at the respective Solo5-hvt section.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Rumprun</li> <li>MirageOS</li> </ul> <p>An example unikernel which utilizes devmapper for block storage:</p> <pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun-raw:latest\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>This document outlines the steps required to set up an environment for running both standard containers and <code>urunc</code>-based ones. It also includes the installation of all <code>urunc</code> supported VM/Sandbox monitors.</p> <p>Although the instructions assume a vanilla Ubuntu 22.04 system, <code>urunc</code> is compatible with a variety of Linux distributions.</p> <p>The installation guide is split in three parts:</p> <p>1. Installation of common container tools and containerd configuration</p> <p>This step involves the installation and configuration of several essential components for a fully functional and reliable container environment. Specifically:</p> <ul> <li>runc</li> <li>containerd</li> <li>CNI plugins</li> <li>nerdctl</li> <li>devmapper and/or blockfile</li> </ul> <p>2. Installation of all supported monitors and additional tools</p> <p>This step installs all currently supported monitors of <code>urunc</code>, along with virtiofsd. Specifically:</p> <ul> <li>solo5-{hvt|spt}</li> <li>qemu</li> <li>firecracker</li> <li>virtiofsd</li> </ul> <p>3. Installation and configuration of <code>urunc</code></p> <p>In the last step the installation of <code>urunc</code> will take place. This step also provides information on how to build <code>urunc</code> from source using Go 1.24.6</p> <p>Let's go.</p> <p>Note: Some of these steps may override existing tools or services. Please make sure to keep backups of any critical configurations.</p>"},{"location":"installation/#step-1-install-container-components-and-tools-and-containerd-configuration","title":"Step 1: Install container components and tools and containerd configuration","text":"<p>This section covers the installation of necessary container components and the configuration of <code>containerd</code>.  If a functioning container setup with the required tools is already present, this step can be skipped.</p>"},{"location":"installation/#install-runc-or-any-other-generic-low-level-container-runtime","title":"Install runc or any other generic low level container runtime","text":"<p>In Kubernetes environments, <code>urunc</code> delegates the management of normal containers (such as pause and sidecar containers) to a typical low-level container runtime like <code>runc</code>,<code>crun</code>, <code>youki</code> etc. For the runc installation either follow the instructions in <code>runc</code>'s repository or download the latest release with the following commands:</p> <pre><code>RUNC_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/opencontainers/runc/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/opencontainers/runc/releases/download/v$RUNC_VERSION/runc.$(dpkg --print-architecture)\nsudo install -m 755 runc.$(dpkg --print-architecture) /usr/local/sbin/runc\nrm -f ./runc.$(dpkg --print-architecture)\n</code></pre>"},{"location":"installation/#install-containerd","title":"Install containerd","text":"<p>For the time being, <code>urunc</code> has been properly tested with containerd as the high-level container runtime. For installation methods or other information, please check containerd's Getting Started guide. The following commands download and install the latest release.</p> <pre><code>CONTAINERD_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/containerd/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containerd/containerd/releases/download/v$CONTAINERD_VERSION/containerd-$CONTAINERD_VERSION-linux-$(dpkg --print-architecture).tar.gz\nsudo tar Cxzvf /usr/local containerd-$CONTAINERD_VERSION-linux-$(dpkg --print-architecture).tar.gz\nrm -f containerd-$CONTAINERD_VERSION-linux-$(dpkg --print-architecture).tar.gz\n</code></pre>"},{"location":"installation/#install-containerd-service","title":"Install containerd service","text":"<p>To configure containerd to start automatically on system boot, set up the corresponding systemd service:</p> <pre><code>CONTAINERD_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/containerd/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://raw.githubusercontent.com/containerd/containerd/v$CONTAINERD_VERSION/containerd.service\nsudo rm -f /lib/systemd/system/containerd.service\nsudo mv containerd.service /lib/systemd/system/containerd.service\nsudo systemctl daemon-reload\nsudo systemctl enable --now containerd\n</code></pre>"},{"location":"installation/#install-cni-plugins","title":"Install CNI plugins","text":"<p>For container networking the CNI plugins are necessary. The following commands download and install in <code>/opt/cni/bin</code> the latest release.</p> <pre><code>CNI_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containernetworking/plugins/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containernetworking/plugins/releases/download/v$CNI_VERSION/cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nrm -f cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\n</code></pre>"},{"location":"installation/#configure-containerd","title":"Configure containerd","text":"<p>In case <code>containerd</code>'s configuration is missing, the default one can be created with the following commands:</p> <pre><code>sudo mkdir -p /etc/containerd/\nsudo mv /etc/containerd/config.toml /etc/containerd/config.toml.bak # There might be no existing configuration.\nsudo containerd config default | sudo tee /etc/containerd/config.toml\nsudo systemctl restart containerd\n</code></pre> <p>To easily migrate containerd's configuration from older to newer versions execute <code>sudo containerd config migrate &gt; /etc/containerd/config.toml</code></p>"},{"location":"installation/#block-based-snapshotters","title":"Block-based snapshotters","text":"<p><code>urunc</code> can leverage block-based snapshots to treat a container's snapshot as a block device for a guest. Currently, <code>urunc</code> has been tested and verified with devmapper and blockfile. Devmapper uses a thinpool for flexible management, while blockfile uses on a pre-allocated scratch file, though it lacks ext2 support and thus it is not compatible with Rumprun unikernels.</p>"},{"location":"installation/#setting-and-configuring-devmapper","title":"Setting and configuring devmapper","text":"<p>To configure the devmapper thinpool, <code>urunc</code> repository contains two helper scripts under the script directory. The first dm_create.sh creates a thinpool, while the second dm_relosd.sh reloads the same thinpool that has been created from dm_create.sh.</p> <p>Note: The scripts use the <code>bc</code> tool, which needs to be installed.</p> <p>To install the scripts:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\nsudo mkdir -p /usr/local/bin/scripts\nsudo mkdir -p /usr/local/lib/systemd/system/\nsudo cp urunc/script/dm_create.sh /usr/local/bin/scripts/dm_create.sh\nsudo cp urunc/script/dm_reload.sh /usr/local/bin/scripts/dm_reload.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_create.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_reload.sh\n</code></pre> <p>To create the thinpool:</p> <pre><code>sudo /usr/local/bin/scripts/dm_create.sh\n</code></pre> <p>However, when the system reboots, the thinpool needs to get reloaded:</p> <pre><code>sudo /usr/local/bin/scripts/dm_reload.sh\n</code></pre> <p>Alternatively, a systemd service can automatically reload the existing thinpool when a system reboots. The <code>urunc</code> repository contains such a service</p> <pre><code>sudo cp urunc/script/dm_reload.service /usr/local/lib/systemd/system/dm_reload.service\nsudo chmod 644 /usr/local/lib/systemd/system/dm_reload.service\nsudo chown root:root /usr/local/lib/systemd/system/dm_reload.service\nsudo systemctl daemon-reload\nsudo systemctl enable dm_reload.service\n</code></pre> <p>At last, update the containerd configuration for devmapper:</p> <ul> <li>In containerd v2.x find and change or append the following lines:</li> </ul> <pre><code>[plugins.'io.containerd.snapshotter.v1.devmapper']\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\n</code></pre> <ul> <li>In containerd v1.x find and change or append the following lines:</li> </ul> <pre><code>[plugins.\"io.containerd.snapshotter.v1.devmapper\"]\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\n</code></pre> <p>After updating the configuration, containerd needs to restart:</p> <pre><code>sudo systemctl restart containerd\n</code></pre> <p>Verify that the devmapper snapshotter is properly configured with:</p> <pre><code>$ sudo ctr plugin ls | grep devmapper\nio.containerd.snapshotter.v1              devmapper                linux/amd64    ok\n</code></pre>"},{"location":"installation/#setting-and-configuring-blockfile","title":"Setting and configuring blockfile","text":"<p>THe first stpe of setting up <code>blockfile</code> is the creation of the scratch file, which will be used from the snapshotter:</p> <pre><code>sudo mkdir -p /opt/containerd/blockfile\nsudo dd if=/dev/zero of=/opt/containerd/blockfile/scratch bs=1M count=500\nsudo mkfs.ext4 /opt/containerd/blockfile/scratch\nsudo chown -R root:root /opt/containerd/blockfile\n</code></pre> <p>After the creation of the scratch file, update the containerd's configuration for the blockfile snapshotter:</p> <ul> <li>In containerd v2.x find and change or append the following lines:</li> </ul> <pre><code>[plugins.'io.containerd.snapshotter.v1.blockfile']\n  fs_type = \"ext4\"\n  mount_options = []\n  recreate_scratch = true\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.blockfile\"\n  scratch_file = \"/opt/containerd/blockfile/scratch\"\n  supported_platforms = [\"linux/amd64\"]\n</code></pre> <ul> <li>In containerd v1.x find and change or append the following lines:</li> </ul> <pre><code>[plugins.\"io.containerd.snapshotter.v1.blockfile\"]\n  fs_type = \"ext4\"\n  mount_options = []\n  recreate_scratch = true\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.blockfile\"\n  scratch_file = \"/opt/containerd/blockfile/scratch\"\n  supported_platforms = [\"linux/amd64\"]\n</code></pre> <p>Blockfile configuration options:</p> <ul> <li><code>root_path</code>: Directory for storing block files (must be writable by containerd).</li> <li><code>fs_type</code>: Filesystem type for block files (supported: ext4)</li> <li><code>scratch_file</code>: The path to the empty file that will be used as the base for the block files.</li> <li><code>recreate_scratch</code>: If set to true, the snapshotter will recreate the scratch file if it is missing.</li> </ul> <p>After updating the configuration, containerd needs to restart:</p> <pre><code>sudo systemctl restart containerd\n</code></pre> <p>Verify that the blockfile snapshotter is properly configured with:</p> <pre><code>$ sudo ctr plugin ls | grep blockfile\n   io.containerd.snapshotter.v1           blockfile               linux/amd64    ok\n</code></pre>"},{"location":"installation/#install-nerdctl","title":"Install nerdctl","text":"<p>To easily interact with containerd, nerdctl offers a Dockerompatible CLI exprerience. The following commands download and install the latest release.</p> <pre><code>NERDCTL_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/nerdctl/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containerd/nerdctl/releases/download/v$NERDCTL_VERSION/nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nsudo tar Cxzvf /usr/local/bin nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nrm -f nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\n</code></pre>"},{"location":"installation/#step-2-install-all-supported-monitors","title":"Step 2: Install all supported monitors","text":"<p>This section includes the installation of all supported monitors and <code>virtiofsd</code>. For ease of use, the monitors-build repository contains releases with various versions of these monitors and tools. Alternatively, each monitor can be downloaded and installed following the respective installtion guide.</p>"},{"location":"installation/#option-1-using-the-monitors-build-repository","title":"Option 1: Using the monitors-build repository","text":"<p>The monitor-builds repository provides a reference setup for building and distributing static binaries of monitors and tools for <code>urunc</code>. In the releases page there are archives with all monitor artifacts for specific versions. However, users can request the creation of a new release with different versions. More information can be found in the respective section of the repository's README file.</p> <p>As an example, the following commands use the <code>FC-v1.7.0_S5-v0.9.3_VFS_-v1.13.0_QM-v10.1.1-9a44e</code> release which contains the following monitors and tools in the specified versions:</p> <ul> <li>Firecracker v1.7.0</li> <li>Solo5 v0.9.3</li> <li>Virtiofsd v1.13.0</li> <li>Qemu v10.1.1</li> </ul> <p>To download and install the monitors in <code>/tmp</code>: <pre><code>wget https://github.com/urunc-dev/monitors-build/releases/download/FC-v1.7.0_S5-v0.9.3_VFS_-v1.13.0_QM-v10.1.1-9a44e/release-amd64-FC-v1.7.0_S5-v0.9.3_VFS_-v1.13.0_QM-v10.1.1-9a44e.tar.gz\nsudo tar Cxzvf /opt release-amd64-FC-v1.7.0_S5-v0.9.3_VFS_-v1.13.0_QM-v10.1.1-9a44e.tar.gz\nrm -f release-amd64-FC-v1.7.0_S5-v0.9.3_VFS_-v1.13.0_QM-v10.1.1-9a44e.tar.gz\n</code></pre></p> <p>After downloading all the binaries, we need to instruct <code>urunc</code> about the location of the binaries. Therefore, in <code>urunc</code>'s configuration. there are three fields that need to get updated:</p> <ol> <li>in each monitor the <code>path</code> field,</li> <li>in Qemu, the <code>data_path</code> field,</li> <li>in Virtiofsd, the <code>path</code> field needs to get updated.</li> </ol> <p>Therefore, change or append the following lines in <code>urunc</code>'s configuration:</p> <pre><code>[monitors.qemu]\npath = \"/opt/urunc/bin/qemu-system-x86_64\"\ndata_path = \"/opt/urunc/share/qemu\"\n\n[monitors.firecracker]\npath = \"/opt/urunc/bin/firecracker\"\n\n[monitors.hvt]\npath = \"/opt/urunc/bin/solo5-hvt\"\n\n[monitors.spt]\npath = \"/opt/urunc/bin/solo5-spt\"\n\n[extra_binaries.virtiofsd]\npath = \"/opt/urunc/bin/virtiofsd\"\n</code></pre>"},{"location":"installation/#option-2-fetching-or-building-from-source","title":"Option 2: Fetching or building from source","text":"<p>Alternatively, each monitor can be simply downloaded or built from source.</p>"},{"location":"installation/#solo5","title":"Solo5","text":"<p>In the case of Solo5, there is only the option of building from source cloning the  respective repository and using the commands below.</p> <pre><code>git clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh  &amp;&amp; make -j$(nproc)\nsudo cp tenders/hvt/solo5-hvt /usr/local/bin\nsudo cp tenders/spt/solo5-spt /usr/local/bin\n</code></pre> <p>NOTE: For the <code>solo5-spt</code> monitor <code>libseccomp-dev</code> is necessary.</p>"},{"location":"installation/#qemu","title":"Qemu","text":"<p>Qemu is a popular VMM and emulator which is available as apcakge from the vast majority of Linux distributions. In apt-based distributions:</p> <pre><code>sudo apt install qemu-system\n</code></pre>"},{"location":"installation/#firecracker","title":"Firecracker","text":"<p>firecracker provides releases with statically-built binaries. Due to some issues of Unikraft with newer versions of Firecracker, the following commands install version 1.7.0:</p> <pre><code>ARCH=\"$(uname -m)\"\nVERSION=\"v1.7.0\"\nrelease_url=\"https://github.com/firecracker-microvm/firecracker/releases\"\ncurl -L ${release_url}/download/${VERSION}/firecracker-${VERSION}-${ARCH}.tgz | tar -xz\n# Rename the binary to \"firecracker\"\nsudo mv release-${VERSION}-${ARCH}/firecracker-${VERSION}-${ARCH} /usr/local/bin/firecracker\nrm -fr release-${VERSION}-${ARCH}\n</code></pre>"},{"location":"installation/#virtiofsd","title":"Virtiofsd","text":"<p>As an alternative to 9pfs, <code>urunc</code> can configure Qemu to use virtiofs. To do that, <code>virtiofsd</code> is necessary. By default <code>urunc</code> searches for the <code>virtiofsd</code> binary under <code>/usr/libexec</code>. However, the path for virtiofsd can be set in the respective section of <code>urunc</code>'s configuration The following commands download and install the latest release from the gitlab repository of virtiofsd:</p> <pre><code>wget https://gitlab.com/-/project/21523468/uploads/0298165d4cd2c73ca444a8c0f6a9ecc7/virtiofsd-v1.13.2.zip\nunzip virtiofsd-v1.13.2.zi\nsudo mv target/x86_64-unknown-linux-musl/release/virtiofsd /usr/libexec/\nrm -rf target virtiofsd-v1.13.2.zip\n</code></pre>"},{"location":"installation/#step-3-install-urunc-and-configure-containerd","title":"Step 3: Install urunc and configure containerd","text":""},{"location":"installation/#installing-urunc","title":"Installing urunc","text":"<p>To install <code>urunc</code>, there are three options:</p> <ol> <li>building from source,</li> <li>grabbing the binaries from the latest release, or</li> <li>grabbing the binaries from the lastest commit in main.</li> </ol>"},{"location":"installation/#option-1-building-from-source","title":"Option 1: Building from source","text":"<p>In order to build <code>urunc</code> from source, any earlier version of Go 1.20.6 should be sufficient, Let's download Go 1.24.6</p> <pre><code>GO_VERSION=1.24.6\nwget -q https://go.dev/dl/go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\nsudo mkdir /usr/local/go${GO_VERSION}\nsudo tar -C /usr/local/go${GO_VERSION} -xzf go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\nsudo tee -a /etc/profile &gt; /dev/null &lt;&lt; EOT\nexport PATH=\\$PATH:/usr/local/go$GO_VERSION/go/bin\nEOT\nrm -f go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\n</code></pre> <p>Note: A re-login to the shell might be necessary for the <code>PATH</code> update.</p> <p>With Go installed, building and installing <code>urunc</code> is as easy as executing the following commands:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\ncd urunc\nmake &amp;&amp; sudo make install\ncd ..\n</code></pre>"},{"location":"installation/#option-2-install-latest-release","title":"Option 2: Install latest release","text":"<p>Alternatively, to get the latest release:</p> <pre><code>URUNC_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/urunc-dev/urunc/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nURUNC_BINARY_FILENAME=\"urunc_static_v${URUNC_VERSION}_$(dpkg --print-architecture)\"\nwget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/$URUNC_BINARY_FILENAME\nchmod +x $URUNC_BINARY_FILENAME\nsudo mv $URUNC_BINARY_FILENAME /usr/local/bin/urunc\n</code></pre> <p>And for <code>containerd-shim-urunc-v2</code>:</p> <pre><code>CONTAINERD_BINARY_FILENAME=\"containerd-shim-urunc-v2_static_v${URUNC_VERSION}_$(dpkg --print-architecture)\"\nwget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/$CONTAINERD_BINARY_FILENAME\nchmod +x $CONTAINERD_BINARY_FILENAME\nsudo mv $CONTAINERD_BINARY_FILENAME /usr/local/bin/containerd-shim-urunc-v2\n</code></pre>"},{"location":"installation/#option-3-install-from-latest-artifacts-tip-of-the-main-branch","title":"Option 3: Install from latest artifacts (tip of the main branch)","text":"<p>Alternatively, to get a <code>urunc</code> binary based on the main branch:</p> <pre><code>URUNC_VERSION=main\nURUNC_BINARY_FILENAME=\"urunc_static_$(dpkg --print-architecture)\"\nwget -q https://s3.nbfc.io/nbfc-assets/github/urunc/dist/$URUNC_VERSION/$(dpkg --print-architecture)/$URUNC_BINARY_FILENAME\nchmod +x $URUNC_BINARY_FILENAME\nsudo mv $URUNC_BINARY_FILENAME /usr/local/bin/urunc\n</code></pre> <p>And for <code>containerd-shim-urunc-v2</code>:</p> <pre><code>CONTAINERD_BINARY_FILENAME=\"containerd-shim-urunc-v2_static_$(dpkg --print-architecture)\"\nwget -q https://s3.nbfc.io/nbfc-assets/github/urunc/dist/$URUNC_VERSION/$(dpkg --print-architecture)/$CONTAINERD_BINARY_FILENAME\nchmod +x $CONTAINERD_BINARY_FILENAME\nsudo mv $CONTAINERD_BINARY_FILENAME /usr/local/bin/containerd-shim-urunc-v2\n</code></pre>"},{"location":"installation/#add-urunc-runtime-to-containerd","title":"Add urunc runtime to containerd","text":"<p>At last, <code>urunc</code> needs to be configured as a runtime in containerd. Make sure to set the chosen snapshotter (<code>devmapper</code> or <code>blockfile</code>).</p> <ul> <li>In containerd v2.x find and change or append the following lines:</li> </ul> <pre><code>[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.urunc]\n    runtime_type = \"io.containerd.urunc.v2\"\n    container_annotations = [\"com.urunc.unikernel.*\"]\n    pod_annotations = [\"com.urunc.unikernel.*\"]\n    snapshotter = \"devmapper\"\n</code></pre> <ul> <li>In containerd v1.x find and change or append the following lines:</li> </ul> <pre><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.urunc]\n    runtime_type = \"io.containerd.urunc.v2\"\n    container_annotations = [\"com.urunc.unikernel.*\"]\n    pod_annotations = [\"com.urunc.unikernel.*\"]\n    snapshotter = \"devmapper\"\n</code></pre> <p>After updating the configuration, containerd needs to restart:</p> <pre><code>sudo systemctl restart containerd\n</code></pre>"},{"location":"installation/#run-example-unikernels","title":"Run example unikernels","text":"<p>Now, let's run some unikernels for every VM/Sandbox monitor, to make sure everything was installed correctly.</p>"},{"location":"installation/#run-a-redis-rumprun-unikernel-over-solo5-hvt","title":"Run a Redis Rumprun unikernel over Solo5-hvt","text":"<pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest\n</code></pre>"},{"location":"installation/#run-a-redis-rumprun-unikernel-over-solo5-spt-with-devmapper","title":"Run a Redis rumprun unikernel over Solo5-spt with devmapper","text":"<pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun-raw:latest\n</code></pre>"},{"location":"installation/#run-a-nginx-unikraft-unikernel-over-qemu","title":"Run a Nginx Unikraft unikernel over Qemu","text":"<pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n</code></pre>"},{"location":"installation/#run-a-nginx-unikraft-unikernel-over-firecracker","title":"Run a Nginx Unikraft unikernel over Firecracker","text":"<pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This document acts as a quickstart guide to showcase <code>urunc</code> features. Please refer to the installation guide for more detailed installation instructions, or the design document for more details regarding <code>urunc</code>'s architecture.</p> <p>We can quickly set <code>urunc</code> either with docker or containerd and nerdctl. We assume a vanilla ubuntu 22.04 environment, although <code>urunc</code> is able to run on a number of GNU/Linux distributions.</p>"},{"location":"quickstart/#using-docker","title":"Using Docker","text":"<p>The easiest and fastest way to try out <code>urunc</code> would be with <code>docker</code> Before doing so, please make sure that the host system satisfies the following dependencies:</p> <ul> <li>Docker</li> <li>Qemu</li> <li><code>urunc</code> and <code>containerd-shim-urunc-v2</code> binaries.</li> </ul>"},{"location":"quickstart/#install-docker","title":"Install Docker","text":"<p>At first we need docker.</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nrm get-docker.sh\nsudo groupadd docker # The group might already exist\nsudo usermod -aG docker $USER\n</code></pre> <p>Note: Please logout and log back in from the shell, in order to be able to use docker without sudo</p>"},{"location":"quickstart/#install-urunc-from-source","title":"Install <code>urunc</code> from source","text":"<p>Then we need <code>urunc</code>:</p> <pre><code>sudo apt install -y git make\ngit clone https://github.com/urunc-dev/urunc.git\ndocker run --rm -ti -v $PWD/urunc:/urunc -w /urunc golang:1.24 bash -c \"git config --global --add safe.directory /urunc &amp;&amp; make\"\nsudo make -C urunc install\n</code></pre>"},{"location":"quickstart/#a-docker-example","title":"A docker example","text":"<p>We will try out a Unikraft unikernel over Qemu.</p>"},{"location":"quickstart/#install-qemu","title":"Install Qemu","text":"<p>Let's make sure that Qemu is installed : <pre><code>sudo apt install -y qemu-system\n</code></pre></p>"},{"location":"quickstart/#run-the-unikernel","title":"Run the unikernel","text":"<p>Now we are ready to run Nginx as a Unikraft unikernel using docker and <code>urunc</code>:</p> <pre><code>$ docker run --rm -d --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n67bec5ab9a748e35faf7c2079002177b9bdc806220e59b6b413836db1d6e4018\n</code></pre> <p>We can inspect the container and get its IP address:</p> <pre><code>$ docker inspect 67bec5ab9a748e35faf7c2079002177b9bdc806220e59b6b413836db1d6e4018 | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre> <p>At last we can curl the Nginx server running inside Unikraft with:</p> <pre><code>$ curl 172.17.0.2\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Hello, world!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Hello, world!&lt;/h1&gt;\n  &lt;p&gt;Powered by &lt;a href=\"http://unikraft.org\"&gt;Unikraft&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"quickstart/#using-containerd-and-nerdctl","title":"Using containerd and nerdctl","text":"<p>The second way to quickly start with <code>urunc</code> would be by setting up a high-level container runtime (e.g. containerd) and using nerdctl.</p>"},{"location":"quickstart/#install-a-high-level-container-runtime","title":"Install a high-level container runtime","text":"<p>First step is to install containerd and setup basic functionality (the <code>CNI</code> plugins and a snapshotter).</p> <p>If a tool is already installed, skip to the next step.</p>"},{"location":"quickstart/#install-and-configure-containerd","title":"Install and configure containerd","text":"<p>We will install containerd from the package manager:</p> <pre><code>sudo apt install containerd\n</code></pre> <p>In this way we will also install <code>runc</code>, but not the necessary CNI plugins. However, before proceeding to CNI plugins, we will generate the default configuration for containerd.</p> <pre><code>sudo mkdir -p /etc/containerd/\nsudo mv /etc/containerd/config.toml /etc/containerd/config.toml.bak # There might be no configuration\nsudo containerd config default | sudo tee /etc/containerd/config.toml\nsudo systemctl restart containerd\n</code></pre>"},{"location":"quickstart/#install-cni-plugins","title":"Install CNI plugins","text":"<pre><code>CNI_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containernetworking/plugins/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containernetworking/plugins/releases/download/v$CNI_VERSION/cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nrm -f cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\n</code></pre>"},{"location":"quickstart/#setup-thinpool-devmapper","title":"Setup thinpool devmapper","text":"<p>In order to make use of directly passing the container's snapshot as block device in the unikernel, we will need to setup the devmapper snapshotter. We can do that by first creating a thinpool, using the respective scripts in <code>urunc</code>'s repo</p> <pre><code>wget -q https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/script/dm_create.sh\nwget -q https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/script/dm_reload.sh\nsudo mkdir -p /usr/local/bin/scripts\nsudo mv dm_create.sh /usr/local/bin/scripts/dm_create.sh\nsudo mv dm_reload.sh /usr/local/bin/scripts/dm_reload.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_create.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_reload.sh\nsudo /usr/local/bin/scripts/dm_create.sh\n</code></pre> <p>Note: The above instructions will create the thinpool, but in case of reboot, you will need to reload it running the <code>dm_reload.sh</code> script. Otherwise check the installation guide for creating a service. </p> <p>At last, we need to modify containerd configuration for the new demapper snapshotter:</p> <ul> <li>In containerd v2.x:</li> </ul> <pre><code>sudo sed -i \"/\\[plugins\\.'io\\.containerd\\.snapshotter\\.v1\\.devmapper'\\]/,/^$/d\" /etc/containerd/config.toml\nsudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;'EOT'\n\n# Customizations for devmapper\n\n[plugins.'io.containerd.snapshotter.v1.devmapper']\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <ul> <li>In containerd v1.x:</li> </ul> <pre><code>sudo sed -i '/\\[plugins\\.\"io\\.containerd\\.snapshotter\\.v1\\.devmapper\"\\]/,/^$/d' /etc/containerd/config.toml\nsudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;'EOT'\n\n# Customizations for devmapper\n\n[plugins.\"io.containerd.snapshotter.v1.devmapper\"]\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <p>Let's verify that the new snapshotter is properly configured:</p> <pre><code>$ sudo ctr plugin ls | grep devmapper\nio.containerd.snapshotter.v1           devmapper                linux/amd64    ok\n</code></pre>"},{"location":"quickstart/#install-nerdctl","title":"Install nerdctl","text":"<p>After installing containerd a nifty tool like nerdctl is useful to get a realistic experience.</p> <pre><code>NERDCTL_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/nerdctl/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containerd/nerdctl/releases/download/v$NERDCTL_VERSION/nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nsudo tar Cxzvf /usr/local/bin nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nrm -f nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\n</code></pre>"},{"location":"quickstart/#install-urunc-from-its-latest-release","title":"Install <code>urunc</code> from its latest release","text":"<p>At last, but not least, we will install <code>urunc</code> from its latest release. At first, we will install the <code>urunc</code> binary:</p> <pre><code>URUNC_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/urunc-dev/urunc/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/urunc_$(dpkg --print-architecture)\nchmod +x urunc_$(dpkg --print-architecture)\nsudo mv urunc_$(dpkg --print-architecture) /usr/local/bin/urunc\n</code></pre> <p>Secondly, we will install the <code>containerd-shim-urunc-v2</code> binary:.</p> <pre><code>wget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/containerd-shim-urunc-v2_$(dpkg --print-architecture)\nchmod +x containerd-shim-urunc-v2_$(dpkg --print-architecture)\nsudo mv containerd-shim-urunc-v2_$(dpkg --print-architecture) /usr/local/bin/containerd-shim-urunc-v2\n</code></pre>"},{"location":"quickstart/#a-nerdctl-containerd-example","title":"A nerdctl-containerd example","text":"<p>We will try out a Rumprun unikernel running over Solo5-hvt with nerdctl.</p>"},{"location":"quickstart/#install-solo5","title":"Install solo5","text":"<p>Lets install <code>solo5-hvt</code>:</p> <pre><code>sudo apt install make gcc pkg-config libseccomp-dev\ngit clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh  &amp;&amp; make -j$(nproc)\nsudo cp tenders/hvt/solo5-hvt /usr/local/bin\n</code></pre>"},{"location":"quickstart/#run-the-unikernel_1","title":"Run the Unikernel!","text":"<p>Now, let's run a Redis unikernel on top of Rumprun and solo5-hvt:</p> <pre><code>sudo nerdctl run -d --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-raw:latest\n</code></pre> <p>We can inspect the running container to check it's IP address:</p> <pre><code>$ sudo nerdctl ps \nCONTAINER ID    IMAGE                                                      COMMAND        CREATED           STATUS    PORTS    NAMES\n8a415b278a9e    harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-raw:latest    \"redis-server /data/\u2026\"    18 seconds ago    Up                 redis-hvt-rumprun-8a415\n$ sudo nerdctl inspect 8a415b278a9e | grep IPAddress\n            \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"172.16.1.2\",\n</code></pre> <p>and we can interact with the redis unikernel:</p> <pre><code>$ telnet 10.4.0.2 6379\nTrying 10.4.0.2...\nConnected to 10.4.0.2.\nEscape character is '^]'.\nping\n+PONG\nquit\n+OK\nConnection closed by foreign host.\n</code></pre>"},{"location":"unikernel-support/","title":"Unikernel support","text":"<p>Unikernels are specialized, minimalistic operating systems constructed to run a single application. By compiling only the necessary components of an OS into the final image, unikernels offer improved performance, security, and smaller footprints compared to traditional OS-based virtual machines.</p> <p>One of the main goals of <code>urunc</code> is to bridge the gap between unikernels and the cloud-native ecosystem. For that reason, <code>urunc</code> aims to support all the available unikernel frameworks and similar technologies.</p> <p>For the time being, <code>urunc</code> provides support for Unikraft and Rumprun unikernels.</p>"},{"location":"unikernel-support/#unikraft","title":"Unikraft","text":"<p>Unikraft is a POSIX-friendly and highly modular unikernel framework designed to make it easier to build optimized, lightweight, and high-performance unikernels. Unlike traditional monolithic unikernel approaches, Unikraft allows developers to include only the components necessary for their application, resulting in reduced footprint and improved performance. At the same time, Unikraft offers Linux binary compatibility allowing easier and effortless execution of existing applications on top of Unikraft.  With support for various programming languages and environments, Unikraft is ideal for building unikernels across a wide range of use cases.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors","title":"VMMs and other sandbox monitors","text":"<p>Unikraft can boot on top of both Xen and KVM hypervisors. Especially in the case of KVM, Unikraft supports Qemu and AWS Firecracker. In both cases, it gets network access through virtio-net. In the case of storage, to the best of our knowledge Unikraft supports two options: a) 9pfs sharing a directory between the host and the unikernel and b) initrd and therefore an initial RamFS.</p>"},{"location":"unikernel-support/#unikraft-and-urunc","title":"Unikraft and <code>urunc</code>","text":"<p>In the case of Unikraft, <code>urunc</code> supports both network and storage I/O over both Qemu and Firecracker VMMs. However, for the time being, <code>urunc</code> only offers support for the initrd option of Unikraft and not for shared-fs. On the other hand, the shared-fs option is Work-In-Progress and we will soon provide an update about this.</p> <p>Unikraft maintains a catalog with available applications as unikernel images. Check out our packaging page on how to get these images and run them on top of <code>urunc</code>.</p> <p>An example of Unikraft on top of Qemu with <code>urunc</code>:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n</code></pre> <p>Another example of Unikraft on top of Firecracker with <code>urunc</code>:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n</code></pre>"},{"location":"unikernel-support/#mirage","title":"Mirage","text":"<p>MirageOS is a library operating system that constructs unikernels for secure, high-performance network applications across various cloud computing and mobile platforms. MirageOS uses the OCaml language, with libraries that provide networking, storage and concurrency support that work under Unix during development, but become operating system drivers when being compiled for production deployment. We can easily set up and build MirageOS unikernels with <code>mirage</code>, which can be installed throgu the Opam source package manager. The framework is fully event-driven, with no support for preemptive threading.</p> <p>MirageOS is characterized from the extremely fast start up times (just a few milliseconds), small binaries (usually a few megabytes), small footprint (requires a few megabytes of memory) and safe logic, as it is completely written in OCaml.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_1","title":"VMMs and other sandbox monitors","text":"<p>MirageOS, as one of the first unikernel frameworks, provides support for a variety of hypervisors and platforms. In particular, MirageOS makes use of Solo5 and can execute as a VM over KVM/Xen and other OSes, such as BSD OSes (FreeBSD, OpenBSD) or even Muen. Especially for KVM, MirageOS supports Qemu and Solo5-hvt.  It can access the network through virtio-net in the case of Qemu and using Solo5's I/O interface in the case of Solo5. For storage, MirageOS supports block-based storage through virtio-block and Solo5's I/O in Qemu and Solo5 respectively.</p> <p>Furthermore, MirageOS is also possible to execute on top of Solo5-spt a sandbox monitor of Solo5 project that does not use hardware-assisted virtualization. In that context, MirageOS can access network and block storage through Solo5's I/O interface.</p>"},{"location":"unikernel-support/#mirageos-and-urunc","title":"MirageOS and <code>urunc</code>","text":"<p>In the case of MirageOS <code>urunc</code> provides support for Solo5, Solo5 and Qemu. For all monitors of Solo5 <code>urunc</code> allows the access of both network and block storage through Solo5's I/O interface and for Qemu through virtio-net and virtio-block.</p> <p>For the time being, the block image that the MirageOS unikernel access during its execution should be placed inside the container image.</p> <p>For more information on packaging MirageOS unikernels for <code>urunc</code> take a look at our packaging page.</p> <p>An example of MirageOS on top of Solo5 using a block image inside the container's rootfs with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/net-mirage-hvt:latest\n</code></pre> <p>An example of MirageOS on top of Solo5 with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/net-mirage-spt:latest\n</code></pre>"},{"location":"unikernel-support/#rumprun","title":"Rumprun","text":"<p>Rumprun is a unikernel framework based on NetBSD, providing support for a variety of POSIX-compliant applications. Rumprun is particularly useful for deploying existing POSIX applications with minimal modifications. As a consequence of its design Rumprun can be up-to-date with the latest changes of NetBSD. However, the current repositories are not totally up-to-date. The repository with the most recent NetBSD version is here.</p> <p>In addition, Rumprun maintains a repository with all ported applications that can be easily used on top of Rumprun.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_2","title":"VMMs and other sandbox monitors","text":"<p>Rumprun, as one of the oldest unikernel frameworks, provides support for both Xen and KVM hypervisors. Especially in the case of KVM, Rumprun supports Qemu and Solo5-hvt. It can access the network through virtio-net in the case of Qemu and using Solo5's I/O interface in the case of Solo5.  As far as we concern, Rumprun only supports block storage through virtio-block and Solo5's I/O in Qemu and Solo5 respectively.</p> <p>Furthermore, Rumprun is also possible to execute on top of Solo5-spt a sandbox monitor of Solo5 project that does not use hardware-assisted virtualization. In that context, Rumprun can access network and block storage through Solo5's I/O interface.</p>"},{"location":"unikernel-support/#rumprun-and-urunc","title":"Rumprun and <code>urunc</code>","text":"<p>In the case of Rumprun, <code>urunc</code> provides support for Solo5 and Solo5, but not yet for Qemu. For all monitors of Solo5 <code>urunc</code> allows the access of both network and block storage through Solo5's I/O interface. In particular, <code>urunc</code> takes advantage of Rumprun block storage and ext2 filesystem support and allows the mounting of the containerd's snapshot directly in the unikernel. This is only possible using devmapper as a snapshotter in containerd. For more information on setting up devmapper, please take a look on our installation guide.</p> <p>Except for devmapper, <code>urunc</code> also supports the option of adding a block image inside the container image and attaching it to Rumprun.</p> <p>For more information on packaging Rumprun unikernels for <code>urunc</code> take a look at our packaging page.</p> <p>An example of Rumprun on top of Solo5 using a block image inside the container's rootfs with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest\n</code></pre> <p>An example of Rumprun on top of Solo5 using devmapper with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun-raw:latest\n</code></pre>"},{"location":"unikernel-support/#mewz","title":"Mewz","text":"<p>Mewz is a unikernel framework written from scratch in Zig, targeting WASM workloads. In contrast to other WASM runtimes that execute on top of general purpose operating systems, Mewz is designed as a specialized kernel where WASM applications can execute. In this way, Mewz provides the minimal required features and environment for executing WASM workloads. In addition, every WASM application executes on a separate Mewz instance, maintaining the single-purpose notion of unikernels.</p> <p>According to the design of Mewz, the WASM application is transformed to an object file which is directly linked against the Mewz kernel. Therefore, when the Mewz kernel boots, it executes the linked WASM application. Mewz has partial support for WASI and it provides support for networking and an in-memory, read-only filesystem. In addition, Mewz has socket compatibility with WasmEdge,</p> <p>A few examples of Mewz unikernels can be found in the examples directory of Mewz's repository.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_3","title":"VMMs and other sandbox monitors","text":"<p>Mewz can execute only on top of Qemu.  It can access the network through a virtio-net PCI device. In the case of storage, Mewz only supports an in-memory read-only filesystem, which is directly linked along with the kernel.</p>"},{"location":"unikernel-support/#mewz-and-urunc","title":"Mewz and <code>urunc</code>","text":"<p>In the case of Mewz, <code>urunc</code> provides support for Qemu. If the container is configured with network access, then <code>urunc</code> will use a virtio-net PCI device to provide network access to Mewz unikernels.</p> <p>For more information on packaging Mewz unikernels for <code>urunc</code> take a look at our packaging page.</p> <p>An example of Mewz on top of Qemu with 'urunc':</p> <pre><code>sudo nerdctl run -m 512M --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/hello-server-qemu-mewz:latest\n</code></pre> <p>Note: As far as we understand, Mewz requires at least 512M of memory to properly boot.</p>"},{"location":"unikernel-support/#linux","title":"Linux","text":"<p>Linux is maybe the most widely used kernel and the vast majority of servers in the cloud use an OS based on Linux kernel. As a result, most applications and services we run on the cloud are built targeting Linux. Of course, Linux is not a unikernel framework. However, thanks to its highly configurable build-system we can create very small, tailored Linux kernels for a single application. The concept was introduced by the Lupine project, which examined how we can turn the Linux kernel into a unikernel.</p> <p>Using Linux, we can execute the vast majority of the existing containers on top of <code>urunc</code>. However, the rational is to target single application containers and not fully-blown distro containers. Focusing on a single application, we can further minimize the Linux kernel and keep only the necessary components for a specific application. Such a design allows the creation of minimal and fast single-application kernels that we can execute on top of <code>urunc</code>.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_4","title":"VMMs and other sandbox monitors","text":"<p>Linux has wide support for different hardware and virtualization targets. It can execute on top of Qemu and Firecracker. It can access the network and storage through various ways (e.g. paravirtualization, emulated devices etc.).</p>"},{"location":"unikernel-support/#linux-and-urunc","title":"Linux and <code>urunc</code>","text":"<p>Focusing on the single-application notion of using the Linux kernel, <code>urunc</code> provides support for both Qemu and Firecracker. For network, <code>urunc</code> will make use of virtio-net either through PCI or MMIO, depending on the monitor. In the case of storage, <code>urunc</code> can use initrd, virtio-block ,9pfs or Virtiofs. In particular, <code>urunc</code> takes advantage of the extensive filesystem support of Linux and can directly mount containerd's snapshot directly to a Linux VM. This is only possible using devmapper as a snapshotter in containerd. For more information on setting up devmapper, please take a look on our installation guide.</p> <p>For more information on packaging applications and executing them on top of Linux with <code>urunc</code> take a look at our running existing containers tutorial.</p> <p>An example of a Nginx alpine image on top of Qemu and Linux with 'urunc' and devmapper as a snapshotter:</p> <pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-linux-raw:latest\n</code></pre> <p>An example of a Redis alpine image transformed to a block file on top of Firecracker and Linux with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-firecracker-linux-block:latest\n</code></pre>"},{"location":"unikernel-support/#future-unikernels-and-frameworks","title":"Future unikernels and frameworks:","text":"<p>In the near future, we plan to add support for the following frameworks:</p> <p>OSv: An OS designed specifically to run as a single application on top of a hypervisor. OSv is known for its performance optimization and supports a wide range of programming languages, including Java, Node.js, and Python.</p>"},{"location":"design/","title":"Design","text":"<p>This section describes the high-level architecture of <code>urunc</code>, along with the design choices and limitations.</p>"},{"location":"design/#overview","title":"Overview","text":"<p><code>urunc</code> is a container runtime designed to bridge the gap between traditional unikernels and containerized environments. It enables seamless integration with cloud-native architectures by leveraging familiar OCI (Open Container Initiative) tools and methodologies. By acting as a unikernel container runtime compatible with the Container Runtime Interface (CRI), <code>urunc</code> allows unikernels to be managed like containers, opening up possibilities for lightweight, secure, and high-performance application deployment.</p> <p>In <code>urunc</code>, the user code runs inside a unikernel on top of a Virtual Machine Monitor (VMM) or a sandbox monitor. As a result, <code>urunc</code> guarantees strong isolation among the containers and inherits the enhanced security features of unikernels, such as their small attack surface.</p> <p>In the unikernel context a single-process application runs directly on top of a Virtual Machine (VM) or a sandbox. At the same time, in the VM context, every VM runs as a process. Subsequently, <code>urunc</code> combines these two characteristics and treats the VM's process, which executes the unikernel that runs the application, as the container's process. This way, <code>urunc</code> does not require any auxiliary process running alongside the unikernel, maintaining as less overhead as possible. Instead <code>urunc</code> directly manages the application running in the unikernel through the VMM or the sandbox monitor. Moreover, <code>urunc</code> does not require any modifications in the unikernel framework and hence all unikernel frameworks and similar technologies can easily integrate with <code>urunc</code>.</p>"},{"location":"design/#execution-flow","title":"Execution flow","text":"<p>The process of starting a new unikernel container with <code>urunc</code>, starts at the higher-level runtime (<code>containerd</code>) level:</p> <ul> <li><code>Containerd</code> unpacks the image into a supported snapshotter (e.g. <code>devmapper</code>)   and invokes <code>urunc</code>, as any other OCI runtime.</li> <li><code>urunc</code> parses the image's rootfs and annotations, initiating the required   setup procedures. In particular, it creates essential pipes for stdio, it   creates the container's state file and runs the <code>prestart</code> hooks (if any).</li> <li>Subsequently, <code>urunc</code> spawns a new process within a distinct network   namespace, stores its PID and invokes the <code>createRuntime</code> and   <code>createContainer</code> hooks.</li> <li>When <code>Containerd</code> starts the container <code>urunc</code> configures any required   resources such as block devices or  network interfaces and runs the   <code>statContainer</code> hooks.</li> <li>Depending on the specified unikernel type and annotations, <code>urunc</code> selects the   appropriate VMM or sandbox monitor (e.g.  Qemu, Solo5-spt) and boots the   unikernel. The unikernel runs inside its own isolated environment, interacting   with external systems through the namespaces and devices configured by   <code>urunc</code>.</li> <li>Finally the unikernel is up and running as a container, and we can manage its   lifecycle like any other container through <code>urunc</code> (e.g., stopping,   restarting, or deleting the container).</li> </ul>"},{"location":"design/#image-format-and-annotations","title":"Image Format and Annotations","text":"<p>To support unikernels in a containerized environment, <code>urunc</code> requires specific metadata embedded in OCI container images. These images must include the unikernel binary, configuration and any other files required from the application or the unikernel and the aforementioned metadata which dictate how the unikernel should be run. The metadata can be passed to <code>urunc</code> either in the form of annotations or as a specific file in the container's rootfs. For a detailed explanation and an up-to-date list of the currently supported annotations take a look at the packaging unikernels page.</p> <p>Although <code>urunc</code>-formatted unikernel images are not designed to be executed by other container runtimes, they can still be stored and distributed via generic container registries, such as Docker Hub or Harbor. This ensures compatibility with standard cloud-native workflows for building, shipping, and deploying applications.</p>"},{"location":"design/seccomp/","title":"Seccomp in Urunc","text":""},{"location":"design/seccomp/#overview","title":"Overview","text":"<p>Seccomp (Secure Computing Mode) is a Linux kernel security feature that restricts the system calls a process can make, limiting the kernel exposure to the processes. Container runtimes make use of this mechanism to further limit a container and enhance overall security. </p>"},{"location":"design/seccomp/#how-seccomp-is-used-in-urunc","title":"How Seccomp is used in 'urunc'","text":"<p>In 'urunc' the application does not execute directly in the host kernel. Instead, 'urunc' makes use of either a VMM (Virtual Machine Monitor) or the <code>solo5-spt</code> tender to execute the application inside a unikernel. As a result, in contrast with other container runtimes, in 'urunc' the applications do not share the same kernel.</p> <p>Thus, a malicious user must take control of the guest kernel and escape to the VMM before attacking the host. To further limit the exposure of the host kernel to the VMM, 'urunc' uses seccomp filters for each supported VMM. In particular, in the case of: - Firecracker, 'urunc' does not have to do anything more, since Firecracker by   default makes uses seccomp filters. - Qemu, 'urunc' makes use of Qemu's sandbox command line options to activate   all possible seccomp filters in Qemu. - Solo5-hvt, 'urunc' applies the seccomp filters before executing   'Solo5-hvt'. - Solo5-spt, 'urunc' can not do anything since solo5-spt makes use of seccomp by   itself.</p>"},{"location":"design/seccomp/#caveats-of-using-seccomp-in-urunc","title":"Caveats of using seccomp in 'urunc'","text":"<p>Since 'urunc', in most cases, makes use of the VMM's mechanisms to enforce the seccomp filters, 'urunc' heavily relies on the VMM to properly restrict the system calls the VMM can use.</p> <p>In the case of 'Solo5-hvt', since 'urunc' is responsible for applying the seccomp filters, proper identification of the required system calls is necessary. Unfortunately, due to dynamic linking and Go's runtime, it is impossible to always predict correctly for every system the necessary system calls for 'Solo5-hvt' execution.</p> <p>Nevertheless, 'Solo5-hvt' with seccomp in 'urunc' has been tested in Ubuntu 20.04 and Ubuntu 22.04. Using 'urunc' and solo5-hvt on different platforms might result in failed execution. For that reason, we strongly recommend running the seccomp test first, by <code>make test_nerdctl_Seccomp</code>. In case the test fails, the seccomp profile for 'Solo5-hvt' needs to get updated.</p> <p>For that reason, we created a toolset to identify the required system calls. The toolset, along with instructions on how to use it, can be found in goscall repository.</p>"},{"location":"design/seccomp/#setting-a-seccomp-profile","title":"Setting a seccomp profile","text":"<p>Due to its design, 'urunc' does not allow the definition of a seccomp profile other than the default. However, users can totally disable seccomp by using the <code>--security-opt seccomp=unconfined</code> command line option. In that scenario, 'urunc' will not make use of any seccomp filters in all the supported VMMs, except of 'Solo5-spt'.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>In this section we provide useful information regarding the development of <code>urunc</code>. In particular, the section contains information about setting up a dev environment, checking performance traces and more.</p> <p>We kindly ask you to check the contributing page before submitting any pull requests, or opening new issues.</p> <p>Join us for our monthly open meetings, held every last Wednesday of the month. These sessions are a great opportunity to share ideas, ask questions, and stay connected with the project team and other contributors.</p> <ul> <li>Meeting Frequency: Monthly (last Wednesday of the month)</li> <li>Time: 15:00 UTC</li> <li>Format: Open agenda + roadmap review Minutes &amp; Agenda</li> <li>Platform: LF meetings</li> <li>Invitation: link</li> <li>Slack channel</li> </ul>"},{"location":"developer-guide/Code-of-Conduct/","title":"Code of Conduct","text":"<p>All maintainers and community members of <code>urunc</code> must abide the CNCF Code of Conduct.</p>"},{"location":"developer-guide/contribute/","title":"Contributing","text":"<p>Urunc is an open-source project licenced under the Apache License 2.0. We welcome anyone who would be interested in contributing to <code>urunc</code>. As a first step, please take a look at the following document and the LLM policy).</p>"},{"location":"developer-guide/contribute/#table-of-contents","title":"Table of contents:","text":"<ol> <li>Code organization</li> <li>How to contribute</li> <li>Opening an issue</li> <li>Requesting new features</li> <li>Submitting a PR</li> <li>Style guide</li> <li>Contact</li> </ol>"},{"location":"developer-guide/contribute/#code-organization","title":"Code organization","text":"<p>Urunc is written in Go and we structure the code and other files as follows:</p> <ul> <li><code>/</code>: The root directory contains the Makefile to build <code>urunc</code>, along with other non-code files, such as the licence, readme and more.</li> <li><code>/docs</code>: This directory contains all the documentation related to <code>urunc</code>, such as the installation guide, timestamping and more.</li> <li><code>/cmd</code>: This directory contains handlers for the various command line options of <code>urunc</code> and the implementation of containerd-shim.</li> <li><code>/internal/metrics</code>: This directory contains the implementation of the metrics logger, which is used for the internal measuring of <code>urunc</code>'s setup steps.</li> <li><code>/pkg</code>: This directory contains the majority of the code for <code>urunc</code>. In particular, the subdirectory <code>/pkg/network/</code> contains network related code as expected, while the <code>/pkg/unikontainers/</code> subdirectory contains the main logic of <code>urunc</code>, along with the VMM/unikernel related logic.</li> </ul> <p>Therefore, we expect any new documentation related files to be placed under <code>/docs</code> and any changes or new files in code to be either in the <code>/cmd/</code> or <code>/pkg/</code> directory.</p>"},{"location":"developer-guide/contribute/#how-to-contribute","title":"How to contribute","text":"<p>There are plenty of ways to contribute to an open source project, even without changing or touching the code. Therefore, anyone who is interested in this project is very welcome to contribute in one of the following ways:</p> <ul> <li>Using <code>urunc</code>. Try it out yourself and let us know your experience. Did   everything work well? Were the instructions clear?</li> <li>Improve or suggest changes to the documentation of the project.   Documentation is very important for every project, hence any ideas on how to   improve the documentation to make it more clear are more than welcome.</li> <li>Request new features. Any proposals for improving or adding new features are   very welcome.</li> <li>Find a bug and report it. Bugs are everywhere and some are hidden very well.   As a result, we would really appreciate it if someone found a bug and   reported it to the maintainers.</li> <li>Find an existing issue that interests you and try to resolve it. To get   started, simply express your interest in a comment, and the maintainers will   either assign the issue to you or ask for a proposal.</li> </ul>"},{"location":"developer-guide/contribute/#opening-an-issue","title":"Opening an issue","text":"<p>We use Github issues to track bugs and requests for new features. Anyone is welcome to open a new issue, which is either related to a bug or a request for a new feature. Please make sure to read the LLM policy in cases where an LLM has been used.</p>"},{"location":"developer-guide/contribute/#reporting-bugs","title":"Reporting bugs","text":"<p>In order to report a bug or misbehavior in <code>urunc</code>, a user can open a new issue explaining the problem. For the time being, we do not use any strict template for reporting any issues. However, in order to easily identify and fix the problem, it would be very helpful to provide enough information. In that context, when opening a new issue regarding a bug, we kindly ask you to:</p> <ul> <li>Mark the issue with the bug label</li> <li> <p>Provide the following information:</p> <ol> <li>A short description of the bug.</li> <li>The respective logs both from the output and containerd.</li> <li>Urunc's version (either the commit's hash or the version).</li> <li>The CPU architecture, VMM and the Unikernel framework used.</li> <li>Any particular steps to reproduce the issue.</li> <li>Keep an eye on the issue for possible questions from the maintainers.</li> </ol> </li> </ul> <p>A template for an issue could be the following one: <pre><code>## Description\nAn explanation of the issue \n\n## System info\n\n- Urunc version:\n- Arch:\n- VMM:\n- Unikernel:\n\n## Steps to reproduce\nA list of steps that can reproduce the issue.\n</code></pre></p>"},{"location":"developer-guide/contribute/#requesting-new-features","title":"Requesting new features","text":"<p>We will be very happy to listen from users about new features that they would like to see in <code>urunc</code>. One way to communicate such a request is using Github issues. For the time being, we do not use any strict template for requesting new features. However, we kindly ask you to mark the issue with the enhancement label and provide a description of the new feature.</p>"},{"location":"developer-guide/contribute/#submitting-a-pr","title":"Submitting a PR","text":"<p>Anyone should feel free to submit a change or an addition to the codebase of <code>urunc</code>. Currently, we use Github's Pull Requests (PRs) to submit changes to <code>urunc</code>'s codebase. Before creating a new PR, please follow the rules below:</p> <ul> <li>Avoid opening PRs for non-existent issues. Please create an issue first.</li> <li>Complete the PR template.</li> <li>In case LLMs have been used, please read the LLM   policy.</li> <li>Avoid changes unrelated to the PR/issue.</li> <li>Test the changes locally.</li> <li>Make sure that the changes do not break the building process of <code>urunc</code>.</li> <li>Make sure that all the tests run successfully.</li> <li>Make sure that no commit in a PR breaks the building process of <code>urunc</code>.</li> <li>Make sure to sign-off your commits.</li> <li>Organize changes into logical commits. Each commit should represent a single,   specific change, avoiding both overly large and overly small commits.</li> <li>Provide meaningful commit messages, describing shortly the changes.</li> <li>Provide a meaningful PR message.</li> </ul> <p>The maintainers and andmins of the <code>urunc</code> project reserve the right to close PRs that do not comply with the above rules,with reference to this contribution guide.</p> <p>A new (draft) PR triggers the following process:</p> <ol> <li>A maintainer will check the PR and apply the <code>ok-to-test</code> label. This will      trigger our CI to make sure the PR changes do not break existing      functionality.</li> <li>The PR owner makes sure the CI tests pass, and marks the PR as ready to review.</li> <li>The reviewers submit their comments &amp; reviews (if any).</li> <li>The PR owner should address comments (if any).</li> <li>If the PR is external (forked repo), a maintainer adds the <code>takeover</code>      label, that triggers the change of the base branch to <code>main-pr&lt;PRID&gt;</code>. If      the PR is internal (local branch) there is no need for the <code>takeover</code> label.</li> <li>As soon as a reviewer approves the PR, an action will be triggered to add      the appropriate trailers in the PR's commits and merge the PR with the      main branch. If the PR is external, the maintainer should rebase &amp; merge for      the action to be triggered. If the PR is internal, the action will be triggered      automatically.</li> </ol>"},{"location":"developer-guide/contribute/#labels-for-the-ci","title":"Labels for the CI","text":"<p>We use github workflows to invoke some tests when a new PR opens for <code>urunc</code>. In particular, we perform the following workflows tests:</p> <ul> <li>Linting of the commit message. Please check the git commit message style below for more info.</li> <li>Spell check, since <code>urunc</code> repository contains its documentation too.</li> <li>License check</li> <li>Code linting for Go.</li> <li>Building artifacts for amd64 and aarch64.</li> <li>Unit tests</li> <li>End-to-end tests</li> </ul> <p>For a better control over the tests and workflows that run in a PR, we define three labels which can be used:</p> <ul> <li><code>ok-to-test</code>: Runs a full CI workflow, meaning all lint tests (commit   message, spellcheck, license), Go's linting, building for x86 and aarch64,   unit tests and at last end-to-end tests.</li> <li><code>skip-build</code>: Skips the building workflows along with unit and end-to end tests   running all the linting tests. This is useful when   the PR is related to docs and it can help for catching spelling errors etc. In   addition, if the changes are not related to the codebase, running the   end-to-end tests is not required and saves some time.</li> <li><code>skip-lint</code>: Skips the linting phase. This is particularly useful on draft   PRs, when we want to just test the functionality of the code (either a bug   fix, or a new feature) and defer the cleanup/polishing of commits, code, and   docs to when the PR will be ready for review.</li> <li><code>takeover</code>: Changes the base branch for the PR from <code>main</code> to <code>main-pr&lt;PRID&gt;</code>.   This is to facilitate adding git trailers on the commit messages, to mention   <code>Reviewers</code> and link to the original github PR for reference.</li> </ul> <p>Note: Both <code>skip-build</code> and <code>skip-lint</code> assume that the <code>ok-to-test</code> label is added.</p>"},{"location":"developer-guide/contribute/#style-guide","title":"Style guide","text":""},{"location":"developer-guide/contribute/#git-commit-messages","title":"Git commit messages","text":"<p>Please follow the below guidelines for your commit messages:</p> <ul> <li>Limit the first line to 72 characters or less.</li> <li>Limit all the other lines to 80 characters</li> <li> <p>Follow the Conventional Commits   specification and, specifically, format the header as <code>&lt;type&gt;[optional scope]:   &lt;description&gt;</code>, where <code>description</code> must not end with a fullstop and <code>type</code>   can be one of:</p> </li> <li> <p>feat: A new feature</p> </li> <li>fix: A bug fix</li> <li>docs: Documentation only changes</li> <li>style: Changes that do not affect the meaning of the code (white-space,     formatting, missing semi-colons, etc)</li> <li>refactor: A code change that neither fixes a bug nor adds a feature</li> <li>perf: A code change that improves performance</li> <li>test: Adding missing tests</li> <li>build: Changes that affect the build system or external dependencies     (example scopes: gulp, broccoli, npm)</li> <li>ci: Changes to our CI configuration files and scripts (example scopes:     Travis, Circle, BrowserStack, SauceLabs)</li> <li>chore: Other changes that don't modify src or test files</li> <li>revert: Reverts a previous commit</li> <li>In case the PR is associated with an issue, please refer to it, using the git trailer <code>Fixes: #Nr_issue</code></li> <li>Always sign-off your commit message</li> </ul>"},{"location":"developer-guide/contribute/#golang-code-styde","title":"Golang code styde","text":"<p>We follow gofmt's rules on formatting GO code. Therefore, we ask all contributors to do the same. Go provides the <code>gofmt</code> tool, which can be used for formatting your code.</p>"},{"location":"developer-guide/contribute/#contact","title":"Contact","text":"<p>We kindly invite everyone interested in <code>urunc</code> to join our Slack channel. To directly communicate with the maintainers, feel free to drop an email At <code>urunc</code>'s maintainers' mailing list</p>"},{"location":"developer-guide/debugging/","title":"Debugging urunc Containers","text":""},{"location":"developer-guide/debugging/#debugging-urunc-containers-with-cntr","title":"Debugging urunc Containers with cntr","text":"<p>This guide explains how to attach to a running <code>urunc</code> container using <code>cntr</code>, in order to inspect its environment and use additional debugging tools.</p> <p><code>cntr</code> overlays an alternative root filesystem on top of the container namespace, allowing access to utilities such as <code>ls</code>, <code>ps</code>, that are not present in the original environment.</p>"},{"location":"developer-guide/debugging/#using-cntr-with-urunc","title":"Using cntr with urunc","text":""},{"location":"developer-guide/debugging/#prerequisites","title":"Prerequisites","text":"<p>Install cntr:</p> <pre><code>cargo install cntr\n</code></pre> <p>If you don't have Rust/Cargo installed:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource ~/.cargo/env\ncargo install cntr\n</code></pre>"},{"location":"developer-guide/debugging/#steps","title":"Steps","text":"<ol> <li> <p>Start a urunc container:</p> <pre><code>sudo docker run -d --name urunc-debug --runtime io.containerd.urunc.v2 -it \\\n  harbor.nbfc.io/nubificus/urunc/dbg/ubuntu:dltme /bin/bash\n</code></pre> </li> <li> <p>Get the container ID:</p> <pre><code>$ sudo docker ps -a\nCONTAINER ID   IMAGE                                             COMMAND       CREATED         STATUS         PORTS     NAMES\n56b93fbd7332   harbor.nbfc.io/nubificus/urunc/dbg/ubuntu:dltme   \"/bin/bash\"   3 seconds ago   Up 3 seconds             urunc-debug\n</code></pre> </li> <li> <p>Attach with cntr:</p> <pre><code>sudo cntr attach 56b93fbd7332\n</code></pre> </li> </ol> <p>You now have an interactive shell with access to debugging tools!</p>"},{"location":"developer-guide/debugging/#output","title":"Output:","text":"<pre><code>$ sudo cntr attach 56b93fbd7332\nroot@host:/var/lib/cntr#\n\n# List PTY devices\nroot@host:/var/lib/cntr# ls -la /dev/pts\ndrwxr-xr-x 2 root root      0 Nov  3 09:07 .\ncrw------- 1 root tty  136, 0 Nov  3 09:07 0\ncrw------- 1 root tty  136, 1 Nov  3 09:11 1\ncrw-rw-rw- 1 root root   5, 2 Nov  3 09:11 ptmx\n\n# Check console device\nroot@host:/var/lib/cntr# ls -la /dev/console\n-rw-rw-rw- 1 root root 0 Nov  3 09:07 /dev/console\n\n# View processes \nroot@host:/var/lib/cntr# ps aux | grep qemu\n\n# Inspect container filesystem\nroot@host:/var/lib/cntr# ls -la \n</code></pre>"},{"location":"developer-guide/debugging/#what-cntr-enables","title":"What <code>cntr</code> Enables","text":"<p>Using <code>cntr</code> with a urunc container gives:</p> <ul> <li>Working PTY devices (<code>/dev/pts</code>, <code>/dev/ptmx</code>, <code>/dev/console</code>)</li> <li>A debugging environment with common tools (e.g., <code>ls</code>, <code>ps</code>, <code>strace</code>)</li> <li>Visibility into the container namespace where the monitor process (qemu/firecracker/solo5) runs</li> </ul> <p>Note: <code>cntr</code> does not enter the unikernel VM \u2014 it only provides access to the container namespace hosting the monitor.</p>"},{"location":"developer-guide/debugging/#debugging-with-logs","title":"Debugging with Logs","text":"<p>To enable debugging logs, we need to pass the <code>--debug</code> flag when calling <code>urunc</code>. Also, to facilitate easier debugging, when the <code>debug</code> flag is true all logs are propagated to the syslog.</p> <p>An easy way to achieve this is to create a Bash wrapper for <code>urunc</code>:</p> <pre><code>sudo mv /usr/local/bin/urunc /usr/local/bin/urunc.default\nsudo tee /usr/local/bin/urunc &gt; /dev/null &lt;&lt;'EOT'\n#!/usr/bin/env bash\nexec /usr/local/bin/urunc.default --debug \"$@\"\nEOT\nsudo chmod +x /usr/local/bin/urunc\n</code></pre>"},{"location":"developer-guide/development/","title":"Setup a Dev environment","text":""},{"location":"developer-guide/development/#prerequisites","title":"Prerequisites","text":"<p>Most of the steps are covered in the installation document. Please refer to it for:</p> <ul> <li>installing a recent version of Go (e.g. 1.24.6)</li> <li>installing <code>containerd</code> and <code>runc</code></li> <li>setting up the devmapper snapshotter</li> <li>installing <code>nerdctl</code> and the <code>CNI</code> plugins</li> <li>installing the relevant hypervisors</li> </ul>"},{"location":"developer-guide/development/#crictl-installation","title":"crictl installation","text":"<p>In addition to the above, we strongly suggest to install crictl which <code>urunc</code> uses for its end-to-end tests. The following commands will install <code>crictl</code></p> <pre><code>VERSION=\"v1.30.0\" # check latest version in /releases page\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz\nsudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin\nrm -f crictl-$VERSION-linux-amd64.tar.gz\n</code></pre> <p>Since default endpoints for <code>crictl</code> are now deprecated, we need to set them up:</p> <pre><code>sudo tee -a /etc/crictl.yaml &gt; /dev/null &lt;&lt;'EOT'\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 20\nEOT\n</code></pre>"},{"location":"developer-guide/development/#urunc-installation","title":"urunc installation","text":"<p>The next step is to clone and build <code>urunc</code>:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\ncd urunc\nmake &amp;&amp; sudo make install\n</code></pre> <p>At last, please  validate that the dev environment has been set correctly by running the:</p> <ul> <li> <p>unit tests: <code>make unittest</code> and</p> </li> <li> <p>end-to-end tests: <code>sudo make e2etest</code></p> </li> </ul> <p>Note: When running <code>make</code> commands for <code>urunc</code> that will use go (i.e. build, unitest, e2etest) you might need to specify the path to the go binary with <code>sudo GO=$(which go) make</code>.</p>"},{"location":"developer-guide/development/#next-steps","title":"Next Steps","text":"<p>For information on debugging urunc containers, see the Debugging Guide.</p>"},{"location":"developer-guide/governance/","title":"Governance","text":"<p><code>urunc</code> is dedicated to enable the deployment of unikernels and single application kernels in cloud-native environments. This governance document explains how the project is run.</p> <ul> <li>Values</li> <li>Roles</li> <li>Meetings</li> <li>CNCF Resources</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifications</li> </ul>"},{"location":"developer-guide/governance/#values","title":"Values","text":"<p><code>urunc</code> and its leadership embrace the following values:</p> <ul> <li> <p>Openness: Communication and decision-making happens in the open and is   discoverable for future reference. As much as possible, all discussions and   work take place in public forums and open repositories.</p> </li> <li> <p>Fairness: All stakeholders have the opportunity to provide feedback and   submit contributions, which will be considered on their merits.</p> </li> <li> <p>Community over Product or Company: Sustaining and growing our community takes   priority over shipping code or sponsors' organizational goals.  Each   contributor participates in the project as an individual.</p> </li> <li> <p>Inclusivity: We innovate through different perspectives and skill sets, which   can only be accomplished in a welcoming and respectful environment.</p> </li> <li> <p>Participation: Responsibilities within the project are earned through   participation, and there is a clear path up the contributor ladder into   leadership positions.</p> </li> </ul>"},{"location":"developer-guide/governance/#roles","title":"Roles","text":"<p>There are several roles relevant to <code>urunc</code>'s governance:</p>"},{"location":"developer-guide/governance/#contributor","title":"Contributor","text":"<p>A Contributor to <code>urunc</code> is someone who has contributed to the project (e.g. code, docs, CI) within the last 12 months. Contributors have read only access to the urunc repositories on GitHub.</p>"},{"location":"developer-guide/governance/#maintainer","title":"Maintainer","text":"<p><code>urunc</code> Maintainers (as defined by the urunc Maintainers team) have write access to the <code>urunc</code> repository on GitHub, which gives the ability to approve / merge / close PRs, trigger the CI and manage / classify project issues. All pull requests require review by a maintainer other than the one submitting it. \"Large\" changes are encouraged to gather consensus from multiple maintainers and interested community members. Maintainers are active Contributors and participants in the project, collectively managing the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about <code>urunc</code> and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which is the governing body for the project.</p>"},{"location":"developer-guide/governance/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>actively participate in meetings, discussions, contributions, code and   documentation reviews for at least 6 months,</li> <li>perform reviews for at least 3 non-trivial pull requests,</li> <li>contribute 3 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and     code review, etc),</li> <li>understanding of the project's code base and coding / documentation     style.</li> </ul> <p>A new Maintainer must be proposed by an established Contributor and/or an existing maintainer by sending a message to the developer mailing list. A simple majority vote of existing Maintainers approves the application. Maintainer nominations will be evaluated without prejudice to employer or demographics.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"developer-guide/governance/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their Maintainer responsibilities, violating the Code of Conduct, or other reasons. Inactivity is defined as a period of very low or no activity in the project for 6 months or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus status. Emeritus Maintainers will still be consulted on some project matters, and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"developer-guide/governance/#admin","title":"Admin","text":"<p><code>urunc</code> Admins (as defined by the urunc Admin team have admin access to the <code>urunc</code> repo, allowing them to do actions like, change the branch protection rules for repositories, delete a repository and manage the access of others. The Admin group is intentionally kept small, however, individuals can be granted temporary admin access to carry out tasks, like creating a secret that is used in a particular CI infrastructure. The Admin list is reviewed and updated twice a year and typically contains: - A subset of the maintainer team - Optionally, some specific people that the Maintainers agree on adding for a   specific purpose (e.g. to manage the CI)</p>"},{"location":"developer-guide/governance/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public developer meeting, which occurs every last Wed of each month at 3pm UTC.</p> <p>Maintainers will also have closed meetings in order to discuss security reports or Code of Conduct violations. Such meetings should be scheduled by any Maintainer on receipt of a security issue or CoC report. All current Maintainers must be invited to such closed meetings, except for any Maintainer who is accused of a CoC violation.</p>"},{"location":"developer-guide/governance/#cncf-resources","title":"CNCF Resources","text":"<p>Any Maintainer may suggest a request for CNCF resources, either in the mailing list, or during a meeting.  A simple majority of Maintainers approves the request.  The Maintainers may also choose to delegate working with the CNCF to non-Maintainer community members, who will then be added to the CNCF's Maintainer List for that purpose.</p>"},{"location":"developer-guide/governance/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved by the maintainers privately. If a Maintainer is directly involved in the report, the Maintainers will instead designate two Maintainers to work with the CNCF Code of Conduct Committee in resolving it.</p>"},{"location":"developer-guide/governance/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves.  If this responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security holes and breaches according to the security policy.</p>"},{"location":"developer-guide/governance/#voting","title":"Voting","text":"<p>While most business in <code>urunc</code> is conducted by \"lazy consensus\",  periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters. Votes may also be taken at the developer meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed, except where otherwise noted. Two-thirds majority votes mean at least two-thirds of all  existing maintainers.</p>"},{"location":"developer-guide/governance/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a 2/3 vote of the Maintainers.</p>"},{"location":"developer-guide/llm-policy/","title":"LLM policy","text":"<p>As a CNCF Sandbox project, <code>urunc</code> adheres to the Linux Foundation Generative AI Policy. All members of the <code>urunc</code> community are therefore expected to read and comply with the above policy. This document enhances the Linux Foundation policy specifically for the <code>urunc</code> project.</p> <p>Large Language Models (LLMs) are capable of analyzing and generating massive amounts of code in a very short period of time. While this can be beneficial, like many open source projects, <code>urunc</code> has experienced an increase in contributions that exhibit low quality and clear signs of unreviewed LLM usage. For that reason, the <code>urunc</code> project has decided to enforce some rules regarding the use of LLM in its development process:</p> <ul> <li>LLM generated code must be treated in the same way as any other code.</li> <li>The use of LLMs for any contributions must be disclosed, including the exact   model that was used.</li> <li>The user of LLM takes full responsibility of the generated output.</li> <li>All LLM-generated content must be reviewed by the contributor before opening   issues or PRs, ensuring correctness, quality, and relevance.</li> <li>The use of LLMs to respond to comments in issues or PRs is not permitted.   Contributors are expected to express their own thoughts and ideas.  Acting as   a mediator between another user and an LLM is not helpful..</li> <li>LLMs may assist in code review but can not replace human reviews. Human   reviewer guidance takes precedence over any LLM comments.</li> </ul> <p>The maintainers and admins of the <code>urunc</code> project reserve the right to close issues or PRs that do not comply with the above rules, with reference to this policy.</p>"},{"location":"developer-guide/llm-policy/#the-rationale-of-the-above-rules","title":"The rationale of the above rules","text":"<p>The <code>urunc</code> project can significantly benefit from the responsible use of LLMs and this policy is not intended to prohibit their use. Instead, it aims to protect the project from the misuse of such tools. LLM outputs are probabilistic in nature and may introduce subtle bugs, outdated practices, security risks, or incorrect assumptions. As such, any code or technical contribution produced with the assistance of an LLMs must be carefully reviewed and tested. Responsibility for the correctness and quality of LLM-generated output lies solely with the contributor who uses the tool.</p> <p>Do not forget that behind the <code>urunc</code> project there are humans who aim to provide meaningful feedback and genuine assistance to the community.  However, an influx of low-quality contributions and bug reports significantly reduces their time and energy to properly review and assist other meaningful contributions.  Nevertheless, <code>urunc</code> is a unique and novel project with specific design choices and constraints that often require deeper explanation and discussion. As a result, LLMs may not produce helpful or accurate output for many <code>urunc</code>-specific use cases.</p> <p>With this in mind, it would be way more beneficial to engage in discussions with other people sharing genuine ideas, thoughts and concerns over an issue, PR or in the Slack channel. There is no need to consult LLMs for every interaction, on the contrary do not hesitate to ask other members. Let's help each other learn and share ideas to improve the project.</p>"},{"location":"developer-guide/maintainers/","title":"The current maintainers of <code>urunc</code>","text":"Name GitHub Username Email Responsibility Charalampos Mainas @cmainas cmainas@nubificus.co.uk Core Maintainer Georgios Ntoutsos @gntouts gntouts@nubificus.co.uk Core Maintainer Anastassios Nanos @ananos ananos@nubificus.co.uk Core Maintainer"},{"location":"developer-guide/security/","title":"Security Policy","text":"<p>Security is one of the main goals of <code>urunc</code>. If you discover a security issue, we ask that you report it promptly and privately to the maintainers.  This page provides information on when and how to report a vulnerability, along with the description of the process of handling such reports.</p>"},{"location":"developer-guide/security/#security-disclosure-policy","title":"Security disclosure policy","text":"<p>The <code>urunc</code> project follows a responsible disclosure model. All vulnerability reports are reviewed by the <code>urunc</code> maintainers.  If necessary, the report may be shared with other trusted contributors to aid in finding a proper fix. </p>"},{"location":"developer-guide/security/#reporting-vulnerabilities","title":"Reporting Vulnerabilities","text":"<p>Please do not open public issues or PRs to report a vulnerability. Instead, use the private vulnerability reporting of the <code>urunc</code>'s Github repository. In particular, in <code>urunc</code>'s repository page, navigate to the Security tab, click <code>Advisories</code> and then <code>Report a vulnerability</code>. Alternatively, the report can be filed via email at <code>security@urunc.io</code>. This address delivers your message securely to all maintainers.</p>"},{"location":"developer-guide/security/#vulnerability-handling-process","title":"Vulnerability handling process","text":"<p>Upon the receipt of a vulnerability report, the following process will take place:</p> <ul> <li>the <code>urunc</code> maintainers will acknowledge and analyze the report within 48   hours</li> <li>A timeline (embargo period) will be agreed upon with the reporter(s) to keep   the vulnerability confidential until a fix is ready</li> <li>The maintainers will prioritize and begin addressing the issue. They may   request additional details or involve trusted contributors to help resolve   the problem securely</li> <li>Reporters are encouraged to participate in solution design or testing. The   maintainers will keep them updated throughout the process</li> <li>At the end of the timeline: a) a proper fix will be merged, b) a new (patched)   version of <code>urunc</code> will get released and c) a public advisory will get published,   giving credits to the reporter(s), unless they prefer to remain anonymous</li> </ul>"},{"location":"developer-guide/security/#what-to-include-in-the-report","title":"What to include in the report","text":"<p>To help the maintainers assess and resolve the issue efficiently, please use the following template:</p> <pre><code>## Title\n_Short title describing the problem._\n\n## Description\n\n### Summary\n_Short summary of the problem. Make the impact and severity as clear as possible. For example: Supplementary groups are not set up properly inside a container._\n\n### Details\n_Give all details on the vulnerability. Pointing to the incriminated source code is very helpful for the maintainer._\n\n### PoC\n_Complete instructions, including specific configuration details, to reproduce the vulnerability._\n\n### Impact\n_What kind of vulnerability is it? Who is impacted?_\n\n## Affected Products\n\n### Ecosystem\n_Should be something related to Go, C, Github Actions etc._\n\n### Package Name\n_eg. github.com/urunc-dev/urunc_\n\n### Affected Versions\n_eg. &lt; 0.5.0_\n\n### Patched Versions\n_eg. 0.5.1\n\n### Severity\n_eg. Low, Critical etc._\n</code></pre> <p>Also, please use one report per vulnerability and try to keep in touch in case the <code>urunc</code> maintainers require more information.</p>"},{"location":"developer-guide/security/#scope-clarification","title":"Scope clarification","text":"<p>As a sandboxed container runtime, <code>urunc</code> makes use of VM or software based monitors to spawn workloads. Therefore, before submitting a report, please ensure the issue lies within <code>urunc</code> itself and not in the guest (uni)kernel or the monitor. If the vulnerability is in those components, kindly report it to their respective teams. However, if urunc uses those components in a way that introduces a security issue, please report it to the urunc maintainers.</p>"},{"location":"developer-guide/timestamps/","title":"Execution time breakdown","text":"<p>To facilitate performance measurements, a few timestamps have been added to the code base to provide a clear view of the time spent on each part of the execution flow.</p>"},{"location":"developer-guide/timestamps/#timestamps","title":"Timestamps","text":"<p>The timestamps currently depicting each unikernel container execution are the following:</p> Timestamp ID Process Description TS00 create <code>urunc create</code> was invoked TS01 create unikontainer struct created for spec TS02 create initial setup completed TS03 create start reexec process (with or without pty) TS04 reexec <code>urunc create --reexec</code> was invoked TS05 reexec close nsenter pipes and setup base dir TS06 create received pids from nsenter TS07 create executed <code>CreateRuntime</code> hooks TS08 create sent <code>ACK</code> IPC message to <code>reexec</code> process TS09 reexec received <code>ACK</code> message from <code>create</code> TS10 create <code>urunc create</code> terminated TS11 start <code>urunc start</code> was invoked TS12 start unikontainer struct created from spec TS13 start sent <code>START</code> IPC message to <code>reexec</code> TS14 reexec received <code>START</code> message from <code>start</code> TS15 reexec joined sandbox network namespace TS16 reexec network setup completed TS17 reexec disk setup completed TS18 reexec <code>execve</code> the hypervisor process"},{"location":"developer-guide/timestamps/#timestamping-logging-method","title":"Timestamping logging method","text":"<p>To log the timestamps with minimal overhead, we opted to use the zerolog package. We were able to keep the delay caused by the timestamp logging in a low level, around 38351ns for the 20 timestamps required. In comparison, when using logrus the overhead was measured at around 71589ns.</p> <p>Timestamp logging is now handled through a fixed schema using zerolog. The previous logger benchmark suite has been removed, as it is no longer relevant to the current timestamping implementation.</p>"},{"location":"developer-guide/timestamps/#how-to-enable-timestamping","title":"How to enable timestamping","text":"<p>In order to capture the timestamps, a separate <code>containerd-shim</code> and container runtime must be configured in your system.</p> <p>To create the \"timestamping\" version of <code>containerd-shim-urunc-v2</code>:</p> <pre><code>sudo tee -a /usr/local/bin/containerd-shim-uruncts-v2 &gt; /dev/null &lt;&lt; 'EOT'\n#!/bin/bash\nURUNC_TIMESTAMPS=1 /usr/local/bin/containerd-shim-urunc-v2 $@\nEOT\nsudo chmod +x /usr/local/bin/containerd-shim-uruncts-v2\n</code></pre> <p>To add the \"timestamping\" urunc to containerd config:</p> <pre><code>sudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt; 'EOT'\n# timestamping urunc\n[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.uruncts]\n    runtime_type = \"io.containerd.uruncts.v2\"\n    container_annotations = [\"com.urunc.unikernel.*\"]\n    pod_annotations = [\"com.urunc.unikernel.*\"]\n    snapshotter = \"devmapper\"\nEOT\nsudo systemctl restart containerd.service\n</code></pre>"},{"location":"developer-guide/timestamps/#how-to-gather-timestamps","title":"How to gather timestamps","text":"<p>Now we need to run a unikernel using the new container runtime <code>uruncts</code>:</p> <pre><code>sudo nerdctl run --rm --snapshotter devmapper --runtime io.containerd.uruncts.v2 harbor.nbfc.io/nubificus/urunc/hello-hvt-rumprun:latest\n</code></pre> <p>The timestamp logs are located at <code>/tmp/urunc.zlog</code>:</p> <pre><code>$ cat /tmp/urunc.zlog | grep TS\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS00\",\"timestampName\":\"CR.invoked\",\"timestampOrder\":0,\"time\":1703676366849599657}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS01\",\"timestampName\":\"CR.unikontainer_created\",\"timestampOrder\":1,\"time\":1703676366850466038}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS02\",\"timestampName\":\"CR.initial_setup\",\"timestampOrder\":2,\"time\":1703676366850709857}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS03\",\"timestampName\":\"CR.start_reexec\",\"timestampOrder\":3,\"time\":1703676366850900287}\n# ... (rest of the output)\n</code></pre> <p>Note: the timestamp destination (<code>/tmp/urunc.zlog</code>) is hardcoded for the time being.</p>"},{"location":"developer-guide/timestamps/#gathering-the-timestamps","title":"Gathering the timestamps","text":"<p>There are 3 Python utilities inside the <code>script/performance</code> directory to help gather the timestamps.</p>"},{"location":"developer-guide/timestamps/#measure-single-container-execution","title":"Measure single container execution","text":"<p>To gather the timestamps produced by a single unikernel container execution, you can use the <code>measure_single.py</code> script, passing the desired container id.</p> <pre><code>cd urunc/script/performance\npython3 measure_single.py 15c769b9be14c59174626521f7964a8ae06e75c48c5cfd91e2829317c15d455b\n</code></pre> <p>If no container ID is specified, it will return an error:</p> <pre><code>$ python3 measure_single.py \nError: Container ID not specified!\n\nUsage:\n        measure_single.py &lt;CONTAINER_ID&gt;\n</code></pre> <p>Sample output:</p> <pre><code>$ python3 measure_single.py 1bd50216c1709b854f78d50ec36cbbc55e0d4bc2e1509344082b51edc974af6d\nTS00 -&gt; TS01:   1086512 ns\nTS01 -&gt; TS02:   97936 ns\nTS02 -&gt; TS03:   119786 ns\n# ... (rest of the output)\n</code></pre>"},{"location":"developer-guide/timestamps/#automatically-measure-multiple-containers","title":"Automatically measure multiple containers","text":"<p>To automatically gather the timestamps produced by multiple unikernel container executions you can use the <code>measure.py</code> script, passing the desired iterations amount. Make sure to use <code>sudo</code> or execute this script as root, as it relies on <code>nerdctl</code> for spawning the unikernel containers.</p> <pre><code>cd urunc/script/performance\nsudo python3 measure.py 5\n</code></pre> <p>If the amount of iterations is not specified, it will return an error:</p> <pre><code>$ sudo python3 measure.py \nError: Iterations not specified!\n\nUsage:\n        measure.py &lt;ITERATIONS&gt;\n</code></pre> <p>Sample output:</p> <pre><code>$ sudo python3 measure.py 2\n{'TS00 -&gt; TS01': {'average': '11544405 ns',\n                  'maximum': '22292698 ns',\n                  'minimum': '796112 ns'},\n 'TS01 -&gt; TS02': {'average': '127228 ns',\n                  'maximum': '157051 ns',\n                  'minimum': '97405 ns'},\n 'TS02 -&gt; TS03': {'average': '120198 ns',\n                  'maximum': '162634 ns',\n                  'minimum': '77763 ns'},\n# ... (rest of the output)\n</code></pre> <p>The same functionality is provided by <code>measure_to_json.py</code>, but instead of <code>stdout</code> the results are saved in a .json file:</p> <pre><code>$ sudo python3 measure_to_json.py 5 ts.json\n$ cat ts.json | jq\n{\n  \"TS00 -&gt; TS01\": {\n    \"maximum\": \"989525 ns\",\n    \"minimum\": \"474103 ns\",\n    \"average\": \"719644 ns\"\n  },\n  \"TS01 -&gt; TS02\": {\n    \"maximum\": \"212337 ns\",\n    \"minimum\": \"76951 ns\",\n    \"average\": \"122868 ns\"\n# ... (rest of the output)\n</code></pre> <p>If the amount of iterations or output file are not specified, it will return an error:</p> <pre><code>$ sudo python3 measure_to_json.py 5 \nError: Iterations or output file not specified!\n\nUsage:\n        measure_to_json.py &lt;ITERATIONS&gt; &lt;OUTPUT&gt;\n</code></pre>"},{"location":"package/","title":"Building/Packaging unikernels","text":"<p>The OCI (Open Container Initiative) image format is a standardized specification for packaging and distributing containerized applications across different platforms and container runtimes. It defines a common structure for container images, including their metadata, layers, and filesystem content.</p> <p>Since <code>urunc</code> is an OCI-compatible container runtime, it expects the unikernel to be placed inside an OCI container image. Nevertheless, in order to differentiate between traditional container images and unikernel OCI images, <code>urunc</code> makes use of annotations or a metadata file (<code>urunc.json</code>) inside the container's rootfs.</p> <p>To facilitate the process, we provide various tools that build and package a unikernel binary, along with the application's necessary files in a container image and set the respective annotations. In particular, we can produce an OCI image with all <code>urunc</code>'s annotations using:</p> <ol> <li>bunny a tool that builds and packages unikernels     using buildkit's LLB and can also act as a frontend for    buildkit.</li> <li>bunix which uses Nix    packages to package a unikernel as an OCI image.</li> </ol> <p>In this section, we will first explain all the annotations that <code>urunc</code> expects, in order to handle unikernels and describe how to build and package unikernels as OCI images using the aforementioned tools.</p> <p>Quick links:</p> <ul> <li>Packaging pre-built unikernels</li> <li>Using unikernels from existing OCI images</li> <li>Packaging and creating unikernel's rootfs</li> </ul>"},{"location":"package/#annotations","title":"Annotations","text":"<p>OCI annotations are key-value metadata used to describe and provide additional context for container images and runtime configurations within the OCI specification. Using these annotations developers can embed non-essential information about containers, such as version details, licensing, build information, or custom runtime parameters, without affecting the core functionality of the container itself. The annotations can be placed in several components of the specification. However, in the case of <code>urunc</code> we are interested about annotations which can reach the container runtime.</p> <p>Using these annotations <code>urunc</code> receives information regarding the type of the unikernel, the VMM or sandbox mechanism to use and more. For the time being, the required annotations are the following:</p> <ul> <li><code>com.urunc.unikernel.unikernelType</code>: The type of the unikernel. Currently   supported values: a) unikraft, b) rumprun, c) mirage.</li> <li><code>com.urunc.unikernel.hypervisor</code>: The VMM or sandbox monitor to run the   unikernel Currently supported values: a) <code>qemu</code>, b) <code>firecracker</code>, c) <code>spt</code>,   d) <code>hvt</code>.</li> <li><code>com.urunc.unikernel.binary</code>: The path to the unikernel binary inside the   container's rootfs</li> <li><code>com.urunc.unikernel.cmdline</code>: The application's cmdline to pass to the   unikernel.</li> </ul> <p>Except of the above, <code>urunc</code> accepts the following optional annotations:</p> <ul> <li><code>com.urunc.unikernel.initrd</code>: The path to the initrd of the unikernel inside   the container's rootfs.</li> <li><code>com.urunc.unikernel.unikernelVersion</code>: The version of the unikernel framework   (e.g.  0.17.0).</li> <li><code>com.urunc.unikernel.block</code>: The path to a block image inside container's   rootfs, which will get attached to the unikernel.</li> <li><code>com.urunc.unikernel.blkMntPoint</code>: The mount point of the block image to   attach in the unikernel.</li> <li><code>com.urunc.unikernel.mountRootfs</code>: A boolean value that if it is <code>true</code>,   requests from <code>urunc</code> to mount the container's image rootfs in the unikernel   (either as a block device or through shared-fs).</li> </ul> <p>Due to the fact that Docker and some high-level container runtimes do not pass the image annotations to the underlying container runtime, <code>urunc</code> can also read the above information from a file inside the container's rootfs. The file should be named <code>urunc.json</code>, it should be placed in the root directory of the container's rootfs and it should have a JSON format with the above information, where the values are base64 encoded.</p>"},{"location":"package/#tools-to-construct-oci-images-with-uruncs-annotations","title":"Tools to construct OCI images with <code>urunc</code>'s annotations","text":"<p>As previously mentioned we currently provide 2 different tools to build and package unikernels in OCI images with <code>urunc</code>'s annotations.</p>"},{"location":"package/#bunny","title":"bunny","text":"<p>In an effort to simplify the process of building various unikernels, we built bunny. Except of building unikernels bunny can also pack existing unikernels (whether locally or from OCI images) as OCI images for <code>urunc</code>. At its core bunny leverages buildkit's LLB, allowing us to create OCI images from any type of file. Currently bunny can process two formats of files: a) the typical Dockerfile-like syntax files and b) <code>bunnyfile</code>, a specialized YAML-based file.</p> <p>When using Dockerfile-like files, bunny can only package pre-built unikernel images; it cannot build them. This format is primarily retained for compatibility with pun and bima, which are no longer maintained. Currently, bunny can handle the following instructions:</p> <ul> <li><code>FROM</code>: Specify an existing OCI image to use as a base.</li> <li><code>COPY</code>: this works as in Dockerfiles. At this moment, only a single copy   operation per instruction (think one copy per line). These files are copied   inside the container's image rootfs.</li> <li><code>LABEL</code>: all LABEL instructions are added as annotations to the container's   image. They are also added to a special <code>urunc.json</code> inside the container's image   rootfs.</li> </ul> <p>To further extend the functionality and provide a common interface to facilitate unikernel building, we defined <code>bunnyfile</code>. It is a YAML-based special file that bunny transforms to LLB with all the necessary steps to build the respective unikernel. Except of building unikernels, bunny can also be used to build or append files in the unikernel's rootfs.</p> <p>The current syntax of <code>bunnyfile</code> is the following one:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest   # [1] Set bunnyfile syntax for automatic recognition from buildkit.\nversion: v0.1                                   # [2] Bunnyfile version.\n\nplatforms:                                      # [3] The target platform for building/packaging.\n  framework: unikraft                           # [3a] The unikernel framework.\n  version: v0.15.0                              # [3b] The version of the unikernel framework.\n  monitor: qemu                                 # [3c] The hypervisor/VMM or any other kind of monitor.\n  architecture: x86                             # [3d] The target architecture.\n\nrootfs:                                         # [4] (Optional) Specifies the rootfs of the unikernel.\n  from: local                                   # [4a] (Optional) The source of the rootfs.\n  path: initrd                                  # [4b] (Required if from is not scratch) The path in the source, where the prebuilt rootfs file resides.\n  type: initrd                                  # [4c] (optional) The type of rootfs (e.g. initrd, raw, block)\n  include:                                      # [4d] (Optional) A list of local files to include in the rootfs\n    - src:dst\n\nkernel:                                         # [5] Specify a prebuilt kernel to use\n  from: local                                   # [5a] Specify the source of a prebuilt kernel.\n  path: kernel                                  # [5b] The path where the kernel image resides.\n\ncmdline: hello                                  # [6] The cmdline of the app.\n</code></pre> <p>For more information regarding the <code>bunnyfile</code> please take a look at the respective section of bunny's README. Furthermore, you can find various different examples and use cases for bunny in the examples directory of bunny's repository.</p>"},{"location":"package/#packaging-a-unikernel-with-bunny","title":"Packaging a unikernel with bunny","text":"<p>Since bunny uses buildkit it supports two modes of execution. In the first mode it acts as a buildkit frontend and in the second mode it outputs a LLB which can be passed to <code>buildctl</code>.Therefore, bunny depends on buildkit which should be installed. However, if docker is already installed, the frontend execution mode of bunny can be used directly without building or installing anything.</p> <p>It is important to note that if we want to use bunny as a frontend for buildkit we need to start the Containerfile with the following line:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:&lt;version&gt;\n</code></pre> <p>Using a Dockerfile-like syntax file</p> <p>If we want to package a locally built Nginx Unikraft unikernel, we can define the a Dockerfile-like syntax file as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY nginx-qemu-x86_64-initrd_qemu-x86_64 /unikernel/kernel\nCOPY rootfs.cpio /unikernel/initrd\n\nLABEL \"com.urunc.unikernel.binary\"=/unikernel/kernel\nLABEL \"com.urunc.unikernel.initrd\"=/unikernel/initrd\nLABEL \"com.urunc.unikernel.cmdline\"=\"nginx -c /nginx/conf/nginx.conf\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"unikraft\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"qemu\"\n</code></pre> <p>Using bunnyfile</p> <p>If we want to package the same unikernel, using <code>bunnyfile</code>, we have to define the file as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: local\n  path: rootfs.cpio\n\nkernel:\n  from: local\n  path: nginx-qemu-x86_64-initrd_qemu-x86_64\n\ncmdline: nginx -c /nginx/conf/nginx.conf\n</code></pre> <p>and we can build it with a docker command:</p> <pre><code>docker build -f bunnyfile -t nubificus/urunc/nginx-unikraft-qemu:test .\n</code></pre> <p>NOTE: We can use the above command and switch form bunnyfile to the Dockerfile-like file and build the same unikernel OCI image.</p> <p>For more information check bunny's README.</p>"},{"location":"package/#bunix","title":"bunix","text":"<p>For Nix users, we have created a set of Nix scripts that we maintain in the bunix repository to build container images for <code>urunc</code>. In contrast to the previous tools, bunix uses a nix file to define the files to package as a container image, along with the <code>urunc</code> annotations. In particular, this file is the <code>args.nix</code> file, which expects the same fields:</p> <ul> <li>name: the name of the container image that Nix will build</li> <li>tag: the tag of the container image that Nix will build</li> <li>files: a list of key-value pairs with all the files to copy inside the   container image. The key-value pairs have the following format:   <code>\"&lt;path-based-on-cwd&gt;\" = \"&lt;path-inside-container&gt;\"</code>.</li> <li>annotations: a list will all the <code>urunc</code> annotations.</li> </ul>"},{"location":"package/#packaging-a-unikernel-with-bunix","title":"Packaging a unikernel with bunix","text":"<p>A necessary requirement to use bunix is the presence of Nix package manager. Then using bunix is as simple as completing the <code>args.nix</code> file.</p> <p>For example to package a locally built Rumprun Hello world unikernel running on top of Solo5-hvt, we should set the <code>args.nix</code> file as:</p> <pre><code>{\n  name = \"nginx-unikraft-qemu\";\n  tag = \"test\";\n  files = {\n    \"./nginx-qemu-x86_64-initrd_qemu-x86_64\" = \"/unikernel/kernel\";\n    \"./rootfs.cpio\" = \"/unikernel/initrd\";\n  };\n  annotations = {\n    unikernelType = \"unikraft\";\n    hypervisor = \"qemu\";\n    binary = \"/unikernel/kernel\";\n    cmdline = \"nginx -c /nginx/conf/nginx.conf\";\n    unikernelVersion = \"\";\n    initrd = \"/unikernel/initrd\";\n    block = \"\";\n    blkMntPoint = \"\";\n  };\n}\n</code></pre> <p>Then we can build the image by simply running the following command inside the repository:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre> <p>Please check bunix's README for more information.</p>"},{"location":"package/pre-built/","title":"Packaging pre-built unikernels for <code>urunc</code>","text":"<p>In this page we will explain the process of packaging an existing / pre-built unikernel as an OCI image with the necessary annotations for <code>urunc</code>. As an example, we will use a network example over MirageOS from mirage-skeleton targeting Solo5-hvt.</p> <p>For simply packaging pre-built unikernel images, we can use both bunny and bunix.</p> <p>NOTE: The below steps can be easily adjusted to any pre-built unikernel image.</p>"},{"location":"package/pre-built/#using-bunny","title":"Using <code>bunny</code>","text":"<p>In the case of bunny and pre-built unikernel images, we can use both supported file syntaxes: a) <code>bunnyfile</code> and b) the Dockerfile-like syntax.</p>"},{"location":"package/pre-built/#using-a-bunnyfile","title":"Using a <code>bunnyfile</code>","text":"<p>In order to package an existing pre-built unikernel image with bunny and a <code>bunnyfile</code> we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: mirage\n  monitor: hvt\n  architecture: x86\n\nkernel:\n  from: local\n  path: network.hvt\n\ncmdline: \"\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a MirageOS unikernel that   will execute on top of Solo5-hvt over x86   architecture.</li> <li>We want to use the <code>network.hvt</code> binary as the unikernel to boot.</li> <li>We do not specify any command line, since the unikernel does not necessarily require one.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/network-mirage-hvt:test .\n</code></pre>"},{"location":"package/pre-built/#using-a-dockerfile-like-syntax","title":"Using a Dockerfile-like syntax","text":"<p>In order to package an existing pre-built unikernel image with bunny and a Dockerfile-like syntax file, we can define the <code>Containerfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY network.hvt /unikernel/network.hvt\n\nLABEL com.urunc.unikernel.binary=/unikernel/network.hvt\nLABEL \"com.urunc.unikernel.cmdline\"=\"\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"mirage\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"hvt\"\nLABEL \"com.urunc.unikernel.mountRootfs\"=\"false\"\n</code></pre> <p>In the above file:</p> <ul> <li>We directly copy the unikernel binary in the OCI's image rootfs.</li> <li>We manually specify through labels the <code>urunc</code> annotations.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/network-mirage-hvt:test .\n</code></pre>"},{"location":"package/pre-built/#using-bunix","title":"Using <code>bunix</code>","text":"<p>In the case of bunix we need to clone the whole repository in the same directly as the unikernel. Then, we simply need to edit the <code>args.nix</code> file as:</p> <pre><code>{\n  name = \"urunc/prebuilt/network-mirage-hvt\";\n  tag = \"test\";\n  files = {\n    \"./network.hvt\" = \"/unikernel/network.hvt\";\n  };\n  annotations = {\n    unikernelType = \"mirage\";\n    hypervisor = \"hvt\";\n    binary = \"/unikernel/network.hvt\";\n    cmdline = \"\";\n    unikernelVersion = \"\";\n    initrd = \"\";\n    block = \"\";\n    blkMntPoint = \"\";\n    mountRootfs = \"false\";\n  };\n}\n</code></pre> <p>In the above file:</p> <ul> <li>We directly specify the files to copy inside the OCI's image rootfs.</li> <li>We manually specify all <code>urunc</code> annotations.</li> </ul> <p>We can build the OCI image by simply running the following command:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre>"},{"location":"package/reuse/","title":"Reusing OCI images that contain unikernels","text":"<p>In this page we will explain how we can reuse existing OCI images that contain unikernels to either update or append <code>urunc</code> annotations. As an example, we will use an existing Unikraft Unikernel image from Unikraft's catalog, The goal will be to transform this image to an OCI image that <code>urunc</code> can handle, by simply appending the necessary annotations.</p> <p>Currently only <code>bunny</code> supports reusing an existing OCI image. However, both file formats, <code>bunnyfile</code> and Dockerfile-like syntax files, can be used.</p> <p>NOTE: The below steps can be easily adjusted to any existing OCI image.</p>"},{"location":"package/reuse/#using-a-bunnyfile","title":"Using a <code>bunnyfile</code>","text":"<p>In order to append <code>urunc</code> annotations in an existing Unikraft OCI image, we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nkernel:\n  from: unikraft.org/nginx:1.15\n  path: /unikraft/bin/kernel\n\ncmdline: \"nginx -c /nginx/conf/nginx.conf\"\n</code></pre> <p>In the above file we specify the followings:</p> <ul> <li>We want to use a Unikraft unikernel that will execute on top of Qemu over x86   architecture.</li> <li>We want to use the unikernel binary <code>/unikraft/bin/kernel</code> from the   <code>unikraft.org/nginx:1.15</code> OCI image.</li> <li>We specify the cmdline for the unikernel as <code>nginx -c /nginx/conf/nginx.conf\"</code></li> </ul> <p>With the above file, <code>bunny</code> will fetch the OCI image and append the <code>urunc</code> annotations. We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/reuse/nginx-unikraft-qemu:test .\n</code></pre>"},{"location":"package/reuse/#using-a-dockerfile-like-syntax","title":"Using a Dockerfile-like syntax","text":"<p>In the case of the Dockerfile-like syntax file, we need to manually specify the <code>urunc</code> annotations, using the respective labels. Therefore, to transform the above <code>bunnyfile</code> to the equivalent <code>Containerfile</code>:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM unikraft.org/nginx:1.15\n\nLABEL com.urunc.unikernel.binary=\"/unikraft/bin/kernel\"\nLABEL \"com.urunc.unikernel.cmdline\"=\"nginx -c /nginx/conf/nginx.conf\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"unikraft\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"qemu\"\n</code></pre> <p>In the above file:</p> <ul> <li>We set the <code>unikraft.org/nginx:1.15</code> as the base for our OCI image.</li> <li>We manually specify through labels the <code>urunc</code> annotations.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/nginx-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/","title":"Packaging and creating unikernel's rootfs for <code>urunc</code>","text":"<p>The unikernel and libOS ecosystem is highly diverse, with each framework offering its own approach to storage. The users can easily get lost on the various storage technologies that each framework supports.  This challenge was one of the key reasons we created bunny and bunix, in an effort to simplify the process and provide a unified interface for managing storage across all unikernel frameworks. On this page, we will explore the current state of our tools and explain how to use them to create and package a root filesystem (rootfs) for a unikernel.</p> <p>For the time being, <code>urunc</code> supports three ways for passing the rootfs to the unikernel: a) through initrd, b) as a virtio-block and c) through shared-fs. In the virtio-block case, <code>urunc</code> can either levarage the container's snapshot and pass the whole container's rootfs as the rootfs, or <code>urunc</code> can make use of a user-created file inside the OCI image to pass as a virtio-block device to the unikernel.</p> <p>Therefore, the users have the following options:</p> <ol> <li>Manually create a rootfs (either initrd or block) and package it along with    the unikernel.</li> <li>Directly copy all the files to the container's rootfs and use devmapper    snapshotter or shared-fs to pass the container's rootfs to the unikernel.</li> <li>Let bunny and bunix create the rootfs file.</li> </ol> <p>NOTE: For the time being, bunny supports the creation of initrd files and bunix does not provide any support for creating the rootfs.</p>"},{"location":"package/rootfs/#creating-an-initrd-file","title":"Creating an initrd file","text":"<p>Some unikernel frameworks and guests support an in-memory ramfs as a rootfs. In these cases, we can use bunny and instruct it to create the rootfs for us with all the specified files. This feature is only supported using a <code>bunnyfile</code> and not a Dockerfile-like syntax file.</p> <p>Let's take a look at it using a Redis Unikraft unikernel as an example, targeting Qemu. We will define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: scratch\n  type: initrd\n  include:\n  - redis.conf:/conf/redis.conf\n\nkernel:\n  from: local\n  path: redis-qemu-x86_64-initrd_qemu-x86_64\n\ncmdline: \"redis-server /conf/redis.conf\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a Unikraft unikernel that will execute on top of Qemu over   x86 architecture.</li> <li>We want to create from <code>scratch</code> a rootfs with <code>initrd</code> as its type. In   particular, we want a initrd file that contains the file <code>redis.conf</code> in   <code>/data/conf/redis.conf</code>. In that way, bunny creates the initrd file for us   and sets up the respective <code>urunc</code> annotations to attach this initrd file   when we boot the unikernel.</li> <li>We want to use the <code>redis-qemu-x86_64-initrd_qemu-x86_64</code> binary from the local build context as the unikernel to boot.</li> <li>We specify the cmdline for the unikernel as <code>redis-server /conf/redis.conf</code></li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/redis-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/#preparing-an-oci-image-to-be-used-as-a-rootfs-for-the-unikernel","title":"Preparing an OCI image to be used as a rootfs for the unikernel","text":"<p>As previously mentioned, <code>urunc</code> is able to pass the whole container's rootfs as the rootfs for the unikernels that support virtio-block or shared-fs. In that scenario, we simply need to copy any local files into the OCI image's rootfs. For this scenario we can use both bunny and bunix.</p> <p>NOTE: In case the unikernel does not supports shared-fs (e.g. 9pfs/virtiofs), we can only use block devices and for that reason we need to create the unikernel container using devmapper as a snapshotter. In that way,<code>urunc</code> will use the snapshot of the container's image and directly attach it to the unikernel as  a block device.</p> <p>As an example, we will use a Redis Rumprun unikernel from Rumprun-packages targeting Solo5-hvt.</p> <p>NOTE: Rumprun does not support attaching a block directly to <code>/</code>, hence <code>urunc</code> will instruct Rumprun to mount it at <code>/data</code>.</p>"},{"location":"package/rootfs/#using-bunny","title":"Using <code>bunny</code>","text":"<p>In the case of bunny, we can use both supported file syntaxes: a) <code>bunnyfile</code> and b) the Dockerfile-like syntax.</p>"},{"location":"package/rootfs/#using-a-bunnyfile","title":"Using a <code>bunnyfile</code>","text":"<p>In order to package an existing pre-built unikernel image and any other files with bunny and a <code>bunnyfile</code> we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: rumprun\n  monitor: hvt\n  architecture: x86\n\nrootfs:\n  from: scratch\n  type: raw\n  include:\n  - redis.conf:/conf/redis.conf\n\nkernel:\n  from: local\n  path: redis.hvt\n\ncmdline: \"redis-server /data/conf/redis.conf\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a Rumprun unikernel   that will execute on top of Solo5-hvt over x86   architecture.</li> <li>We want to create a rootfs from <code>scratch</code> with a <code>raw</code> type, meaning that   we will just copy the   specified files directly to the OCI image's rootfs. In particular, we copy the   file <code>redis.conf</code> and place it at <code>/conf/redis.conf</code>.This is similar to   <code>COPY</code> in Dockerfile.  Because of this type selection, bunny will also set up   the respective annotations to mount the OCI images rootfs directly to the   unikernel.</li> <li>We want to use the <code>redis.hvt</code> binary as the unikernel to boot.</li> <li>We specify the cmdline for the unikernel as <code>redis-server  /data/conf/redis.conf</code></li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/redis-rumprun-hvt:test .\n</code></pre>"},{"location":"package/rootfs/#using-a-dockerfile-like-syntax","title":"Using a Dockerfile-like syntax","text":"<p>In order to package an existing pre-built unikernel image and any other files with bunny using a Dockerfile-like syntax file, we can define the <code>Containerfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:0.2.0\nFROM scratch\n\nCOPY redis.hvt /unikernel/redis.hvt\nCOPY redis.conf /conf/redis.conf\n\nLABEL com.urunc.unikernel.binary=/unikernel/redis.hvt\nLABEL \"com.urunc.unikernel.cmdline\"=\"redis-server /data/conf/redis.conf\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"rumprun\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"hvt\"\nLABEL \"com.urunc.unikernel.mountRootfs\"=\"true\"\n</code></pre> <p>In the above file:</p> <ul> <li>We directly copy the unikernel binary and any files that we want to have in   the OCI's image rootfs.</li> <li>We manually specify through labels the necessary <code>urunc</code> annotations,   including the <code>com.urunc.unikernel.mountRootfs</code> which instructs <code>urunc</code> to   attach the container snapshot as a block device to the unikernel.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/redis-rumprun-hvt:test .\n</code></pre>"},{"location":"package/rootfs/#using-bunix","title":"Using <code>bunix</code>","text":"<p>In the case of bunix we need the whole repository in the same directly as the unikernel. Then, we simply need to edit the <code>args.nix</code> file. For our pre-built Redis Rumprun unikernel we can define the files as:</p> <pre><code>{\n  name = \"urunc/prebuilt/redis-rumprun-hvt\";\n  tag = \"test\";\n  files = {\n    \"./redis.hvt\" = \"/unikernel/redis.hvt\";\n    \"./redis.conf\" = \"/conf/redis.conf\";\n  };\n  annotations = {\n    unikernelType = \"rumprun\";\n    hypervisor = \"hvt\";\n    binary = \"/unikernel/redis.hvt\";\n    cmdline = \"hello\";\n    unikernelVersion = \"\";\n    initrd = \"\";\n    block = \"\";\n    blkMntPoint = \"\";\n    mountRootfs = \"true\";\n  };\n}\n</code></pre> <p>In the above file:</p> <ul> <li>We directly specify the files to copy inside the OCI's image rootfs.</li> <li>We manually specify through labels the necessary <code>urunc</code> annotations,   including the <code>com.urunc.unikernel.mountRootfs</code> which instructs <code>urunc</code> to   attach the container snapshot as a block device to the unikernel.</li> </ul> <p>We can build the OCI image by simply running the following command:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre>"},{"location":"package/rootfs/#packaging-a-pre-built-rootfs-along-with-the-unikernel","title":"Packaging a pre-built rootfs along with the unikernel","text":"<p>At last, there is always the option to manually create the rootfs file for the unikernel and then package the unikernel binary and the rootfs file setting up the respective annotations.</p> <p>As an example, we will use a simple  C HTTP Web Server from Unikraft's catalog.</p>"},{"location":"package/rootfs/#using-bunny_1","title":"Using <code>bunny</code>","text":"<p>In the case of bunny, we can use both supported file syntaxes: a) <code>bunnyfile</code> and b) the Dockerfile-like syntax.</p>"},{"location":"package/rootfs/#using-a-bunnyfile_1","title":"Using a <code>bunnyfile</code>","text":"<p>In order to package an existing pre-built unikernel and its rootfs with bunny and a <code>bunnyfile</code> we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: local\n  path: rootfs.cpio\n\nkernel:\n  from: local\n  path: app-elfloader-qemu-x86_64-initrd_qemu-x86_64\n\ncmdline: \"/chttp\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a Unikraft unikernel that will execute   on top of Qemu over x86 architecture.</li> <li>We want to use a local file as a rootfs, specifically the <code>rootfs.cpio</code> file   in the local build context.</li> <li>We want to use the <code>app-elfloader-qemu-x86_64-initrd_qemu-x86_64</code> binary as   the unikernel to boot.</li> <li>We specify the cmdline for the unikernel as <code>/chttp</code></li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/chttp-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/#using-a-dockerfile-like-syntax_1","title":"Using a Dockerfile-like syntax","text":"<p>We can do all the above using a Dockerfile-like syntax file as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY app-elfloader-qemu-x86_64-initrd_qemu-x86_64 /unikernel/kernel\nCOPY rootfs.cpio /unikernel/rootfs.cpio\n\nLABEL \"com.urunc.unikernel.binary\"=/unikernel/kernel\nLABEL \"com.urunc.unikernel.initrd\"=\"/unikernel/rootfs.cpio\"\nLABEL \"com.urunc.unikernel.cmdline\"=\"/chttp\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"unikraft\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"qemu\"\n</code></pre> <p>In the above file:</p> <ul> <li>We directly copy the unikernel binary and the cpio file in   the OCI's image rootfs.</li> <li>We manually specify all <code>urunc</code> annotations, including the initrd one to   specify the file to use as initrd for the unikernel.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/chttp-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/#using-bunix_1","title":"Using <code>bunix</code>","text":"<p>In the case of bunix we need the whole repository in the same directory as the unikernel and the cpio file. Then, we simply need to edit the <code>args.nix</code> file as:</p> <pre><code>{\n  name = \"urunc/prebuilt/chttp-unikraft-qemu\";\n  tag = \"test\";\n  files = {\n    \"./app-elfloader-qemu-x86_64-initrd_qemu-x86_64\" = \"/unikernel/kernel\";\n    \"./rootfs.cpio\" = \"/unikernel/rootfs.cpio\";\n  };\n  annotations = {\n    unikernelType = \"unikraft\";\n    hypervisor = \"qemu\";\n    binary = \"/unikernel/kernel\";\n    cmdline = \"/chttp\";\n    unikernelVersion = \"\";\n    initrd = \"/unikernel/rootfs.cpio\";\n    block = \"\";\n    blkMntPoint = \"\";\n    mountRootfs = \"\";\n  };\n}\n</code></pre> <p>In the above file:</p> <ul> <li>We directly specify the files to copy inside the OCI's image rootfs.</li> <li>We manually specify all <code>urunc</code> annotations, including the initrd one to   specify the file to use as initrd for the unikernel.</li> </ul> <p>We can build the OCI image by simply running the following command:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section we include some end-to-end tutorials on deploying <code>urunc</code>-compatible workloads to various environments:</p> <ul> <li><code>urunc</code> in Kubernetes</li> <li><code>urunc</code> in EKS</li> <li>Knative integration</li> <li>Non root monitor execution</li> <li>Running existing containers over Linux</li> <li>Running vAccel-enabled Containers with <code>urunc</code></li> </ul>"},{"location":"tutorials/How-to-urunc-on-k8s/","title":"How to use urunc with k8s","text":"<p>This guide assumes you have a working Kubernetes cluster.</p> <p>To use <code>urunc</code> in a k8s cluster there are 2 options:</p> <ul> <li>Manual installation</li> <li>Using urunc-deploy</li> </ul>"},{"location":"tutorials/How-to-urunc-on-k8s/#manual-installation","title":"Manual Installation","text":""},{"location":"tutorials/How-to-urunc-on-k8s/#install-urunc","title":"Install urunc","text":"<p>Before we start, we need to have working Kubernetes cluster with urunc installed on one or more nodes.</p>"},{"location":"tutorials/How-to-urunc-on-k8s/#add-urunc-as-a-runtimeclass","title":"Add urunc as a RuntimeClass","text":"<p>First, we need to add <code>urunc</code> as a runtime class for the k8s cluster:</p> <pre><code>cat &lt;&lt; EOF | tee urunc-runtimeClass.yaml\nkind: RuntimeClass\napiVersion: node.k8s.io/v1\nmetadata:\n    name: urunc\nhandler: urunc\nEOF\n\nkubectl apply -f urunc-runtimeClass.yaml\n</code></pre> <p>To verify the runtimeClass was added:</p> <pre><code>kubectl get runtimeClass\n</code></pre>"},{"location":"tutorials/How-to-urunc-on-k8s/#create-a-test-deployment","title":"Create a test deployment","text":"<p>To properly test the newly added k8s runtime class, create a test deployment:</p> <pre><code>cat &lt;&lt;EOF | tee nginx-urunc.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx-urunc\n  name: nginx-urunc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-urunc\n  template:\n    metadata:\n      labels:\n        run: nginx-urunc\n    spec:\n      runtimeClassName: urunc\n      containers:\n      - image: harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n        imagePullPolicy: Always\n        name: nginx-urunc\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-urunc\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx-urunc\n  sessionAffinity: None\n  type: ClusterIP\nEOF\n\nkubectl apply -f nginx-urunc.yaml\n</code></pre> <p>Now, we should be able to see the created Pod:</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"tutorials/How-to-urunc-on-k8s/#urunc-deploy","title":"urunc-deploy","text":"<p><code>urunc-deploy</code> provides a Dockerfile, which contains all of the binaries and artifacts required to run <code>urunc</code>, as well as reference DaemonSets, which can be utilized to install <code>urunc</code> runtime on a running Kubernetes cluster.</p>"},{"location":"tutorials/How-to-urunc-on-k8s/#urunc-deploy-in-k3s","title":"urunc-deploy in k3s","text":"<p>To install in a k3s cluster, first we need to create the RBAC:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre> <p>Then, we create the <code>urunc-deploy</code> Daemonset, followed by the k3s customization:</p> <pre><code>kubectl apply -k https://github.com/urunc-dev/urunc//deployment/urunc-deploy/urunc-deploy/overlays/k3s?ref=main\n</code></pre> <p>Finally, we need to create the appropriate k8s runtime class:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre> <p>To uninstall:</p> <pre><code>kubectl delete -k https://github.com/urunc-dev/urunc//deployment/urunc-deploy/urunc-deploy/overlays/k3s?ref=main\nkubectl apply -k https://github.com/urunc-dev/urunc//deployment/urunc-deploy/urunc-cleanup/overlays/k3s?ref=main\n</code></pre> <p>After the cleanup is completed and the <code>urunc-deploy</code> Pod is terminated:</p> <pre><code>kubectl delete -k https://github.com/urunc-dev/urunc//deployment/urunc-deploy/urunc-cleanup/overlays/k3s?ref=main\nkubectl delete -f https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\nkubectl delete -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre>"},{"location":"tutorials/How-to-urunc-on-k8s/#urunc-deploy-in-k8s-with-containerd","title":"urunc-deploy in k8s with containerd","text":"<p>To install in a k8s cluster, first we need to create the RBAC:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre> <p>Then, we create the <code>urunc-deploy</code> Daemonset:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-deploy/base/urunc-deploy.yaml\n</code></pre> <p>Finally, we need to create the appropriate k8s runtime class:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre> <p>To uninstall:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-deploy/base/urunc-deploy.yaml\nkubectl apply -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-cleanup/base/urunc-cleanup.yaml\n</code></pre> <p>After the cleanup is completed:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-cleanup/base/urunc-cleanup.yaml\nkubectl delete -f https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\nkubectl delete -f https://raw.githubusercontent.com/urunc-dev/urunc/main/deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre> <p>Now, we can create new <code>urunc</code> deployments using the instruction provided in manual installation.</p>"},{"location":"tutorials/How-to-urunc-on-k8s/#how-urunc-deploy-works","title":"How urunc-deploy works","text":"<p><code>urunc-deploy</code> consists of several components and steps that install <code>urunc</code> along with the supported hypervisors, configure <code>containerd</code> and Kubernetes (k8s) to use <code>urunc</code>, and provide a simple way to remove those components from the cluster.</p> <p>The daemonset automatically installs all required artifacts under <code>/opt/urunc</code> and configures <code>urunc</code> via a configuration file at <code>/etc/urunc/config.toml</code>.</p> <p>During installation, the following steps take place:</p> <ul> <li>A RBAC role is created to allow <code>urunc-deploy</code> to run with privileged access.</li> <li>The <code>urunc-deploy</code> Pod is deployed with privileges on the host, and the <code>containerd</code> configuration is mounted inside the Pod.</li> <li><code>urunc-deploy</code> performs the following tasks:<ul> <li>Copies <code>urunc</code> and <code>containerd-shim-urunc-v2</code> binaries to the host under <code>/usr/local/bin</code>.</li> <li>Copies hypervisor binaries to the host under <code>/opt/urunc/bin</code>.</li> <li>Copies QEMU data files to <code>/opt/urunc/share</code>.</li> <li>Installs a configuration file at <code>/etc/urunc/config.toml</code>.</li> <li>Creates a backup of the current <code>containerd</code> configuration file.</li> <li>Edits the <code>containerd</code> configuration file to add <code>urunc</code> as a supported runtime.</li> <li>Restarts <code>containerd</code>, if necessary.</li> <li>Labels the Node with label <code>urunc.io/urunc-runtime=true</code>.</li> </ul> </li> <li>Finally, <code>urunc</code> is added as a runtime class in k8s.</li> </ul> <p>During cleanup, these changes are reverted:</p> <ul> <li>The <code>urunc</code> and <code>containerd-shim-urunc-v2</code> binaries are deleted from <code>/usr/local/bin</code>.</li> <li>The <code>/opt/urunc</code> directory containing hypervisor binaries and QEMU data files is deleted.</li> <li>The <code>/etc/urunc</code> configuration directory is deleted.</li> <li>The <code>containerd</code> configuration file is restored to the pre-<code>urunc-deploy</code> state.</li> <li>The <code>urunc.io/urunc-runtime=true</code> label is removed from the Node.</li> <li>The RBAC role, the <code>urunc-deploy</code> Pod and the runtime class are removed.</li> </ul>"},{"location":"tutorials/Non-root-monitor-execution/","title":"Non-root execution of monitor","text":"<p>To enhance security, <code>urunc</code> supports running the monitor process (VMM or seccomp monitor) as a non-root user. This can be as simple as setting the respective uid/gid for the execution of the container.</p>"},{"location":"tutorials/Non-root-monitor-execution/#running-the-monitor-as-non-root-user","title":"Running the monitor as non-root user","text":"<p>By default <code>urunc</code> will execute the monitor setting up the <code>uid</code>, <code>gid</code> and <code>additionalGids</code> from the container's OCI configuration. As a result, we simply need to instruct <code>urunc</code> to run a container as the desired user.</p>"},{"location":"tutorials/Non-root-monitor-execution/#docker-and-nerdctl","title":"Docker and Nerdctl","text":"<p>In the case of docker and nerdctl, we can set the user and the groups of the container with the <code>--user &lt;uid&gt;:&lt;gid&gt;</code> option and the additional groups using <code>--group-add &lt;gid&gt;</code> for each additional group. Therefore, to run a KVM-enabled monitor with <code>urunc</code> as <code>nobody</code>, we use the following command:</p> <pre><code>sudo nerdctl run  --user 65534:65534 --runtime \"io.containerd.urunc.v2\" --rm -it harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n</code></pre> <p>Note The commands are the same for docker.</p>"},{"location":"tutorials/Non-root-monitor-execution/#in-a-k8s-deployment","title":"In a k8s deployment","text":"<p>Similarly, in the case of Kubernetes, we can specify the monitor's process user and groups by defining the container's user and groups. We can do that in the <code>securityContext</code> field of the deployment yaml:</p> <pre><code>securityContext:\n  runAsUser: 65534\n  runAsGroup: 65534\n  supplementalGroups: [1000]\n</code></pre> <p>For more information regarding the Security Context of a Pod / Container take a look at Kubernetes's documentation.</p>"},{"location":"tutorials/Running-urunc-with-kind/","title":"Running urunc with kind (Kubernetes in Docker)","text":"<p>This guide provides a step-by-step process to set up and run the <code>urunc</code> runtime with a <code>kind</code> (Kubernetes in Docker) cluster on an Ubuntu 22.04 host and  deploy a test NGINX unikernel.</p>"},{"location":"tutorials/Running-urunc-with-kind/#prerequisites","title":"Prerequisites","text":"<ul> <li>Host: Ubuntu 22.04 with <code>sudo</code> privileges.</li> <li>Tools: <code>docker</code>, <code>kind</code>, and <code>kubectl</code> installed.</li> <li>KVM: Host must support KVM and nested virtualization.</li> </ul>"},{"location":"tutorials/Running-urunc-with-kind/#overview","title":"Overview","text":"<p>The goal is to: 1. Configure a <code>kind</code> cluster with KVM access for <code>urunc</code>. 2. Install <code>urunc</code> and required hypervisors (QEMU, Firecracker, Solo5) inside the <code>kind</code> node. 3. Set up <code>containerd</code> to use <code>urunc</code> as a runtime. 4. Deploy a test NGINX unikernel Pod using the <code>urunc</code> runtime.</p>"},{"location":"tutorials/Running-urunc-with-kind/#steps","title":"Steps","text":""},{"location":"tutorials/Running-urunc-with-kind/#step-1-enable-kvm-and-nested-virtualization","title":"Step 1: Enable KVM and Nested Virtualization","text":"<p>Ensure the host supports KVM and nested virtualization.</p> <ol> <li> <p>Check KVM support:</p> <pre><code>lsmod | grep kvm\n</code></pre> <p>If you see <code>kvm_intel</code> or <code>kvm_amd</code>, KVM is loaded. If not, install it:</p> <pre><code>sudo apt update\nsudo apt install -y qemu-kvm\nsudo adduser $USER kvm\n</code></pre> </li> <li> <p>Enable nested virtualization:</p> <p>There are some differences on how to enable nested virtualization depending on your CPU:</p> <ul> <li> <p>For Intel:</p> <pre><code>echo \"options kvm-intel nested=Y\" | sudo tee /etc/modprobe.d/kvm-nested.conf\nsudo modprobe -r kvm_intel\nsudo modprobe kvm_intel\ncat /sys/module/kvm_intel/parameters/nested\n</code></pre> <p>Output should be <code>Y</code> or <code>1</code>.</p> </li> <li> <p>For AMD:</p> <pre><code>echo \"options kvm-amd nested=1\" | sudo tee /etc/modprobe.d/kvm-nested.conf\nsudo modprobe -r kvm_amd\nsudo modprobe kvm_amd\ncat /sys/module/kvm_amd/parameters/nested\n</code></pre> <p>Output should be <code>1</code>.</p> </li> </ul> </li> <li> <p>Verify KVM:</p> <pre><code>sudo apt install -y cpu-checker\nkvm-ok\n</code></pre> <p>Output should include \"KVM acceleration can be used\".</p> </li> </ol>"},{"location":"tutorials/Running-urunc-with-kind/#step-2-create-a-kind-cluster-with-kvm-access","title":"Step 2: Create a kind Cluster with KVM Access","text":"<p>Configure the <code>kind</code> cluster to allow KVM access for running unikernels.</p> <ol> <li> <p>Delete existing cluster (if any):    <pre><code>kind delete cluster --name urunc-test\n</code></pre></p> </li> <li> <p>Create kind-config.yaml:    <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraMounts:\n      - hostPath: /dev/kvm\n        containerPath: /dev/kvm\n</code></pre></p> </li> <li> <p>Create the cluster:    <pre><code>kind create cluster --name urunc-test --config kind-config.yaml\n</code></pre></p> </li> <li> <p>Verify KVM access:    <pre><code>docker exec urunc-test-control-plane ls /dev/kvm\n</code></pre>    Output should show <code>/dev/kvm</code>.</p> </li> </ol>"},{"location":"tutorials/Running-urunc-with-kind/#step-3-install-urunc-and-dependencies-in-the-kind-node","title":"Step 3: Install urunc and Dependencies in the kind Node","text":"<p>Install <code>urunc</code>, hypervisors, and dependencies inside the <code>kind</code> node container.</p> <ol> <li> <p>Access the node container:    <pre><code>docker exec -it urunc-test-control-plane bash\n</code></pre></p> </li> <li> <p>Install basic dependencies:    <pre><code>apt update\napt install -y git wget build-essential libseccomp-dev pkg-config bc\n</code></pre></p> </li> <li> <p>Verify runc (already installed in kind):    <pre><code>which runc\n</code></pre>    If not found, install it:    <pre><code>RUNC_VERSION=$(curl -s https://api.github.com/repos/opencontainers/runc/releases/latest | grep 'tag_name' | cut -d\\\" -f4 | sed 's/v//')\nwget -q https://github.com/opencontainers/runc/releases/download/v$RUNC_VERSION/runc.$(dpkg --print-architecture)\nchmod +x runc.$(dpkg --print-architecture)\nmv runc.$(dpkg --print-architecture) /usr/local/sbin/runc\n</code></pre></p> </li> <li> <p>Configure containerd:    Ensure <code>containerd</code> is running:    <pre><code>systemctl status containerd || containerd &amp;\n</code></pre>    Create default config:    <pre><code>mkdir -p /etc/containerd\ncontainerd config default &gt; /etc/containerd/config.toml\n</code></pre></p> </li> <li> <p>Use overlayfs snapshotter:    Since <code>devmapper</code> is not supported in <code>kind</code> containers, use <code>overlayfs</code>:    <pre><code>sed -i 's/snapshotter = \"devmapper\"/snapshotter = \"overlayfs\"/' /etc/containerd/config.toml\n</code></pre></p> </li> <li> <p>Install Go:    <pre><code>GO_VERSION=1.24.6\nwget -q https://go.dev/dl/go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\nmkdir /usr/local/go${GO_VERSION}\ntar -C /usr/local/go${GO_VERSION} -xzf go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\necho \"export PATH=\\$PATH:/usr/local/go$GO_VERSION/go/bin\" &gt;&gt; /etc/profile\nsource /etc/profile\nrm -f go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\n</code></pre></p> </li> <li> <p>Install urunc:    <pre><code>git clone https://github.com/nubificus/urunc.git\ncd urunc\nmake\nmake install\ncd ..\n</code></pre></p> </li> <li> <p>Install hypervisors:    <pre><code># QEMU\napt install -y qemu-system\n\n# Firecracker\nARCH=\"$(uname -m)\"\nVERSION=\"v1.7.0\"\ncurl -L https://github.com/firecracker-microvm/firecracker/releases/download/${VERSION}/firecracker-${VERSION}-${ARCH}.tgz | tar -xz\nmv release-${VERSION}-${ARCH}/firecracker-${VERSION}-${ARCH} /usr/local/bin/firecracker\nrm -fr release-${VERSION}-${ARCH}\n\n# Solo5\ngit clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh &amp;&amp; make -j$(nproc)\ncp tenders/hvt/solo5-hvt /usr/local/bin\ncp tenders/spt/solo5-spt /usr/local/bin\ncd ..\n</code></pre></p> </li> <li> <p>Add urunc to containerd:    <pre><code>cat &lt;&lt;EOF | tee -a /etc/containerd/config.toml\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.urunc]\n  runtime_type = \"io.containerd.urunc.v2\"\n  container_annotations = [\"com.urunc.unikernel.*\"]\n  pod_annotations = [\"com.urunc.unikernel.*\"]\n  snapshotter = \"overlayfs\"\nEOF\nsystemctl restart containerd || containerd &amp;\n</code></pre></p> </li> <li> <p>Exit the container:     <pre><code>exit\n</code></pre></p> </li> </ol>"},{"location":"tutorials/Running-urunc-with-kind/#step-4-create-runtimeclass","title":"Step 4: Create RuntimeClass","text":"<p>Define the <code>urunc</code> RuntimeClass for Kubernetes.</p> <ol> <li> <p>Create urunc-runtimeClass.yaml:    <pre><code>cat &lt;&lt; EOF | tee urunc-runtimeClass.yaml\nkind: RuntimeClass\napiVersion: node.k8s.io/v1\nmetadata:\n   name: urunc\nhandler: urunc\nEOF\n</code></pre></p> </li> <li> <p>Apply the RuntimeClass:    <pre><code>kubectl apply -f urunc-runtimeClass.yaml\n</code></pre></p> </li> <li> <p>Verify:    <pre><code>kubectl get runtimeClass\n</code></pre></p> </li> </ol>"},{"location":"tutorials/Running-urunc-with-kind/#step-5-deploy-nginx-unikernel","title":"Step 5: Deploy NGINX Unikernel","text":"<ol> <li> <p>Create nginx-urunc.yaml:    <pre><code>cat &lt;&lt;EOF | tee nginx-urunc.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-urunc\n  labels:\n    run: nginx-urunc\nspec:\n  runtimeClassName: urunc\n  containers:\n    - name: nginx\n      image: harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n      imagePullPolicy: Always\n      ports:\n        - containerPort: 80\n          protocol: TCP\n      resources:\n        requests:\n          cpu: 10m\nEOF\n</code></pre></p> </li> <li> <p>Apply nginx-urunc.yaml:    <pre><code>kubectl apply -f nginx-urunc.yaml\n</code></pre></p> </li> </ol>"},{"location":"tutorials/Running-urunc-with-kind/#step-6-verify-the-deployment","title":"Step 6: Verify the Deployment","text":"<ol> <li> <p>Check Pods:    <pre><code>kubectl get pods\n</code></pre>    Look for a Pod named <code>nginx-urunc-xxxx-yyyy</code> in <code>Running</code> state.</p> </li> <li> <p>Check Logs:    <pre><code>kubectl logs &lt;pod-name&gt;\n</code></pre></p> </li> </ol> <p>You should see Unikraft logs indicating NGINX startup without errors.</p> <p>Output:     <pre><code>[    1.118355] Info: [libukcpio] &lt;cpio.c @  233&gt; Extracting /./nginx/logs/error.log (0 bytes)\n[    1.150129] Info: [libukcpio] &lt;cpio.c @  286&gt; Creating directory /./nginx/html\n[    1.178385] Info: [libukcpio] &lt;cpio.c @  233&gt; Extracting /./nginx/html/index.html (180 bytes)\n[    1.213847] Info: [libukcpio] &lt;cpio.c @  286&gt; Creating directory /./nginx/conf\n[    1.243744] Info: [libukcpio] &lt;cpio.c @  233&gt; Extracting /./nginx/conf/mime.types (5058 bytes)\n[    1.280254] Info: [libukcpio] &lt;cpio.c @  233&gt; Extracting /./nginx/conf/nginx.conf (361 bytes)\n[    1.319238] Info: [libdevfs] &lt;devfs_vnops.c @  309&gt; Mount devfs to /dev...VFS: mounting devfs at /dev\nPowered by\no.   .o       _ _               __ _\nOo   Oo  ___ (_) | __ __  __ _ ' _) :_\noO   oO ' _ `| | |/ /  _)' _` | |_|  _)\noOo oOO| | | | |   (| | | (_) |  _) :_\nOoOoO ._, ._:_:_,\\_._,  .__,_:_, \\___)\n      Epimetheus 0.12.0~4c7352c0-custom\n[    1.472947] Info: [libukboot] &lt;boot.c @  348&gt; Pre-init table at 0x29f0d8 - 0x29f0d8\n[    1.509741] Info: [libukboot] &lt;boot.c @  359&gt; Constructor table at 0x29f0d8 - 0x29f0d8\n[    1.547873] Info: [libukboot] &lt;boot.c @  369&gt; Calling main(3, ['/unikernel/app-nginx_kvm-x86_64', '-c', '/nginx/conf/nginx.conf'])\n</code></pre></p>"},{"location":"tutorials/Running-urunc-with-kind/#step-7-verify-urunc-runtime-usage","title":"Step 7: Verify urunc Runtime Usage","text":"<p>Confirm that the <code>nginx-urunc</code> Pod uses the <code>urunc</code> runtime.</p> <ol> <li>Check Pod RuntimeClass:    <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre>    Look for:    <pre><code>Runtime Class Name:  urunc\n</code></pre></li> </ol>"},{"location":"tutorials/Running-vaccel-with-urunc/","title":"Running vAccel-enabled Containers with <code>urunc</code>","text":"<p>In this tutorial, we describe how to run vAccel-enabled Linux containers  with <code>urunc</code> using QEMU or Firecracker as the underlying hypervisor.</p>"},{"location":"tutorials/Running-vaccel-with-urunc/#qemu","title":"QEMU","text":"<p>When running under QEMU, communication between the guest and the host agent is established  using vsock. The RPC agent listens on a vsock address on the host, and the guest connects to  it using the same address.</p>"},{"location":"tutorials/Running-vaccel-with-urunc/#host-side","title":"Host side","text":"<p>Start the vAccel RPC agent on the host and configure it to listen on a vsock address: <pre><code>export VACCEL_PLUGINS=libvaccel-noop.so\nexport VACCEL_LOG_LEVEL=4\nvaccel-rpc-agent -a vsock://2:2049\n</code></pre> This spawns the RPC agent and makes it available to guests via vsock port <code>2049</code>.</p>"},{"location":"tutorials/Running-vaccel-with-urunc/#guest-side","title":"Guest side","text":"<p>Run the container image with the appropriate runtime annotations: <pre><code>sudo docker run --runtime io.containerd.urunc.v2 --rm -it --annotation com.urunc.unikernel.vAccel=\"vsock\" --annotation com.urunc.unikernel.RPCAddress=\"vsock://2:2049\" --pull always harbor.nbfc.io/nubificus/ubuntu-vaccel-urunc-qemu:x86_64\n</code></pre> Inside the container, execute an example vAccel workload: <pre><code>classify /usr/share/vaccel/images/example.jpg 1\n</code></pre></p>"},{"location":"tutorials/Running-vaccel-with-urunc/#expected-output","title":"Expected output","text":"<p><pre><code>2025.12.11-15:02:16.45 - &lt;info&gt; vAccel 0.7.1-51-499fc2f7\n2025.12.11-15:02:16.50 - &lt;info&gt; Registered plugin rpc 0.2.1-10-3d4d748c\nInitialized session with id: 1\nclassification tags: This is a dummy classification tag!\nclassification imagename: This is a dummy imgname!\n</code></pre> If you rerun the command with a higher log level: <pre><code>export VACCEL_LOG_LEVEL=4\nclassify /usr/share/vaccel/images/example.jpg 1\n</code></pre> The expected output is: <pre><code>2025.12.11-15:03:04.72 - &lt;debug&gt; Initializing vAccel\n2025.12.11-15:03:04.72 - &lt;info&gt; vAccel 0.7.1-51-499fc2f7\n2025.12.11-15:03:04.72 - &lt;debug&gt; Config:\n2025.12.11-15:03:04.72 - &lt;debug&gt;   plugins = libvaccel-rpc.so\n2025.12.11-15:03:04.72 - &lt;debug&gt;   log_level = debug\n2025.12.11-15:03:04.72 - &lt;debug&gt;   log_file = (null)\n2025.12.11-15:03:04.72 - &lt;debug&gt;   profiling_enabled = false\n2025.12.11-15:03:04.72 - &lt;debug&gt;   version_ignore = false\n2025.12.11-15:03:04.74 - &lt;debug&gt; Created top-level rundir: /run/user/0/vaccel/oMcLeO\n2025.12.11-15:03:04.75 - &lt;info&gt; Registered plugin rpc 0.2.1-10-3d4d748c\n2025.12.11-15:03:04.76 - &lt;debug&gt; rpc is a VirtIO module\n2025.12.11-15:03:04.76 - &lt;debug&gt; Registered op exec from plugin rpc\n2025.12.11-15:03:04.77 - &lt;debug&gt; Registered op exec_with_resource from plugin rpc\n2025.12.11-15:03:04.77 - &lt;debug&gt; Registered op image_classify from plugin rpc\n2025.12.11-15:03:04.77 - &lt;debug&gt; Registered op image_detect from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op image_segment from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op image_depth from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op image_pose from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op tflite_model_load from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op tflite_model_unload from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op tflite_model_run from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op torch_model_load from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op torch_model_run from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op blas_sgemm from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op fpga_arraycopy from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op fpga_mmult from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op fpga_parallel from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op fpga_vectoradd from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op minmax from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op opencv from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op tf_model_load from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op tf_model_unload from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Registered op tf_model_run from plugin rpc\n2025.12.11-15:03:04.78 - &lt;debug&gt; Loaded plugin rpc from libvaccel-rpc.so\n2025.12.11-15:03:04.79 - &lt;debug&gt; [rpc] Initializing new remote session\n2025.12.11-15:03:04.86 - &lt;debug&gt; [rpc] Initialized remote session 2\n2025.12.11-15:03:04.86 - &lt;debug&gt; New rundir for session 1: /run/user/0/vaccel/oMcLe1\n2025.12.11-15:03:04.87 - &lt;debug&gt; Initialized session 1 with plugin rpc (remote id: )\nInitialized session with id: 1\n2025.12.11-15:03:04.88 - &lt;debug&gt; session:1 Looking for func implementing op image_cy\n2025.12.11-15:03:04.88 - &lt;debug&gt; Returning func for op image_classify from plugin rc\n2025.12.11-15:03:04.89 - &lt;debug&gt; [rpc] session:2 Executing op image_classify\nclassification tags: This is a dummy classification tag!\nclassification imagename: This is a dummy imgname!\n2025.12.11-15:03:04.90 - &lt;debug&gt; [rpc] Releasing remote session 2\n2025.12.11-15:03:04.91 - &lt;debug&gt; Released session 1\n2025.12.11-15:03:04.91 - &lt;debug&gt; Cleaning up vAccel\n2025.12.11-15:03:04.91 - &lt;debug&gt; Cleaning up sessions\n2025.12.11-15:03:04.92 - &lt;debug&gt; Cleaning up resources\n2025.12.11-15:03:04.92 - &lt;debug&gt; Cleaning up plugins\n2025.12.11-15:03:04.93 - &lt;debug&gt; Unregistered plugin rpc\n</code></pre></p>"},{"location":"tutorials/Running-vaccel-with-urunc/#firecracker","title":"Firecracker","text":"<p>For Firecracker, the guest still uses vsock, but the host-side agent listens on a unix  socket instead. urunc automatically translates the unix socket address into a vsock  address and bind-mounts the socket path into the guest.</p>"},{"location":"tutorials/Running-vaccel-with-urunc/#host-side_1","title":"Host side","text":"<p>Start the RPC agent using a unix socket: <pre><code>export VACCEL_PLUGINS=libvaccel-noop.so\nexport VACCEL_LOG_LEVEL=4\nsudo mkdir /vaccel  # if does not exist\nsudo chown &lt;user&gt; /vaccel\nvaccel-rpc-agent -a unix:///vaccel/vaccel.sock_2049\n</code></pre> The socket directory must be accessible so it can be bind-mounted into the guest.</p>"},{"location":"tutorials/Running-vaccel-with-urunc/#guest-side_1","title":"Guest side","text":"<p>Run the container using <code>nerdctl</code> and the <code>devmapper</code> snapshotter: <pre><code>sudo nerdctl run --runtime io.containerd.urunc.v2 --rm -it --snapshotter devmapper --annotation com.urunc.unikernel.vAccel=\"vsock\" --annotation com.urunc.unikernel.RPCAddress=\"unix:///vaccel/vaccel.sock_2049\" --pull always harbor.nbfc.io/nubificus/ubuntu-vaccel-urunc-fc:x86_64\n</code></pre></p> <p>Run the same example workload: <pre><code>classify /usr/share/vaccel/images/example.jpg 1\n</code></pre></p>"},{"location":"tutorials/Running-vaccel-with-urunc/#expected-output_1","title":"Expected output","text":"<p>The expected output is identical to the QEMU execution.</p>"},{"location":"tutorials/eks-tutorial/","title":"EKS Setup for <code>urunc</code>","text":"<p>In this tutorial, we\u2019ll walk through the complete process of provisioning an Amazon EKS (Elastic Kubernetes Service) cluster from scratch using the AWS CLI, <code>eksctl</code>, and a few supporting tools.</p> <p>Our goal is to create a Kubernetes-native environment capable of securely running containers with <code>urunc</code> \u2014 a unikernel container runtime. This tutorial sets up a production-grade EKS cluster, complete with custom networking, Calico CNI plugin for fine-grained pod networking, and node groups ready to schedule unikernel workloads.</p> <p>We\u2019ll cover:</p> <ul> <li>Tooling prerequisites</li> <li>VPC and networking setup</li> <li>Cluster bootstrapping with <code>eksctl</code></li> <li>Calico installation and configuration</li> <li>Managed node group provisioning</li> <li><code>urunc</code> installation</li> <li>Example deployment of unikernels</li> </ul>"},{"location":"tutorials/eks-tutorial/#tooling-setup-for-eks-cluster-provisioning","title":"Tooling Setup for EKS Cluster Provisioning","text":"<p>This section ensures your local environment is equipped with all the required tools to interact with AWS and provision your EKS cluster.</p>"},{"location":"tutorials/eks-tutorial/#prerequisites","title":"Prerequisites","text":"<p>You'll need the following CLI tools installed and configured:</p>"},{"location":"tutorials/eks-tutorial/#1-aws-cli","title":"1. AWS CLI","text":"<p>Used to interact with AWS services like IAM, EC2, CloudFormation, etc.</p> <p>Install AWS CLI (v2 recommended) <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre></p> <p>Verify installation: <pre><code>aws --version\n</code></pre></p> <p>Configure it with your credentials: <pre><code>aws configure\n</code></pre></p> <p>You'll be prompted to enter:</p> <ul> <li>AWS Access Key ID </li> <li>AWS Secret Access Key</li> <li>Default region (e.g., <code>eu-central-1</code>)</li> <li>Default output format (e.g., <code>json</code>)</li> </ul>"},{"location":"tutorials/eks-tutorial/#2-eksctl","title":"2. eksctl","text":"<p>The official CLI tool for managing EKS clusters.</p> <p>Download and install eksctl:</p> <pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\n</code></pre> <p>Verify the installation: <pre><code>eksctl version\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#3-kubectl","title":"3. kubectl","text":"<p>The Kubernetes CLI used to interact with your EKS cluster.</p> <p>Install kubectl (replace version as needed):</p> <pre><code>curl -LO \"https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Verify the installation: <pre><code>kubectl version --client\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#4-jq","title":"4. jq","text":"<p>A lightweight and flexible command-line JSON processor, used in helper scripts.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y jq\n</code></pre>"},{"location":"tutorials/eks-tutorial/#5-ssh-keypair-for-node-access","title":"5. SSH Keypair (for Node Access)","text":"<p>Ensure you have a key pair uploaded to AWS for SSH access to EC2 instances.</p> <p>Generate an SSH key if you don\u2019t have one: <pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/awseks -N \"\"\n</code></pre> Import the public key into AWS (or use an existing one)</p> <pre><code>aws ec2 import-key-pair \\\n  --key-name awseks \\\n  --public-key-material fileb://~/.ssh/awseks.pub\n</code></pre>"},{"location":"tutorials/eks-tutorial/#cluster-setup","title":"Cluster Setup","text":"<p>We begin by provisioning an Amazon EKS cluster with private subnets and Calico as the CNI instead of the default AWS CNI.</p>"},{"location":"tutorials/eks-tutorial/#vpc-with-private-subnets","title":"VPC with Private Subnets","text":"<p>We use the official EKS CloudFormation template to create a VPC with private subnets.</p> <pre><code>export STACK_NAME=\"urunc-tutorial\"\nexport REGION=\"eu-central-1\"\naws cloudformation create-stack \\\n  --region $REGION \\\n  --stack-name $STACK_NAME \\\n  --template-url https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml\n</code></pre> <p>The output of the above command would verify the successful creation of the VPC:</p> <pre><code>{\n    \"StackId\": \"arn:aws:cloudformation:eu-central-1:058264306458:stack/urunc-tutorial/ec8ae800-0fbc-11f0-bda2-0a29df3fde61\"\n}\n</code></pre>"},{"location":"tutorials/eks-tutorial/#create-iam-role-for-the-eks-cluster","title":"Create IAM Role for the EKS Cluster","text":"<p>We define a trust policy allowing EKS to assume a role. Create a json file (e.g. <code>eks-cluster-role-trust-policy.json</code>) with the following contents: <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre></p> <p>Create the role:</p> <pre><code>aws iam create-role \\\n  --role-name uruncTutorialRole \\\n  --assume-role-policy-document file://eks-cluster-role-trust-policy.json\n</code></pre> <p>Attach the required EKS policy: <pre><code>aws iam attach-role-policy \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy \\\n  --role-name uruncTutorialRole\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#extract-public-subnet-ids-if-needed","title":"Extract Public Subnet IDs (if needed)","text":"<p>This helper script (<code>get_pub_subnets.sh</code>) identifies public subnets in the current region by checking for routes to an Internet Gateway:</p> <pre><code>#!/bin/bash\n\nREGION=\"eu-central-1\"\nsubnets=$(aws ec2 describe-subnets --query 'Subnets[*].{ID:SubnetId}' --output text --region $REGION)\nroute_tables=$(aws ec2 describe-route-tables --query 'RouteTables[*].{ID:RouteTableId,Associations:Associations[*].SubnetId,Routes:Routes[*]}' --output json --region $REGION)\n\npublic_subnets=()\n\nfor subnet in $subnets; do\n  associated_route_table=$(echo $route_tables | jq -r --arg SUBNET \"$subnet\" '.[] | select(.Associations[]? == $SUBNET) | .ID')\n  if [ -n \"$associated_route_table\" ]; then\n    has_igw=$(echo $route_tables | jq -r --arg RTID \"$associated_route_table\" '.[] | select(.ID == $RTID) | .Routes[] | select(.GatewayId != null) | .GatewayId' | grep 'igw-')\n    if [ -n \"$has_igw\" ]; then\n      public_subnets+=(\"$subnet\")\n    fi\n  fi\ndone\n\npublic_subnets_str=$(IFS=,; echo \"${public_subnets[*]}\")\necho \"$public_subnets_str\"\n</code></pre> <p>Run it to retrieve subnet IDs: <pre><code>bash get_pub_subnets.sh\n</code></pre></p> <p>Example output: <pre><code>subnet-02bcaca5ac39eca7a,subnet-0d0667e2156169998\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#create-the-eks-cluster-with-calico-cni","title":"Create the EKS Cluster with Calico CNI","text":"<p>It is time to set up the cluster and managed node groups with Calico networking.</p>"},{"location":"tutorials/eks-tutorial/#step-1-create-eks-control-plane-with-private-subnets-and-no-initial-node-group","title":"Step 1: Create EKS control plane with private subnets and no initial node group","text":"<p>Use the subnets from the command above.</p> <pre><code>export CLUSTER_NAME=\"urunc-tutorial\"\nexport REGION=\"eu-central-1\"\nexport SUBNETS=\"subnet-02bcaca5ac39eca7a,subnet-0d0667e2156169998\"\neksctl create cluster \\\n  --name ${CLUSTER_NAME} \\\n  --region $REGION \\\n  --version 1.30 \\\n  --vpc-private-subnets $SUBNETS \\\n  --without-nodegroup\n</code></pre> <p>Example output: <pre><code>2 sequential tasks: { create cluster control plane \"urunc-tutorial\", wait for control plane to become ready\n}\n2025-04-02 12:29:16 [\u2139]  building cluster stack \"eksctl-urunc-tutorial-cluster\"\n2025-04-02 12:29:19 [\u2139]  deploying stack \"eksctl-urunc-tutorial-cluster\"\n2025-04-02 12:29:49 [\u2139]  waiting for CloudFormation stack \"eksctl-urunc-tutorial-cluster\"\n[...]\n2025-04-02 12:39:26 [\u2139]  waiting for the control plane to become ready\n2025-04-02 12:39:27 [\u2714]  saved kubeconfig as \"~/.kube/config\"\n2025-04-02 12:39:27 [\u2139]  no tasks\n2025-04-02 12:39:27 [\u2714]  all EKS cluster resources for \"urunc-tutorial\" have been created\n2025-04-02 12:39:27 [\u2714]  created 0 nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:39:27 [\u2714]  created 0 managed nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:39:35 [\u2139]  kubectl command should work with \"~/.kube/config\", try 'kubectl get nodes'\n2025-04-02 12:39:35 [\u2714]  EKS cluster \"urunc-tutorial\" in \"eu-central-1\" region is ready\n</code></pre></p> <p>Now, you should have the control-plane deployed and ready. The first thing to do is to remove the AWS CNI, as gateway ARP entries are statically populated.</p>"},{"location":"tutorials/eks-tutorial/#step-2-remove-aws-cni","title":"Step 2: Remove AWS CNI","text":"<pre><code>kubectl delete daemonset -n kube-system aws-node\n</code></pre> <p>Expected output: <pre><code>daemonset.apps \"aws-node\" deleted\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#step-3-add-calico-cni","title":"Step 3: Add Calico CNI:","text":"<pre><code>kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml\n</code></pre> <p>Note: There are cases where a large set of manifests can cause a failure to the above command. If it does, try to re-issue the command.</p> <p>Expected output: <pre><code>namespace/tigera-operator created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created\nserviceaccount/tigera-operator created\nclusterrole.rbac.authorization.k8s.io/tigera-operator created\nclusterrolebinding.rbac.authorization.k8s.io/tigera-operator created\ndeployment.apps/tigera-operator created\n</code></pre></p> <p>Create an installation resource to provision the <code>calico-node</code> daemonset: <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: Installation\napiVersion: operator.tigera.io/v1\nmetadata:\n  name: default\nspec:\n  kubernetesProvider: EKS\n  cni:\n    type: Calico\n  calicoNetwork:\n    bgp: Disabled\nEOF\n</code></pre></p> <p>Expected output:</p> <pre><code>installation.operator.tigera.io/default created\n</code></pre>"},{"location":"tutorials/eks-tutorial/#step-4-provision-nodes","title":"Step 4: Provision nodes","text":"<p>Now, you are ready to provision nodes for the cluster. Use the following description to create two bare-metal nodes, one for each supported architecture (<code>amd64</code> and <code>arm64</code>):</p> <p>Note: Make sure the <code>metadata.name</code> entry corresponds to the name you specified for your cluster above, and that the managedNodeGroups.[].subnets entry correspond to the ones specified above.</p> <pre><code>eksctl create nodegroup -f - &lt;&lt;EOF\n---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: urunc-tutorial\n  region: eu-central-1\n\nmanagedNodeGroups:\n  - name: a1-metal\n    instanceType: a1.metal\n    amiFamily: Ubuntu2204\n    desiredCapacity: 1\n    minSize: 1\n    maxSize: 1\n    volumeSize: 150\n    volumeType: gp3\n    volumeEncrypted: true\n    privateNetworking: true\n    ssh:\n      allow: true\n      publicKeyName: awseks\n    subnets: [\"subnet-02bcaca5ac39eca7a\",\"subnet-0d0667e2156169998\"]\n    iam:\n      withAddonPolicies:\n        cloudWatch: true\n  - name: c5-metal\n    instanceType: c5.metal\n    amiFamily: Ubuntu2204\n    desiredCapacity: 1\n    minSize: 1\n    maxSize: 1\n    volumeSize: 150\n    volumeType: gp3\n    volumeEncrypted: true\n    privateNetworking: true\n    ssh:\n      allow: true\n      publicKeyName: awseks\n    subnets: [\"subnet-02bcaca5ac39eca7a\",\"subnet-0d0667e2156169998\"]\n    iam:\n      withAddonPolicies:\n        cloudWatch: true\nEOF\n</code></pre> <p>Example output: <pre><code>2025-04-02 12:39:44 [\u2139]  will use version 1.30 for new nodegroup(s) based on control plane version\n2025-04-02 12:39:46 [!]  \"aws-node\" was not found\n2025-04-02 12:39:48 [\u2139]  nodegroup \"a1-metal-cni\" will use \"ami-0eb5f4a5031f47d7b\" [Ubuntu2204/1.30]\n2025-04-02 12:39:49 [\u2139]  using EC2 key pair \"awseks\"\n2025-04-02 12:39:49 [\u2139]  nodegroup \"c5-metal-cni\" will use \"ami-0375252546bcbdbfa\" [Ubuntu2204/1.30]\n2025-04-02 12:39:49 [\u2139]  using EC2 key pair \"awseks\"\n2025-04-02 12:39:50 [\u2139]  2 nodegroups (a1-metal-cni, c5-metal-cni) were included (based on the include/exclude rules)\n2025-04-02 12:39:50 [\u2139]  will create a CloudFormation stack for each of 2 managed nodegroups in cluster \"urunc-tutorial\"\n2025-04-02 12:39:50 [\u2139]\n2 sequential tasks: { fix cluster compatibility, 1 task: {\n2 parallel tasks: { create managed nodegroup \"a1-metal\", create managed nodegroup \"c5-metal\"\n} }\n}\n2025-04-02 12:39:50 [\u2139]  checking cluster stack for missing resources\n2025-04-02 12:39:50 [\u2139]  cluster stack has all required resources\n2025-04-02 12:39:51 [\u2139]  building managed nodegroup stack \"eksctl-urunc-tutorial-nodegroup-a1-metal-cni\"\n2025-04-02 12:39:51 [\u2139]  building managed nodegroup stack \"eksctl-urunc-tutorial-nodegroup-c5-metal-cni\"\n2025-04-02 12:39:51 [\u2139]  deploying stack \"eksctl-urunc-tutorial-nodegroup-c5-metal\"\n2025-04-02 12:39:51 [\u2139]  deploying stack \"eksctl-urunc-tutorial-nodegroup-a1-metal\"\n2025-04-02 12:39:51 [\u2139]  waiting for CloudFormation stack \"eksctl-urunc-tutorial-nodegroup-c5-metal\"\n2025-04-02 12:39:51 [\u2139]  waiting for CloudFormation stack \"eksctl-urunc-tutorial-nodegroup-a1-metal\"\n[...]\n2025-04-02 12:44:05 [\u2139]  no tasks\n2025-04-02 12:44:05 [\u2714]  created 0 nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:44:06 [\u2139]  nodegroup \"a1-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-103-211.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2139]  waiting for at least 1 node(s) to become ready in \"a1-metal\"\n2025-04-02 12:44:06 [\u2139]  nodegroup \"a1-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-103-211.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2139]  nodegroup \"c5-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-32-137.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2139]  waiting for at least 1 node(s) to become ready in \"c5-metal\"\n2025-04-02 12:44:06 [\u2139]  nodegroup \"c5-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-32-137.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2714]  created 2 managed nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:44:07 [\u2139]  checking security group configuration for all nodegroups\n2025-04-02 12:44:07 [\u2139]  all nodegroups have up-to-date cloudformation templates\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#step-5-enable-ssh-access-optional","title":"Step 5: Enable SSH access (optional)","text":"<p>Finally, for debug purposes, enable external SSH access to the nodes:</p> <p>Note: Example for one of the two security groups <pre><code>aws ec2 authorize-security-group-ingress --group-id sg-0d655f9002aec154e --protocol tcp --port 22 --cidr 0.0.0.0/0 --region eu-central-1\n</code></pre></p> <p>Example output: <pre><code>{\n    \"Return\": true,\n    \"SecurityGroupRules\": [\n        {\n            \"SecurityGroupRuleId\": \"sgr-09634d2d1eb260e7a\",\n            \"GroupId\": \"sg-0d655f9002aec154e\",\n            \"GroupOwnerId\": \"058264306458\",\n            \"IsEgress\": false,\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": 22,\n            \"ToPort\": 22,\n            \"CidrIpv4\": \"0.0.0.0/0\",\n            \"SecurityGroupRuleArn\": \"arn:aws:ec2:eu-central-1:058264306458:security-group-rule/sgr-09634d2d1eb260e7a\"\n        }\n    ]\n}\n</code></pre></p> <p>Below is a script to enable external SSH access to all security groups:</p> <p>Note: Careful, this exposes SSH access to all of your nodes!</p> <pre><code>#!/bin/bash\naws ec2 describe-security-groups --region eu-central-1 --query \"SecurityGroups[*].GroupId\" --output text | tr '\\t' '\\n' | \\\nwhile read sg_id; do\n    echo \"Enabling SSH access for $sg_id...\"\n    aws ec2 authorize-security-group-ingress \\\n        --group-id \"$sg_id\" \\\n        --protocol tcp \\\n        --port 22 \\\n        --cidr 0.0.0.0/0 \\\n        --region eu-central-1 2&gt;&amp;1 | grep -v \"InvalidPermission.Duplicate\"\ndone\n</code></pre>"},{"location":"tutorials/eks-tutorial/#verify-the-cluster-is-operational","title":"Verify the cluster is operational","text":"<p>We have successfully setup the cluster. Let's see what we have using a simple <code>kubectl get pods -o wide -A</code>:</p> <pre><code>NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE                                               NOMINATED NODE   READINESS GATES\ncalico-system     calico-kube-controllers-64cf794c44-jnggx   1/1     Running   0          3m52s   172.16.50.196     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-xcqrj                          1/1     Running   0          3m47s   192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-xn6fc                          1/1     Running   0          3m48s   192.168.103.211   ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-typha-84546c84b6-86wfx              1/1     Running   0          3m52s   192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ncalico-system     csi-node-driver-jzs7g                      2/2     Running   0          3m52s   172.16.139.1      ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\ncalico-system     csi-node-driver-sstkj                      2/2     Running   0          3m52s   172.16.50.193     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system       coredns-6f6d89bcc9-dkn6z                   1/1     Running   0          10m     172.16.50.195     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system       coredns-6f6d89bcc9-ld454                   1/1     Running   0          10m     172.16.50.194     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system       kube-proxy-7mnbs                           1/1     Running   0          4m3s    192.168.103.211   ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system       kube-proxy-nx5wk                           1/1     Running   0          4m4s    192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ntigera-operator   tigera-operator-76ff79f7fd-z7t7d           1/1     Running   0          7m17s   192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Also, let's check out the nodes using <code>kubectl get nodes --show-labels</code>:</p> <p><pre><code>$ kubectl get nodes --show-labels\nNAME                                               STATUS   ROLES    AGE   VERSION   LABELS\nip-192-168-103-211.eu-central-1.compute.internal   Ready    &lt;none&gt;   10m   v1.30.6   alpha.eksctl.io/cluster-name=urunc-tutorial,alpha.eksctl.io/instance-id=i-0f1dc1ede23d8e5a7,alpha.eksctl.io/nodegroup-name=a1-metal-cni,beta.kubernetes.io/arch=arm64,beta.kubernetes.io/instance-type=a1.metal,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0eb5f4a5031f47d7b,eks.amazonaws.com/nodegroup=a1-metal-cni,eks.amazonaws.com/sourceLaunchTemplateId=lt-0a89f4d0e008cf6f6,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=eu-central-1,failure-domain.beta.kubernetes.io/zone=eu-central-1b,k8s.io/cloud-provider-aws=8c600fe081bc4d4e16d89383ee5c2ac7,kubernetes.io/arch=arm64,kubernetes.io/hostname=ip-192-168-103-211.eu-central-1.compute.internal,kubernetes.io/os=linux,node-lifecycle=on-demand,node.kubernetes.io/instance-type=a1.metal,topology.k8s.aws/zone-id=euc1-az3,topology.kubernetes.io/region=eu-central-1,topology.kubernetes.io/zone=eu-central-1b\nip-192-168-32-137.eu-central-1.compute.internal    Ready    &lt;none&gt;   10m   v1.30.6   alpha.eksctl.io/cluster-name=urunc-tutorial,alpha.eksctl.io/instance-id=i-033fcef7c9cf7b5aa,alpha.eksctl.io/nodegroup-name=c5-metal-cni,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=c5.metal,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0375252546bcbdbfa,eks.amazonaws.com/nodegroup=c5-metal-cni,eks.amazonaws.com/sourceLaunchTemplateId=lt-0894d82a5833f577b,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=eu-central-1,failure-domain.beta.kubernetes.io/zone=eu-central-1a,k8s.io/cloud-provider-aws=8c600fe081bc4d4e16d89383ee5c2ac7,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-192-168-32-137.eu-central-1.compute.internal,kubernetes.io/os=linux,node-lifecycle=on-demand,node.kubernetes.io/instance-type=c5.metal,topology.k8s.aws/zone-id=euc1-az2,topology.kubernetes.io/region=eu-central-1,topology.kubernetes.io/zone=eu-central-1a\n</code></pre> Let's do a test deployment. Create a file called <code>nginx-test-deployment.yaml</code> with the following content:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-stock\n  labels:\n    app: nginx-stock\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-stock\n  template:\n    metadata:\n      labels:\n        app: nginx-stock\n    spec:\n      containers:\n      - name: nginx-stock\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        resources:\n          #limits:\n            #memory: 768Mi\n          requests:\n            memory: 60Mi\n</code></pre> <p>And deploy it: <pre><code>kubectl apply -f nginx-test-deployment.yaml\n</code></pre></p> <p>This should deploy 2 replicas of NGINX. Check the status:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Example output: <pre><code>NAME                           READY   STATUS    RESTARTS   AGE     IP                NODE                                               NOMINATED NODE   READINESS GATES\nnginx-stock-7d54d66484-k9rj5   1/1     Running   0          42s     172.16.50.197     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nnginx-stock-7d54d66484-nn696   1/1     Running   0          42s     172.16.139.2      ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>And let's try to check network connectivity between pods. Let's run a simple network debug container as a pod:</p> <pre><code>kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n</code></pre> <p>Expected output: <pre><code>If you don't see a command prompt, try pressing enter.\ntmp-shell:~# \n</code></pre></p> <p>If we issue a simple <code>curl</code> command to one of the pods IPs, we should get a response from the NGINX server: <pre><code>tmp-shell:~# curl 172.16.139.2\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> There we go! We have a working EKS cluster, with Calico and two bare-metal nodes. Time to setup urunc! </p>"},{"location":"tutorials/eks-tutorial/#urunc-setup","title":"<code>urunc</code> setup","text":"<p>The easiest way to setup <code>urunc</code> in such a setting is to use <code>urunc-deploy</code>. This process follows the principles of <code>kata-deploy</code> and is build to work on <code>k8s</code> and <code>k3s</code>. The process is as follows:</p>"},{"location":"tutorials/eks-tutorial/#1-clone-the-repo","title":"1. Clone the repo","text":"<pre><code>git clone https://github.com/urunc-dev/urunc\n</code></pre>"},{"location":"tutorials/eks-tutorial/#2-apply-the-manifests","title":"2.  Apply the manifests","text":"<p>First we need to create the RBAC <pre><code>kubectl apply -f deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre></p> <p>Then, we create the <code>urunc-deploy</code> daemonset: <pre><code>kubectl apply -f deployment/urunc-deploy/urunc-deploy/base/urunc-deploy.yaml\n</code></pre></p> <p>Finally, we need to create the appropriate k8s runtime class: <pre><code>kubectl apply -f deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre></p> <p>Example output:</p> <pre><code>serviceaccount/urunc-deploy-sa created\nclusterrole.rbac.authorization.k8s.io/urunc-deploy-role created\nclusterrolebinding.rbac.authorization.k8s.io/urunc-deploy-rb created\ndaemonset.apps/urunc-deploy created\nruntimeclass.node.k8s.io/urunc created\n</code></pre> <p>Monitor the deploy pods once they change their status to <code>Running</code>: <pre><code>kubectl logs -f -n kube-system -l name=urunc-deploy\n</code></pre></p> <p>Example output: <pre><code>Installing qemu\nInstalling solo5-hvt\nInstalling solo5-spt\nAdd urunc as a supported runtime for containerd\nContainerd conf file: /etc/containerd/config.toml\nPlugin ID: \"io.containerd.grpc.v1.cri\"\nOnce again, configuration file is /etc/containerd/config.toml\nreloading containerd\nnode/ip-192-168-103-211.eu-central-1.compute.internal labeled\nurunc-deploy completed successfully\nInstalling qemu\nInstalling solo5-hvt\nInstalling solo5-spt\nAdd urunc as a supported runtime for containerd\nContainerd conf file: /etc/containerd/config.toml\nPlugin ID: \"io.containerd.grpc.v1.cri\"\nOnce again, configuration file is /etc/containerd/config.toml\nreloading containerd\nnode/ip-192-168-32-137.eu-central-1.compute.internal labeled\nurunc-deploy completed successfully\n</code></pre></p> <p>Now we've got urunc installed on each node, along with the supported hypervisors! Let's try to deploy a unikernel! </p>"},{"location":"tutorials/eks-tutorial/#run-a-unikernel","title":"Run a unikernel","text":"<p>Create a YAML file (e.g. <code>nginx-urunc.yaml</code>) with the following contents:</p> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx-urunc\n  name: nginx-urunc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-urunc\n  template:\n    metadata:\n      labels:\n        run: nginx-urunc\n    spec:\n      runtimeClassName: urunc\n      containers:\n      - image: harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n        imagePullPolicy: Always\n        name: nginx-urunc\n        command: [\"sleep\"]\n        args: [\"infinity\"]\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 10m\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-urunc\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx-urunc\n  sessionAffinity: None\n  type: ClusterIP\n</code></pre> Issuing the command below: <pre><code>kubectl apply -f nginx-urunc.yaml\n</code></pre> will produce the following output: <pre><code>deployment.apps/nginx-urunc created\nservice/nginx-urunc created\n</code></pre> and will create a deployment of an NGINX unikernel, from the container image pushed at <code>harbor.nbfc.io/nubificus/urunc/nginx-hvt-rumprun-block:latest</code></p> <p>Inspecting the pods with <code>kubectl get pods -o wide</code> reveals the status: <pre><code>default           nginx-urunc-998b889c4-x798f                1/1     Running             0          2s      172.16.50.225     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>and following up on the previous test, we do:</p> <p><pre><code>kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n</code></pre> To get a shell in a pod in the cluster: <pre><code>If you don't see a command prompt, try pressing enter.\ntmp-shell:~# \n</code></pre></p> <p>and we <code>curl</code> the pod's IP:</p> <pre><code>tmp-shell:~# curl 172.16.50.225\n&lt;html&gt;\n&lt;body style=\"font-size: 14pt;\"&gt;\n    &lt;img src=\"logo150.png\"/&gt;\n    Served to you by &lt;a href=\"http://nginx.org/\"&gt;nginx&lt;/a&gt;, running on a\n    &lt;a href=\"http://rumpkernel.org\"&gt;rump kernel&lt;/a&gt;...\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"tutorials/eks-tutorial/#conclusions","title":"Conclusions","text":"<p>You now have a fully functional EKS cluster with custom VPC networking and Calico CNI, all set up to run unikernel containers via <code>urunc</code>.</p> <p>We have covered how to:</p> <ul> <li>Provision foundational infrastructure on AWS  </li> <li>Deploy a secure and customizable Kubernetes cluster   </li> <li>Configure networking via Calico  </li> <li>Prepare node groups with SSH access for hands-on debugging or remote setup</li> <li>Install <code>urunc</code> via <code>urunc-deploy</code></li> <li>Deploy an example unikernel</li> </ul> <p>With your EKS cluster up and running, equipped with Calico networking and ready for <code>urunc</code>, you now have a powerful, Kubernetes-native foundation for exploring the next generation of lightweight, secure container runtimes!</p>"},{"location":"tutorials/existing-container-linux/","title":"Running existing containers in <code>urunc</code> with Linux","text":"<p>While Linux is not a unikernel framework, it remains the most widely used kernel in cloud infrastructure. As a result, the majority of applications and services are built to run on Linux. At the same time, Linux has a very highly configurable build system, and as proven by Lupine, we can build tailored Linux kernels optimized for running a single application.</p> <p>With this goal in mind, this guide walks through the steps required to take an existing container image and execute it on top of <code>urunc</code> as a Linux virtual machine (VM).</p> <p>Overall, we need to do the followings:</p> <ol> <li>Build or reuse a Linux kernel.</li> <li>(Optional but recommended) Build or fetch an init process.</li> <li>Prepare the final image by appending the Linux kernel (and init) and set up    <code>urunc</code> annotations.</li> </ol>"},{"location":"tutorials/existing-container-linux/#linux-kernel","title":"Linux kernel","text":"<p>The main requirement for running existing containers on top of <code>urunc</code> is a Linux kernel. From <code>urunc</code>'s side there are no specific kernel configuration options required, but since Linux will run on virtual machine monitors like Qemu or Firecracker, the kernel should be configured with the necessary drivers (e.g., virtio devices).</p> <p>To simplify this, you can find here a sample x86 kernel configuration based on Linux v6.14, which builds a minimal kernel around 13\u202fMiB in size. Note that this configuration excludes features like cgroups and certain system calls, so additional customization may be required depending on your application.</p> <p>Alternatively, prebuilt kernels are available via the following container images:</p> <ul> <li><code>harbor.nbfc.io/nubificus/urunc/linux-kernel-qemu:v6.14</code></li> <li><code>harbor.nbfc.io/nubificus/urunc/linux-kernel-firecracker:v6.14</code></li> </ul> <p>Each image contains the Linux kernel binary at <code>/kernel</code>.</p>"},{"location":"tutorials/existing-container-linux/#init-process","title":"Init process","text":"<p>After booting, the Linux kernel hands control to the init process, the first user-space program. This process acts as the root of the process tree and must remain running. If it exits, the kernel will panic.</p> <p>In single-application environments, the application itself can serve as init. However, this is not always reliable:</p> <ul> <li>If the application exits, the system halts.</li> <li>CLI argument handling may be incorrect: Linux does not natively support   multi-word arguments via kernel boot parameters. Each space-separated word is treated as a separate argument.</li> <li>A few necessary operations (such as mount /proc, set default route) might be   required before executing the application.</li> <li>The Linux kernel can only handle a limited number of characters in its boot   parameters. If too many or very long environment variables are passed, the   boot process will fail once that limit is exceeded. </li> </ul> <p>Especially, for CLI argument handing, <code>urunc</code> follows a simple convention. All multi-word CLI arguments are wrapped in single quotes and the init process (or application) is expected to reconstruct them properly.</p> <p>For all the above reasons, we strongly recommend using a dedicated init process. We provide urunit; a lightweight init designed specifically for <code>urunc</code>. It performs the following actions:</p> <ol> <li>Sets default route through eth0. This is necessary when we deploy    the container in a Kubernetes cluster, where there is a high chance that    the gateway of the container might be in a different subnet than the IP.    As a result, Linux kernel will fail to set the gateway.</li> <li>Groups multi-word arguments correctly.</li> <li>Mounts <code>/proc</code> and <code>/sys</code>.</li> <li>Reads and sets the environment variables for the application execution</li> <li>Reads and sets the uid,gid and working directory for the application execution</li> <li>Acts as a reaper, cleaning up zombie processes.</li> </ol> <p>To pass the necessary information to urunit, <code>urunc</code> uses a configuration file that passes to the VM with the following format:</p> <pre><code>UES\n/* list of environment variables */\nUEE\nUCS\nUID:&lt;uid&gt;\nGID:&lt;gid&gt;\nWD:&lt;working_dir&gt;\nUCE\n</code></pre> <p>In order to minimize the dependencies for the Linux kernel running as guest, <code>urunc</code> attaches this configuration file as an initrd of the VM and sets the <code>retain_initrd</code> kernel boot parameter. In that way, urunit will be able to read the configuration file from <code>/sys/firmware/initrd</code>. In case the guest is configured to boot with a user-specified initrd, then <code>urunc</code> will archive in the cpio format the configuration file and concatenate it with the user-provided initrd. The Linux kernel is smart enough to handle concatenated initrd and hence the configuration file will appear in the guest's rootfs.</p> <p>As a result, in case urunit is used we strongly recommend to configure the Linux kernel setting the following configuration options:</p> <ul> <li>Support for initrd <code>CONFIG_BLK_DEV_INITRD</code></li> <li>Support for sysfs <code>CONFIG_SYSFS</code></li> <li>Support for procfs <code>CONFIG_PROC_FS</code></li> </ul> <p>You can obtain urunit in two ways:</p> <ul> <li>Fetch a static binary from urunit's release   page.   Via the container image: <code>harbor.nbfc.io/nubificus/urunit:latest</code>,   with the binary located at <code>/urunit</code>.</li> </ul>"},{"location":"tutorials/existing-container-linux/#preparing-the-image","title":"Preparing the image","text":"<p>To differentiate traditional containers from unikernels, <code>urunc</code> uses specific annotations. Therefore, to run a container with a Linux kernel on <code>urunc</code>, these annotations must be configured, and the Linux kernel must be included in the container image\u2019s root filesystem. To simplify this process, we will use bunny.</p> <p>Another important aspect is preparing the root filesystem (rootfs). Since we're booting a full Linux virtual machine, a proper rootfs must be provided. There are three main ways to do this:</p> <ol> <li>Using directly the rootfs of the container's image (requires either a block based snapshotter or 9pfs/Virtiofs).</li> <li>Creating a block image out of a container's image rootfs.</li> <li>Creating a initrd.</li> </ol>"},{"location":"tutorials/existing-container-linux/#using-directly-the-containers-rootfs","title":"Using directly the container's rootfs","text":"<p>The simplest way to boot an existing container with a Linux kernel on <code>urunc</code> is to reuse the container\u2019s rootfs. This is possible either through shared-fs between the host and the Linux VM or by using devmapper as the snapshotter. In the latter case containerd's devmapper snapshotter will create a snapshot of the container;s image in the form of a block image and <code>urunc</code> can then directly attach this block image to the VM.</p> <p>To set up devmapper as a snapshotter please refer to the installation guide.</p>"},{"location":"tutorials/existing-container-linux/#preparing-the-container-image","title":"Preparing the container image.","text":"<p>In this case preparing the container image involves two key steps:</p> <ol> <li>Append the Linux kernel binary to the container image.</li> <li>Set the appropriate <code>urunc</code> annotations.</li> </ol> <p>These tasks can be easily automated with bunny.</p> <p>Let's use as an example the <code>redis:alpine</code> container image using the Linux kernel from <code>harbor.nbfc.io/nubificus/urunc/linux-kernel-qemu:v6.14</code>. The respective <code>bunnyfile</code> would look like:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: linux\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: redis:alpine\n  type: raw\n\nkernel:\n  from: harbor.nbfc.io/nubificus/urunc/linux-kernel-qemu:v6.14\n  path: /kernel\n\ncmdline: \"/usr/local/bin/redis-server\"\n</code></pre> <p>We can build the container with:</p> <pre><code>docker build -f bunnyfile -t redis/apline/linux/qemu:latest .\n</code></pre> <p>Alternatively, if the Linux kernel was built locally, we can update the kernel section of the bunnyfile to reference the local binary:</p> <pre><code>kernel:\n  from: local\n  path: bzImage\n</code></pre> <p>By default, this setup will run redis-server as the init process.  To include urunit in the redis:alpine image, we can use the following Containerfile:</p> <pre><code>FROM harbor.nbfc.io/nubificus/urunit:latest AS init\n\nFROM redis:alpine\n\nCOPY --from=init /urunit /urunit\n</code></pre> <p>NOTE: We are working towards enabling the addition of extra files from the <code>bunnyfile</code>. We will update this page once this feature is supported.</p> <p>After building the above container, make sure to specify it in the <code>from</code> field of rootfs in <code>bunnyfile</code>:</p> <pre><code>rootfs:\n  from: redis/urunit:alpine\n  type: raw\n</code></pre> <p>At last we need to modify the <code>cmdline</code> section of <code>bunnyfile</code> to execute urunit:</p> <pre><code>cmdline: \"/urunit /usr/local/bin/redis-server\"\n</code></pre>"},{"location":"tutorials/existing-container-linux/#running-the-container","title":"Running the container","text":"<p>Unfortunately, Docker requires additional setup to work with the devmapper snapshotter. Therefore, in order to run the container using Docker, we can only use shared-fs.</p> <pre><code>docker run --rm -it --runtime \"io.containerd.urunc.v2\" redis/apline/linux/qemu:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ docker inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre></p> <p>and we should be able to ping it:</p> <pre><code>ping -c 3 172.17.0.2\n</code></pre> <p>ALternatively, if we want to use devmapper as the snapshotter, we can use nerdctl, which integrates seamlessly with containerd and supports devmapper out of the box.</p> <p>First, transfer the container image from Docker\u2019s image store to containerd: <pre><code>docker save redis/apline/linux/qemu:latest | nerdctl load\n</code></pre></p> <p>With the image now available in containerd, we\u2019re ready to run the container using urunc and the devmapper snapshotter:</p> <pre><code>nerdctl run --rm -it --snapshotter devmapper --runtime \"io.containerd.urunc.v2\" redis/apline/linux/qemu:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ nerdctl inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"172.16.1.2\",\n</code></pre></p> <p>and we should be able to ping it:</p> <pre><code>ping -c 3 10.4.0.2\n</code></pre>"},{"location":"tutorials/existing-container-linux/#using-a-block-image","title":"Using a block image","text":"<p>If we have a block image that can be used as a rootfs, we can instruct <code>urunc</code> to pass this block image as a rootfs.</p>"},{"location":"tutorials/existing-container-linux/#preparing-the-container-image_1","title":"Preparing the container image.","text":"<p>To prepare the container image we will need to first create block image. For that purpose, we will use <code>nginx:alpine</code> image and we will choose to run it on top of Firecracker. We can create the block image with the following steps:</p> <pre><code>dd if=/dev/zero of=rootfs.ext2 bs=1 count=0 seek=60M\nmkfs.ext2 rootfs.ext2\nmkdir tmp_mnt\nmount rootfs.ext2 tmp_mnt\ndocker export $(docker create nginx:alpine) -o nginx_alpine.tar\ntar -xf nginx_alpine.tar -C tmp_mnt\nwget -O tmp_mnt/urunit https://github.com/nubificus/urunit/releases/download/v0.1.0/urunit_x86_64 # If we want urunit as init\nchmod +x tmp_mnt/urunit # If we want urunit as init\numount tmp_mnt\n</code></pre> <p>Now we have a block image, <code>rootfs.ext2</code>, generated from the <code>nginx:alpine</code> container and including urunit latest release. To package everything together, we will use a file with Containerfile-like syntax, just to demonstrate how to manually define the required annotations for <code>urunc</code>:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY vmlinux /kernel\nCOPY nginx_rootfs.ext2 /rootfs.ext2\n\nLABEL \"com.urunc.unikernel.binary\"=\"/kernel\"\nLABEL \"com.urunc.unikernel.cmdline\"=\"/urunit /usr/sbin/nginx -g 'daemon off;error_log stderr debug;\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"linux\"\nLABEL \"com.urunc.unikernel.block\"=\"/rootfs.ext2\"\nLABEL \"com.urunc.unikernel.blkMntPoint\"=\"/\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"firecracker\"\n</code></pre> <p>We can build the container with:</p> <pre><code>docker build -f Containerfile -t nginx/apline/linux/firecracker:latest .\n</code></pre>"},{"location":"tutorials/existing-container-linux/#running-the-container_1","title":"Running the container","text":"<p>We can run the container with the following command:</p> <pre><code>docker run --rm -it --runtime \"io.containerd.urunc.v2\" nginx/apline/linux/firecracker:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ docker inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre></p> <p>and we should be able to curl it:</p> <pre><code>$ curl 172.17.0.2\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"tutorials/existing-container-linux/#using-initrd-as-a-rootfs","title":"Using initrd as a rootfs","text":"<p>Similarly to the previous approach, we can create an initrd instead of a block image to use as the root filesystem. To demonstrate this, we will use the <code>traefik/whoami</code> container image as an example.</p>"},{"location":"tutorials/existing-container-linux/#preparing-the-container-image_2","title":"Preparing the container image.","text":"<p>First let's create the initrd:</p> <pre><code>mkdir tmp_rootfs\ndocker export $(docker create traefik/whoami) | tar -C tmp_rootfs/ -xvf -\nwget -O tmp_rootfs/urunit https://github.com/nubificus/urunit/releases/download/v0.1.0/urunit_x86_64 # If we want urunit as init\nchmod +x tmp_rootfs/urunit\ncd tmp_rootfs\nfind . | cpio -H newc -o &gt; ../rootfs.initrd\n</code></pre> <p>NOTE: We are working towards enabling the creation of the initrd directly from bunny. We will update this page once this feature is supported.</p> <p>Now we have an initrd <code>rootfs.initrd</code> generated from <code>traefik/whoami</code> and with urunit that we got from its latest release.  In order to pack everything together, we can use the following <code>bunnyfile</code>:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: linux\n  monitor: firecracker\n  architecture: x86\n\nrootfs:\n  from: local\n  type: initrd\n  path: rootfs.initrd\n\nkernel:\n  from: harbor.nbfc.io/nubificus/urunc/linux-kernel-firecracker:v6.14\n  path: /kernel\n\ncmdline: \"/urunit /whoami\"\n</code></pre> <p>We can build the container with:</p> <pre><code>docker build -f bunnyfile -t traefik/whoami/linux/firecracker:latest .\n</code></pre>"},{"location":"tutorials/existing-container-linux/#running-the-container_2","title":"Running the container","text":"<p>We can run the container with the following command:</p> <pre><code>docker run --rm -it --runtime \"io.containerd.urunc.v2\" traefik/whoami/linux/firecracker:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ docker inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre></p> <p>and we should be able to curl it:</p> <pre><code>$ curl 172.17.0.2\nHostname: urunc\nIP: 127.0.0.1\nIP: 172.17.0.2\nRemoteAddr: 172.17.0.1:42684\nGET / HTTP/1.1\nHost: 172.17.0.2\nUser-Agent: curl/7.68.0\nAccept: */*\n</code></pre>"},{"location":"tutorials/knative/","title":"Knative + urunc: Deploying Serverless Unikernels","text":"<p>This guide walks you through deploying Knative Serving using <code>urunc</code>. You\u2019ll build Knative from a custom branch and use <code>ko</code> for seamless image building and deployment.</p>"},{"location":"tutorials/knative/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster</li> <li>A Docker-compatible registry (e.g. Harbor, Docker Hub)</li> <li>Ubuntu 20.04 or newer</li> <li>Basic <code>git</code>, <code>curl</code>, <code>kubectl</code>, and <code>docker</code> installed</li> </ul>"},{"location":"tutorials/knative/#environment-setup","title":"Environment Setup","text":"<p>Install Docker, Go &gt;= 1.21, and <code>ko</code>:</p>"},{"location":"tutorials/knative/#install-go-121","title":"Install Go 1.21","text":"<pre><code>sudo mkdir /usr/local/go1.21\nwget https://go.dev/dl/go1.21.5.linux-amd64.tar.gz\nsudo tar -zxvf go1.21.5.linux-amd64.tar.gz -C /usr/local/go1.21/\nrm go1.21.5.linux-amd64.tar.gz\n</code></pre>"},{"location":"tutorials/knative/#verify-go-installation-should-be-1215","title":"Verify Go installation (Should be 1.21.5)","text":"<pre><code>$ export GOROOT=/usr/local/go1.21/go \n$ export PATH=$GOROOT/bin:$PATH  \n$ export GOPATH=$HOME/go \n$ go version\ngo version go1.21.5 linux/amd64\n</code></pre>"},{"location":"tutorials/knative/#install-ko-version0151","title":"Install ko VERSION=0.15.1","text":"<pre><code>export OS=Linux\nexport ARCH=x86_64\ncurl -sSfL \"https://github.com/ko-build/ko/releases/download/v${VERSION}/ko_${VERSION}_${OS}_${ARCH}.tar.gz\" -o ko.tar.gz\nsudo tar -zxvf ko.tar.gz -C /usr/local/bin` \n</code></pre>"},{"location":"tutorials/knative/#clone-and-build-knative-with-the-queue-proxy-patch","title":"Clone and Build Knative with the queue-proxy patch","text":""},{"location":"tutorials/knative/#set-your-container-registry","title":"Set your container registry","text":"<p>Note: You should be able to use dockerhub for this. e.g. <code>&lt;yourdockerhubid&gt;/knative</code></p> <pre><code>export KO_DOCKER_REPO='harbor.nbfc.io/nubificus/knative-install-urunc'\n</code></pre>"},{"location":"tutorials/knative/#clone-urunc-enabled-knative-serving","title":"Clone urunc-enabled Knative Serving","text":"<pre><code>git clone https://github.com/nubificus/serving -b feat_urunc \ncd serving/\nko resolve -Rf ./config/core/ &gt; knative-custom.yaml\n</code></pre>"},{"location":"tutorials/knative/#apply-knatives-manifests-to-the-local-k8s","title":"Apply knative's manifests to the local k8s","text":"<pre><code>kubectl apply -f knative-custom.yaml\n</code></pre> <p>Alternatively, you could use our latest build: <pre><code>kubectl apply -f https://s3.nbfc.io/knative/knative-v1.17.0-urunc-5220308.yaml\n</code></pre></p> <p>Note: There are cases where due to the large manifests, kubectl fails. Try a second time, or use <code>kubectl create -f https://s3.nbfc.io/knative/knative-v1.17.0-urunc-5220308.yaml</code></p>"},{"location":"tutorials/knative/#setup-networking-kourier","title":"Setup Networking (Kourier)","text":""},{"location":"tutorials/knative/#install-kourier-patch-ingress-and-domain-configs","title":"Install kourier, patch ingress and domain configs","text":"<pre><code>kubectl apply -f https://github.com/knative/net-kourier/releases/latest/download/kourier.yaml \nkubectl patch configmap/config-network -n knative-serving --type merge -p \\ \n  '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}'\nkubectl patch configmap/config-domain -n knative-serving --type merge -p \\ \n  '{\"data\":{\"127.0.0.1.nip.io\":\"\"}}'\n</code></pre>"},{"location":"tutorials/knative/#enable-runtimeclass-and-urunc-support","title":"Enable RuntimeClass and urunc Support","text":""},{"location":"tutorials/knative/#install-urunc","title":"Install <code>urunc</code>","text":"<p>You can follow the documentation to install <code>urunc</code> from: Installing</p>"},{"location":"tutorials/knative/#enable-runtimeclass-for-services-nodeselector-and-affinity","title":"Enable runtimeClass for services, nodeSelector and affinity","text":"<pre><code>kubectl patch configmap/config-features --namespace knative-serving --type merge --patch '{\"data\":{\n  \"kubernetes.podspec-affinity\":\"enabled\",\n  \"kubernetes.podspec-runtimeclassname\":\"enabled\",\n  \"kubernetes.podspec-nodeselector\":\"enabled\"\n}}'\n</code></pre>"},{"location":"tutorials/knative/#deploy-a-sample-urunc-service","title":"Deploy a Sample urunc Service","text":"<pre><code>kubectl get ksvc -A -o wide\n</code></pre> <p>Should be empty. Create an simple httpreply service, based on a simple C program:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/nubificus/c-httpreply/refs/heads/main/service.yaml\n</code></pre>"},{"location":"tutorials/knative/#check-knative-service","title":"Check Knative Service","text":"<pre><code>kubectl get ksvc -A -o wide \n</code></pre>"},{"location":"tutorials/knative/#test-the-service-replace-ip-with-actual-ingress-ip","title":"Test the service (replace IP with actual ingress IP)","text":"<pre><code>curl -v -H \"Host: hellocontainerc.default.127.0.0.1.nip.io\" http://&lt;INGRESS_IP&gt;\n</code></pre> <p>Now, let's create a <code>urunc</code>-compatible function. Create a service, based on Unikraft's httreply example: </p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/nubificus/app-httpreply/refs/heads/feat_generic/service.yaml\n</code></pre> <p>You should be able to see this being created:</p> <pre><code>$ kubectl get ksvc -o wide\nNAME             URL                                                  LATESTCREATED              LATESTREADY                READY   REASON\nhellounikernelfc http://hellounikernelfc.default.127.0.0.1.nip.io     hellounikernelfc-00001     hellounikernelfc-00001     True\n</code></pre> <p>and once it's on a <code>Ready</code> state, you could issue a request:</p> <p>Note: 10.244.9.220 is the IP of the <code>kourier-internal</code> svc. You can check your own from: <code>kubectl get svc -n kourier-system |grep kourier-internal</code></p> <pre><code>$ curl -v -H \"Host: hellounikernelfc.default.127.0.0.1.nip.io\" http://10.244.9.220:80\n*   Trying 10.244.9.220:80...\n* Connected to 10.244.9.220 (10.244.9.220) port 80 (#0)\n&gt; GET / HTTP/1.1\n&gt; Host: hellounikernelfc.default.127.0.0.1.nip.io\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt;\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; content-length: 14\n&lt; content-type: text/html; charset=UTF-8\n&lt; date: Tue, 08 Apr 2025 15:47:45 GMT\n&lt; x-envoy-upstream-service-time: 774\n&lt; server: envoy\n&lt;\nHello, World!\n* Connection #0 to host 10.244.9.220 left intact\n</code></pre>"},{"location":"tutorials/knative/#wrapping-up","title":"Wrapping Up","text":"<p>You're now running unikernel-based workloads via Knative and <code>urunc</code>! With this setup, you can push the boundaries of lightweight, secure, and high-performance serverless deployments \u2014 all within a Kubernetes-native environment.</p>"}]}